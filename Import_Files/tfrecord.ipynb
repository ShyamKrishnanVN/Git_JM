{"cells": [{"cell_type": "markdown", "metadata": {"id": "0_0.3797539742654885"}, "execution_count": null, "source": ["##### Copyright 2019 The TensorFlow Authors."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.764973054861328"}, "execution_count": null, "source": ["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at \n#\n# https://www.apache.org/licenses/LICENSE-2.0  \n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.16730291033812272"}, "execution_count": null, "source": ["# TFRecord and tf.train.Example\n\n<table class=\"tfo-notebook-buttons\" align=\"left\">\n  <td>\n    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/load_data/tfrecord\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n  </td>\n  <td>\n    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/load_data/tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n  </td>\n  <td>\n    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/load_data/tfrecord.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n  </td>\n</table>"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.8576107260543027"}, "execution_count": null, "source": ["The TFRecord format is a simple format for storing a sequence of binary records.\n\n[Protocol buffers](https://developers.google.com/protocol-buffers/) are a cross-platform, cross-language library for efficient serialization of structured data.\n\nProtocol messages are defined by `.proto` files, these are often the easiest way to understand a message type.\n\nThe `tf.train.Example` message (or protobuf) is a flexible message type that represents a `{\"string\": value}` mapping. It is designed for use with TensorFlow and is used throughout the higher-level APIs such as [TFX](https://www.tensorflow.org/tfx/)."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.09648095533109013"}, "execution_count": null, "source": ["This notebook demonstrates how to create, parse, and use the `tf.train.Example` message, and then serialize, write, and read `tf.train.Example` messages to and from `.tfrecord` files.\n\nNote: While useful, these structures are optional. There is no need to convert existing code to use TFRecords, unless you are [using tf.data](https://www.tensorflow.org/guide/data) and reading data is still the bottleneck to training. You can refer to [Better performance with the tf.data API](https://www.tensorflow.org/guide/data_performance) for dataset performance tips.\n\nNote: In general, you should shard your data across multiple files so that you can parallelize I/O (within a single host or across multiple hosts). The rule of thumb is to have at least 10 times as many files as there will be hosts reading data. At the same time, each file should be large enough (at least 10 MB+ and ideally 100 MB+) so that you can benefit from I/O prefetching. For example, say you have `X` GB of data and you plan to train on up to `N` hosts. Ideally, you should shard the data to ~`10*N` files, as long as ~`X/(10*N)` is 10 MB+ (and ideally 100 MB+). If it is less than that, you might need to create fewer shards to trade off parallelism benefits and I/O prefetching benefits."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.3038196037412546"}, "execution_count": null, "source": ["## Setup"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8998669028085331"}, "execution_count": null, "source": ["import tensorflow as tf\n\nimport numpy\nimport IPython.display as display"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2946619842915672"}, "execution_count": null, "source": ["## `tf.train.Example`"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9856133839969383"}, "execution_count": null, "source": ["### Data types for `tf.train.Example`"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2651656814239314"}, "execution_count": null, "source": ["Fundamentally, a `tf.train.Example` is a `{\"string\": tf.train.Feature}` mapping.\n\nThe `tf.train.Feature` message type can accept one of the following three types (See the [`.proto` file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto) for reference). Most other generic types can be coerced into one of these:\n\n1. `tf.train.BytesList` (the following types can be coerced)\n\n  - `string`\n  - `byte`\n\n1. `tf.train.FloatList` (the following types can be coerced)\n\n  - `float` (`float32`)\n  - `double` (`float64`)\n\n1. `tf.train.Int64List` (the following types can be coerced)\n\n  - `bool`\n  - `enum`\n  - `int32`\n  - `uint32`\n  - `int64`\n  - `uint64`"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.046051705479114746"}, "execution_count": null, "source": ["In order to convert a standard TensorFlow type to a `tf.train.Example`-compatible `tf.train.Feature`, you can use the shortcut functions below. Note that each function takes a scalar input value and returns a `tf.train.Feature` containing one of the three `list` types above:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.12087384930484446"}, "execution_count": null, "source": ["# The following functions can be used to convert a value to a type compatible\n# with tf.train.Example.\n\ndef _bytes_feature(value):\n  \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n  if isinstance(value, type(tf.constant(0))):\n    value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _float_feature(value):\n  \"\"\"Returns a float_list from a float / double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef _int64_feature(value):\n  \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.8704981561229543"}, "execution_count": null, "source": ["Note: To stay simple, this example only uses scalar inputs. The simplest way to handle non-scalar features is to use `tf.io.serialize_tensor` to convert tensors to binary-strings. Strings are scalars in TensorFlow. Use `tf.io.parse_tensor` to convert the binary-string back to a tensor."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.3940909676205444"}, "execution_count": null, "source": ["Below are some examples of how these functions work. Note the varying input types and the standardized output types. If the input type for a function does not match one of the coercible types stated above, the function will raise an exception (e.g. `_int64_feature(1.0)` will error out because `1.0` is a float\u2014therefore, it should be used with the `_float_feature` function instead):"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.10280759651884774"}, "execution_count": null, "source": ["print(_bytes_feature(b'test_string'))\nprint(_bytes_feature(u'test_bytes'.encode('utf-8')))\n\nprint(_float_feature(np.exp(1)))\n\nprint(_int64_feature(True))\nprint(_int64_feature(1))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9612516801099975"}, "execution_count": null, "source": ["All proto messages can be serialized to a binary-string using the `.SerializeToString` method:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.0036923795496730527"}, "execution_count": null, "source": ["feature = _float_feature(np.exp(1))\n\nfeature.SerializeToString()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.17339430494489272"}, "execution_count": null, "source": ["### Creating a `tf.train.Example` message"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.14292908088370537"}, "execution_count": null, "source": ["Suppose you want to create a `tf.train.Example` message from existing data. In practice, the dataset may come from anywhere, but the procedure of creating the `tf.train.Example` message from a single observation will be the same:\n\n1. Within each observation, each value needs to be converted to a `tf.train.Feature` containing one of the 3 compatible types, using one of the functions above.\n\n1. You create a map (dictionary) from the feature name string to the encoded feature value produced in #1.\n\n1. The map produced in step 2 is converted to a [`Features` message](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto#L85)."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.21558347333654848"}, "execution_count": null, "source": ["In this notebook, you will create a dataset using NumPy.\n\nThis dataset will have 4 features:\n\n* a boolean feature, `False` or `True` with equal probability\n* an integer feature uniformly randomly chosen from `[0, 5]`\n* a string feature generated from a string table by using the integer feature as an index\n* a float feature from a standard normal distribution\n\nConsider a sample consisting of 10,000 independently and identically distributed observations from each of the above distributions:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6002809452101063"}, "execution_count": null, "source": ["# The number of observations in the dataset.\nn_observations = int(1e4)\n\n# Boolean feature, encoded as False or True.\nfeature0 = np.random.choice([False, True], n_observations)\n\n# Integer feature, random from 0 to 4.\nfeature1 = np.random.randint(0, 5, n_observations)\n\n# String feature.\nstrings = np.array([b'cat', b'dog', b'chicken', b'horse', b'goat'])\nfeature2 = strings[feature1]\n\n# Float feature, from a standard normal distribution.\nfeature3 = np.random.randn(n_observations)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.3917799940529645"}, "execution_count": null, "source": ["Each of these features can be coerced into a `tf.train.Example`-compatible type using one of `_bytes_feature`, `_float_feature`, `_int64_feature`. You can then create a `tf.train.Example` message from these encoded features:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1305742098675453"}, "execution_count": null, "source": ["def serialize_example(feature0, feature1, feature2, feature3):\n  \"\"\"\n  Creates a tf.train.Example message ready to be written to a file.\n  \"\"\"\n  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n  # data type.\n  feature = {\n      'feature0': _int64_feature(feature0),\n      'feature1': _int64_feature(feature1),\n      'feature2': _bytes_feature(feature2),\n      'feature3': _float_feature(feature3),\n  }\n\n  # Create a Features message using tf.train.Example.\n\n  example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n  return example_proto.SerializeToString()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.21331816340324616"}, "execution_count": null, "source": ["For example, suppose you have a single observation from the dataset, `[False, 4, bytes('goat'), 0.9876]`. You can create and print the `tf.train.Example` message for this observation using `create_message()`. Each single observation will be written as a `Features` message as per the above. Note that the `tf.train.Example` [message](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto#L88) is just a wrapper around the `Features` message:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6312399759948573"}, "execution_count": null, "source": ["# This is an example observation from the dataset.\n\nexample_observation = []\n\nserialized_example = serialize_example(False, 4, b'goat', 0.9876)\nserialized_example"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4115983240085359"}, "execution_count": null, "source": ["To decode the message use the `tf.train.Example.FromString` method."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5372114485374226"}, "execution_count": null, "source": ["example_proto = tf.train.Example.FromString(serialized_example)\nexample_proto"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6445618001057605"}, "execution_count": null, "source": ["## TFRecords format details\n\nA TFRecord file contains a sequence of records. The file can only be read sequentially.\n\nEach record contains a byte-string, for the data-payload, plus the data-length, and  CRC-32C ([32-bit CRC](https://en.wikipedia.org/wiki/Cyclic_redundancy_check#CRC-32_algorithm) using the [Castagnoli polynomial](https://en.wikipedia.org/wiki/Cyclic_redundancy_check#Standards_and_common_use)) hashes for integrity checking.\n\nEach record is stored in the following formats:\n\n    uint64 length\n    uint32 masked_crc32_of_length\n    byte   data[length]\n    uint32 masked_crc32_of_data\n\nThe records are concatenated together to produce the file. CRCs are\n[described here](https://en.wikipedia.org/wiki/Cyclic_redundancy_check), and\nthe mask of a CRC is:\n\n    masked_crc = ((crc >> 15) | (crc << 17)) + 0xa282ead8ul\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7260453204412249"}, "execution_count": null, "source": ["Note: There is no requirement to use `tf.train.Example` in TFRecord files. `tf.train.Example` is just a method of serializing dictionaries to byte-strings. Any byte-string that can be decoded in TensorFlow could be stored in a TFRecord file. Examples include: lines of text, JSON (using `tf.io.decode_json_example`), encoded image data, or serialized `tf.Tensors` (using `tf.io.serialize_tensor`/`tf.io.parse_tensor`). See the `tf.io` module for more options."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.034928450442804015"}, "execution_count": null, "source": ["## TFRecord files using `tf.data`"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9675700993356962"}, "execution_count": null, "source": ["The `tf.data` module also provides tools for reading and writing data in TensorFlow."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9342899399023359"}, "execution_count": null, "source": ["### Writing a TFRecord file\n\nThe easiest way to get the data into a dataset is to use the `from_tensor_slices` method.\n\nApplied to an array, it returns a dataset of scalars:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.25872604299523805"}, "execution_count": null, "source": ["tf.data.Dataset.from_tensor_slices(feature1)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2270345199081527"}, "execution_count": null, "source": ["Applied to a tuple of arrays, it returns a dataset of tuples:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.01654296931728161"}, "execution_count": null, "source": ["features_dataset = tf.data.Dataset.from_tensor_slices((feature0, feature1, feature2, feature3))\nfeatures_dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.20774162330115198"}, "execution_count": null, "source": ["# Use `take(1)` to only pull one example from the dataset.\nfor f0,f1,f2,f3 in features_dataset.take(1):\n  print(f0)\n  print(f1)\n  print(f2)\n  print(f3)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4579395373170376"}, "execution_count": null, "source": ["Use the `tf.data.Dataset.map` method to apply a function to each element of a `Dataset`.\n\nThe mapped function must operate in TensorFlow graph mode\u2014it must operate on and return `tf.Tensors`. A non-tensor function, like `serialize_example`, can be wrapped with `tf.py_function` to make it compatible.\n\nUsing `tf.py_function` requires to specify the shape and type information that is otherwise unavailable:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.27851987227083796"}, "execution_count": null, "source": ["def tf_serialize_example(f0,f1,f2,f3):\n  tf_string = tf.py_function(\n    serialize_example,\n    (f0, f1, f2, f3),  # Pass these args to the above function.\n    tf.string)      # The return type is `tf.string`.\n  return tf.reshape(tf_string, ()) # The result is a scalar."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.09598579019812781"}, "execution_count": null, "source": ["tf_serialize_example(f0, f1, f2, f3)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.050036469038879705"}, "execution_count": null, "source": ["Apply this function to each element in the dataset:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7779301197657664"}, "execution_count": null, "source": ["serialized_features_dataset = features_dataset.map(tf_serialize_example)\nserialized_features_dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.957150965432225"}, "execution_count": null, "source": ["def generator():\n    #ggaga\n  for features in features_dataset:\n    yield serialize_example(*features)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9737794154356676"}, "execution_count": null, "source": ["serialized_features_dataset = tf.data.Dataset.from_generator(\n    generator, output_types=tf.string, output_shapes=())"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9360712635900099"}, "execution_count": null, "source": ["serialized_features_dataset"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.0594776989057888"}, "execution_count": null, "source": ["And write them to a TFRecord file:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.09548830656086893"}, "execution_count": null, "source": ["filename = 'test.tfrecord'\nwriter = tf.data.experimental.TFRecordWriter(filename)\nwriter.write(serialized_features_dataset)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.15078165466853477"}, "execution_count": null, "source": ["### Reading a TFRecord file"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5797479946669009"}, "execution_count": null, "source": ["You can also read the TFRecord file using the `tf.data.TFRecordDataset` class.\n\nMore information on consuming TFRecord files using `tf.data` can be found in the [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data#consuming_tfrecord_data) guide.\n\nUsing `TFRecordDataset`s can be useful for standardizing input data and optimizing performance."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7654125024595724"}, "execution_count": null, "source": ["filenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.735085239501015"}, "execution_count": null, "source": ["At this point the dataset contains serialized `tf.train.Example` messages. When iterated over it returns these as scalar string tensors.\n\nUse the `.take` method to only show the first 10 records.\n\nNote: iterating over a `tf.data.Dataset` only works with eager execution enabled."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7034961934058783"}, "execution_count": null, "source": ["for raw_record in raw_dataset.take(10):\n  print(repr(raw_record))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6407290306816347"}, "execution_count": null, "source": ["These tensors can be parsed using the function below. Note that the `feature_description` is necessary here because `tf.data.Dataset`s use graph-execution, and need this description to build their shape and type signature:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3838168915232816"}, "execution_count": null, "source": ["# Create a description of the features.\nfeature_description = {\n    'feature0': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature1': tf.io.FixedLenFeature([], tf.int64, default_value=0),\n    'feature2': tf.io.FixedLenFeature([], tf.string, default_value=''),\n    'feature3': tf.io.FixedLenFeature([], tf.float32, default_value=0.0),\n}\n\ndef _parse_function(example_proto):\n  # Parse the input `tf.train.Example` proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, feature_description)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7729443328440659"}, "execution_count": null, "source": ["Alternatively, use `tf.parse example` to parse the whole batch at once. Apply this function to each item in the dataset using the `tf.data.Dataset.map` method:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.19576711464449792"}, "execution_count": null, "source": ["parsed_dataset = raw_dataset.map(_parse_function)\nparsed_dataset"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2869642668890984"}, "execution_count": null, "source": ["Use eager execution to display the observations in the dataset. There are 10,000 observations in this dataset, but you will only display the first 10. The data is displayed as a dictionary of features. Each item is a `tf.Tensor`, and the `numpy` element of this tensor displays the value of the feature:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6863517560399568"}, "execution_count": null, "source": ["for parsed_record in parsed_dataset.take(10):\n  print(repr(parsed_record))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.9269841963185252"}, "execution_count": null, "source": ["Here, the `tf.parse_example` function unpacks the `tf.train.Example` fields into standard tensors."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6771240158372445"}, "execution_count": null, "source": ["## TFRecord files in Python"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.0742890441687527"}, "execution_count": null, "source": ["The `tf.io` module also contains pure-Python functions for reading and writing TFRecord files."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7276855594068994"}, "execution_count": null, "source": ["### Writing a TFRecord file"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4797495396879561"}, "execution_count": null, "source": ["Next, write the 10,000 observations to the file `test.tfrecord`. Each observation is converted to a `tf.train.Example` message, then written to file. You can then verify that the file `test.tfrecord` has been created:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9895469238032764"}, "execution_count": null, "source": ["# Write the `tf.train.Example` observations to the file.\nwith tf.io.TFRecordWriter(filename) as writer:\n  for i in range(n_observations):\n    example = serialize_example(feature0[i], feature1[i], feature2[i], feature3[i])\n    writer.write(example)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.0772919153297027"}, "execution_count": null, "source": ["!du -sh {filename}"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4553387882387423"}, "execution_count": null, "source": ["### Reading a TFRecord file\n\nThese serialized tensors can be easily parsed using `tf.train.Example.ParseFromString`:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.2502510730559342"}, "execution_count": null, "source": ["filenames = [filename]\nraw_dataset = tf.data.TFRecordDataset(filenames)\nraw_dataset"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7674559284760589"}, "execution_count": null, "source": ["for raw_record in raw_dataset.take(1):\n  example = tf.train.Example()\n  example.ParseFromString(raw_record.numpy())\n  print(example)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.6579807009960059"}, "execution_count": null, "source": ["That returns a `tf.train.Example` proto which is dificult to use as is, but it's fundamentally a representation of a:\n\n```\nDict[str,\n     Union[List[float],\n           List[int],\n           List[str]]]\n```\n\nThe following code manually converts the `Example` to a dictionary of NumPy arrays, without using TensorFlow Ops. Refer to [the PROTO file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto) for detials."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9336709394186227"}, "execution_count": null, "source": ["result = {}\n# example.features.feature is the dictionary\nfor key, feature in example.features.feature.items():\n  # The values are the Feature objects which contain a `kind` which contains:\n  # one of three fields: bytes_list, float_list, int64_list\n\n  kind = feature.WhichOneof('kind')\n  result[key] = np.array(getattr(feature, kind).value)\n\nresult"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.5626159717573489"}, "execution_count": null, "source": ["## Walkthrough: Reading and writing image data"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.14972547876597075"}, "execution_count": null, "source": ["This is an end-to-end example of how to read and write image data using TFRecords. Using an image as input data, you will write the data as a TFRecord file, then read the file back and display the image.\n\nThis can be useful if, for example, you want to use several models on the same input dataset. Instead of storing the image data raw, it can be preprocessed into the TFRecords format, and that can be used in all further processing and modelling.\n\nFirst, let's download [this image](https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg) of a cat in the snow and [this photo](https://upload.wikimedia.org/wikipedia/commons/f/fe/New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg) of the Williamsburg Bridge, NYC under construction."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.770030075427903"}, "execution_count": null, "source": ["### Fetch the images"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3540928036990938"}, "execution_count": null, "source": ["cat_in_snow  = tf.keras.utils.get_file(\n    '320px-Felis_catus-cat_on_snow.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/320px-Felis_catus-cat_on_snow.jpg')\n\nwilliamsburg_bridge = tf.keras.utils.get_file(\n    '194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg',\n    'https://storage.googleapis.com/download.tensorflow.org/example_images/194px-New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6112769860656773"}, "execution_count": null, "source": ["display.display(display.Image(filename=cat_in_snow))\ndisplay.display(display.HTML('Image cc-by: <a \"href=https://commons.wikimedia.org/wiki/File:Felis_catus-cat_on_snow.jpg\">Von.grzanka</a>'))"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.20606600590994506"}, "execution_count": null, "source": ["display.display(display.Image(filename=williamsburg_bridge))\ndisplay.display(display.HTML('<a \"href=https://commons.wikimedia.org/wiki/File:New_East_River_Bridge_from_Brooklyn_det.4a09796u.jpg\">From Wikimedia</a>'))"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.47993598244166025"}, "execution_count": null, "source": ["### Write the TFRecord file"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.2959141758556665"}, "execution_count": null, "source": ["As before, encode the features as types compatible with `tf.train.Example`. This stores the raw image string feature, as well as the height, width, depth, and arbitrary `label` feature. The latter is used when you write the file to distinguish between the cat image and the bridge image. Use `0` for the cat image, and `1` for the bridge image:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9955094828141047"}, "execution_count": null, "source": ["image_labels = {\n    cat_in_snow : 0,\n    williamsburg_bridge : 1,\n}"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.47834298732829494"}, "execution_count": null, "source": ["# This is an example, just using the cat image.\nimage_string = open(cat_in_snow, 'rb').read()\n\nlabel = image_labels[cat_in_snow]\n\n# Create a dictionary with features that may be relevant.\ndef image_example(image_string, label):\n  image_shape = tf.io.decode_jpeg(image_string).shape\n\n  feature = {\n      'height': _int64_feature(image_shape[0]),\n      'width': _int64_feature(image_shape[1]),\n      'depth': _int64_feature(image_shape[2]),\n      'label': _int64_feature(label),\n      'image_raw': _bytes_feature(image_string),\n  }\n\n  return tf.train.Example(features=tf.train.Features(feature=feature))\n\nfor line in str(image_example(image_string, label)).split('\\n')[:15]:\n  print(line)\nprint('...')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.4969620684091731"}, "execution_count": null, "source": ["Notice that all of the features are now stored in the `tf.train.Example` message. Next, functionalize the code above and write the example messages to a file named `images.tfrecords`:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.5726025811643465"}, "execution_count": null, "source": ["# Write the raw image files to `images.tfrecords`.\n# First, process the two images into `tf.train.Example` messages.\n# Then, write to a `.tfrecords` file.\nrecord_file = 'images.tfrecords'\nwith tf.io.TFRecordWriter(record_file) as writer:\n  for filename, label in image_labels.items():\n    image_string = open(filename, 'rb').read()\n    tf_example = image_example(image_string, label)\n    writer.write(tf_example.SerializeToString())"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.8947932760923452"}, "execution_count": null, "source": ["!du -sh {record_file}"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.7172070444173873"}, "execution_count": null, "source": ["### Read the TFRecord file\n\nYou now have the file\u2014`images.tfrecords`\u2014and can now iterate over the records in it to read back what you wrote. Given that in this example you will only reproduce the image, the only feature you will need is the raw image string. Extract it using the getters described above, namely `example.features.feature['image_raw'].bytes_list.value[0]`. You can also use the labels to determine which record is the cat and which one is the bridge:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6374977760586453"}, "execution_count": null, "source": ["raw_image_dataset = tf.data.TFRecordDataset('images.tfrecords')\n\n# Create a dictionary describing the features.\nimage_feature_description = {\n    'height': tf.io.FixedLenFeature([], tf.int64),\n    'width': tf.io.FixedLenFeature([], tf.int64),\n    'depth': tf.io.FixedLenFeature([], tf.int64),\n    'label': tf.io.FixedLenFeature([], tf.int64),\n    'image_raw': tf.io.FixedLenFeature([], tf.string),\n}\n\ndef _parse_image_function(example_proto):\n  # Parse the input tf.train.Example proto using the dictionary above.\n  return tf.io.parse_single_example(example_proto, image_feature_description)\n\nparsed_image_dataset = raw_image_dataset.map(_parse_image_function)\nparsed_image_dataset"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "0_0.017902977874969528"}, "execution_count": null, "source": ["Recover the images from the TFRecord file:"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.7319181100336334"}, "execution_count": null, "source": ["for image_features in parsed_image_dataset:\n  image_raw = image_features['image_raw'].numpy()\n  display.display(display.Image(data=image_raw))"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}