{"cells": [{"cell_type": "code", "metadata": {"id": "0_0.2073223916128628"}, "execution_count": null, "source": ["@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/tfrecord.ipynb'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/yaml_file.js'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/yaml_file.md'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/yaml_file.ini'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/yaml_file.toml'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/yaml_file.yaml'\n@SYS.DATASANDBOX_PATH + '2472457046/Repo/Import_Files/Subdir/jupyter_test.py'"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.23708710177254955"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = model, modelName = 'Model_Dev_Sklearn_V1', modelType = 'ml', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.3742967054481985"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nloaded_model = nb.load_saved_model(projectId = '684851203', modelId = '732430338')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6787519225086602"}, "execution_count": null, "source": ["X_test_copy = X_test"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.36059769162079136"}, "execution_count": null, "source": ["nb.predict(model = loaded_model, dataframe = X_test_copy,modeltype='ml') \n #provide model and dataframe"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.1913395828620963"}, "execution_count": null, "source": ["import tensorflow"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.6567866525303787"}, "execution_count": null, "source": ["tensorflow.__version__"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "0_0.9784847957293759"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Covid_data = nb.get_data('11111706181363529', '@SYS.USERID','True', {},['11111706693784870'])\ndf_Covid_data\n#The first function parameter refers to the service ID of the dataset.\n#@SYS.USERID refers to the user ID of the current user.\n#If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n#{} refers to the filters applied to the dataset.\n#[] refers to the data preparations applied to the dataset.\n#After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator."], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}