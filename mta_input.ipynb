{"cells": [{"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["from datetime import datetime, timedelta\n", "import json\n", "import logging\n", "logging.basicConfig()\n", "logger = logging.getLogger('MTA')\n", "logger.setLevel(logging.INFO)\n", "\n", "import boto3\n", "import pandas as pd\n", "import psycopg2\n", "import base64\n", "\n", "\n", "def get_redshift_secret():\n", "    global AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REGION_NAME, SECRET_NAME\n", "    # connects to AWS secrets manager to get credentials\n", "    session = boto3.session.Session()\n", "    \n", "    client = session.client(\n", "        service_name='secretsmanager', region_name=REGION_NAME, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n", "\n", "    try:\n", "        get_secret_value_response = client.get_secret_value(SecretId=SECRET_NAME)\n", "    except Exception as e:\n", "        logger.error(\"ERROR in getting redshift secrets\")\n", "        raise\n", "    else:\n", "        if 'SecretString' in get_secret_value_response:\n", "            secret = get_secret_value_response['SecretString']\n", "        else:\n", "            decoded_binary_secret = base64.b64decode(\n", "                get_secret_value_response['SecretBinary'])\n", "    return json.loads(secret)\n", "\n", "\n", "def get_redshift_connection():\n", "    # creates data base connection and passes it back to caller\n", "    try:\n", "        secrets = get_redshift_secret()\n", "        conn = psycopg2.connect(dbname=secrets['DATABASE'],\n", "                                host=secrets['HOST'],\n", "                                port=secrets['PORT'],\n", "                                user=secrets['USER'],\n", "                                password=secrets['PASSWORD'])\n", "        return conn\n", "\n", "    except Exception as e:\n", "        logger.error(f'Error getting redshift connection details: Exception: {str(e)}')\n", "        raise\n", "\n", "        \n", "def repair_table(client, date):\n", "    global ATHENA_DATABASE, ATHENA_TABLE, ATHENA_STATUS_OUTPUT_LOCATION, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REGION_NAME\n", "    # Create an Athena client\n", "    athena = boto3.client('athena', region_name=REGION_NAME, aws_access_key_id=AWS_ACCESS_KEY_ID, aws_secret_access_key=AWS_SECRET_ACCESS_KEY)\n", "\n", "    # Specify the Athena database and table name\n", "    database_name = ATHENA_DATABASE\n", "    table_name = ATHENA_TABLE\n", "    status_output_location = ATHENA_STATUS_OUTPUT_LOCATION\n", "\n", "    # Define the SQL query to run the MSCK repair operation\n", "    table_repair_query = f\"ALTER TABLE {database_name}.{table_name} ADD IF NOT EXISTS PARTITION (advertiser_name = '{client}', event_date = '{date}')\"\n", "\n", "    # Start the query execution\n", "    response = athena.start_query_execution(\n", "        QueryString=table_repair_query,\n", "        QueryExecutionContext={\n", "            'Database': database_name\n", "        },\n", "        ResultConfiguration={\n", "            'OutputLocation': status_output_location,  # Specify the S3 bucket for query results\n", "        }\n", "    )\n", "    # Get the query execution ID\n", "    query_execution_id = response['QueryExecutionId']\n", "    logger.info(f\"Started table repair for table with query execution ID: {query_execution_id}\")\n", "    \n", "    # Wait for the query to complete\n", "    while True:\n", "        query_status = athena.get_query_execution(QueryExecutionId=query_execution_id)\n", "        status = query_status['QueryExecution']['Status']['State']\n", "\n", "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n", "            break\n", "\n", "    # Retrieve and return the query results if the query succeeded\n", "    if status == 'SUCCEEDED':\n", "#         results = athena.get_query_results(QueryExecutionId=query_execution_id)\n", "#         results = format_athena_results(results)\n", "        logger.debug(f'Athena query ran successfully')\n", "#         return results\n", "    elif status == 'FAILED':\n", "        raise Exception('Query from Athena failed')\n", "        \n", "        \n", "def prepare_input(date_delta, lookback_days, media_type, branch, kpi_table, channel_table\n", "    ):\n", "    logger.info('Preparing client KPI input')\n", "    pixel_date = (datetime.now() + timedelta(days=-date_delta)).date().isoformat()\n", "    if pixel_date.lower() == \"yesterday\":\n", "        new_pixel_date = datetime.now() + timedelta(days=-1)\n", "        new_pixel_date_string = f'{new_pixel_date.year}-{str(\"{:02d}\".format(new_pixel_date.month))}-{str(\"{:02d}\".format(new_pixel_date.day))}'\n", "\n", "        new_pixel_p1_date = datetime.now()\n", "        new_pixel_p1_date_string = f'{new_pixel_p1_date.year}-{str(\"{:02d}\".format(new_pixel_p1_date.month))}-{str(\"{:02d}\".format(new_pixel_p1_date.day))}'\n", "\n", "        logger.info(f\"Changing Pixel Date to {new_pixel_date_string}\")\n", "        pixel_dt = datetime(\n", "            year=int(new_pixel_date_string.split(\"-\")[0]),\n", "            month=int(new_pixel_date_string.split(\"-\")[1]),\n", "            day=int(new_pixel_date_string.split(\"-\")[2]),\n", "            hour=0,\n", "        )\n", "        pixel_p1_dt = datetime(\n", "            year=int(new_pixel_p1_date_string.split(\"-\")[0]),\n", "            month=int(new_pixel_p1_date_string.split(\"-\")[1]),\n", "            day=int(new_pixel_p1_date_string.split(\"-\")[2]),\n", "            hour=0,\n", "        )\n", "    else:\n", "        pixel_dt = datetime(\n", "            year=int(pixel_date.split(\"-\")[0]),\n", "            month=int(pixel_date.split(\"-\")[1]),\n", "            day=int(pixel_date.split(\"-\")[2]),\n", "            hour=0,\n", "        )\n", "        pixel_p1_dt = pixel_dt + timedelta(days=1)\n", "\n", "    # Get Impression window start date using pixel date and lookback days\n", "    #lookback_days = -1 * lookback_days\n", "    impression_dt = pixel_dt - timedelta(days=lookback_days)\n", "\n", "    # Pixel / Impression Lookback datetime formatted strings\n", "    pixel_dts = f'{pixel_dt.year}-{str(\"{:02d}\".format(pixel_dt.month))}-{str(\"{:02d}\".format(pixel_dt.day))} 00:00:00'\n", "    pixel_p1_dts = f'{pixel_p1_dt.year}-{str(\"{:02d}\".format(pixel_p1_dt.month))}-{str(\"{:02d}\".format(pixel_p1_dt.day))} 00:00:00'\n", "    impression_dts = f'{impression_dt.year}-{str(\"{:02d}\".format(impression_dt.month))}-{str(\"{:02d}\".format(impression_dt.day))} 00:00:00'\n", "\n", "    client_kpi_input = kpi_table.join(channel_table.set_index('clientkey_map'), on='clientkey_map')\n", "    client_kpi_input[['impression_dts','pixel_dts','media_type','lookback_days','branch']] = pd.DataFrame([[impression_dts, pixel_dts, media_type, lookback_days, branch]], index=client_kpi_input.index)\n", "    client_kpi_input.loc[(client_kpi_input['enabled_mta_channel'] == False) & (client_kpi_input['enabled_mta_ott'] == True), 'media_type'] = 'ott'\n", "    client_kpi_input.loc[(client_kpi_input['enabled_mta_channel'] == False) & (client_kpi_input['enabled_mta_audio'] == True), 'media_type'] = 'audio'\n", "    client_kpi_input.loc[(client_kpi_input['enabled_mta_channel'] == False) & (client_kpi_input['enabled_mta_digital'] == True), 'media_type'] = 'digital'\n", "    client_kpi_input = client_kpi_input.rename(columns={'clientkey_map':'clientkey', 'kpi_name_map':'kpi', 'enabled_mta_digital':'include_channel_digital', 'enabled_mta_ott':'include_channel_ott', 'enabled_mta_audio':'include_channel_audio'})\n", "    return client_kpi_input\n", "\n", "\n", "def start_func(df, aws_access_key_id, aws_secret_access_key, region_name, secret_name, \n", "        date_delta, lookback_days, media_type, \n", "        branch, athena_database, athena_status_output_location\n", "    ):\n", "    athena_table = 'pixels'\n", "    global AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REGION_NAME, SECRET_NAME, conn, cursor, ATHENA_DATABASE, ATHENA_TABLE, ATHENA_STATUS_OUTPUT_LOCATION\n", "    AWS_ACCESS_KEY_ID = aws_access_key_id\n", "    AWS_SECRET_ACCESS_KEY = aws_secret_access_key\n", "    REGION_NAME = region_name\n", "    SECRET_NAME = secret_name\n", "    ATHENA_DATABASE = athena_database\n", "    ATHENA_TABLE = athena_table\n", "    ATHENA_STATUS_OUTPUT_LOCATION = athena_status_output_location\n", "    \n", "    conn = get_redshift_connection()\n", "    cursor = conn.cursor() \n", "    logger.info('Fetching data from tables')\n", "  \n", "    query1 = f\"\"\"SELECT\n", "        DISTINCT(k.clientkey_map),\n", "        k.kpi_name_map\n", "        FROM portal_global_settings.kpi_mappings k\n", "        LEFT JOIN portal_global_settings.client_mappings c ON(c.clientkey_map = k.clientkey_map)\n", "        WHERE k.media_type IN('ott')\n", "        AND k.enabled = true\n", "        AND c.client_status != 'inactive'\n", "        GROUP BY 1,2\n", "        ORDER BY 1,2;\"\"\"\n", "    kpi_list = pd.read_sql(query1,conn)\n", "\n", "    logger.info(f\"Fetched client-kpi mappings for {len(kpi_list['clientkey_map'].unique())} clients\")\n", "    \n", "    query2 = f\"\"\"SELECT  DISTINCT clientkey_map, enabled_mta_channel, enabled_mta_ott, enabled_mta_audio, enabled_mta_digital, enabled_mta \n", "        FROM portal_global_settings.client_mappings c\n", "        WHERE c.client_status != 'inactive'\n", "    \"\"\"\n", "    channel_results = pd.read_sql(query2,conn)\n", "    logger.info(f\"Fetched channel settings for {len(channel_results['clientkey_map'].unique())} clients\")\n", "    \n", "    if str(branch) not in [\"dev\", \"master\"]:\n", "        logger.error(\"ERROR: Invalid branch (dev, master)\")\n", "        raise ValueError('Invalid branch (dev, master)')\n", "\n", "    results = prepare_input(int(date_delta), int(lookback_days), media_type, branch, kpi_list, channel_results)\n", "    logger.info(f\"Sending final input data for MTA processing for {len(results['clientkey'].unique())} clients\")\n", "\n", "    results = results[~results['clientkey'].isin(('demo', 'varsitytutors', 'singlecare'))].reset_index(drop=True)\n", "    results.loc[results['clientkey'] == 'overstock', 'include_channel_digital'] = False\n", "\n", "    pixel_date = results.loc[0, 'pixel_dts'][:10]\n", "    for client in results['clientkey'].unique():\n", "        repair_table(client, pixel_date)\n", "    return results"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.12"}}, "nbformat": 4, "nbformat_minor": 2}