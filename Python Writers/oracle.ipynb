{"cells": [{"cell_type": "code", "metadata": {"id": "72450055_0.154180578052308"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [])\ndf\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "72450055_0.23925513275116583"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nnb.oracle_writer('qauser', 'qauser_123', 'bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com', '1521', 'ORCL', 'Oracle_Append_Test_v1', df)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "72450055_0.6822201387450044"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [])\ndf_Doha_Data\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "72450055_0.8976123594516112"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\n# Calculate the total number of rows in the DataFrame\ntotal_rows = len(df_Doha_Data)\n# Define the batch size\nbatch_size = 5000\n# Calculate the number of batches\nnum_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)\n# Iterate over the DataFrame in batches\nfor batch_num in range(num_batches):\n    # Calculate the start and end indices of the current batch\n    start_idx = batch_num * batch_size\n    end_idx = min((batch_num + 1) * batch_size, total_rows)\n    \n    # Extract the current batch DataFrame\n    batch_df = df_Doha_Data.iloc[start_idx:end_idx]\n    \n    # Write the current batch DataFrame to MySQL database\n    nb.oracle_writer('qauser', 'qauser_123', 'bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com', '1521', 'ORCL', 'Oracle_Append_1586461', batch_df)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "72450055_0.6213312389676993"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_list = [df_Doha_Data]\nfor df in df_list:\n    # Write each DataFrame to MySQL database\n     nb.oracle_writer('qauser', 'qauser_123', 'bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com', '1521', 'ORCL', 'Oracle_Append_1.6M_v1', df)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433208_0.357721935645406"}, "execution_count": null, "source": ["from datetime import time\nfrom sqlalchemy.dialects import oracle\nfrom sqlalchemy import create_engine\nimport numpy as np\ndef oracle_writer(username, password, host, port, database, tablename, df):\n    \"\"\"    Writes a DataFrame to a oracle database.     \"\"\"\n    try:\n        port = str(port)\n        for column in df.columns:\n            if df[column].apply(lambda x: isinstance(x, time)).any():\n                df[column] = df[column].apply(lambda x: x.isoformat() if isinstance(x, time) else x)\n        dtype_mapping = {}\n        for column in df.select_dtypes(include=['float64']).columns:\n            dtype_mapping[column] = oracle.NUMBER(precision=18, scale=9)\n        # Adjust mapping for numpy types\n        for column in df.select_dtypes(include=[np.int64]).columns:\n            dtype_mapping[column] = oracle.NUMBER()\n        print(dtype_mapping)\n        engine = create_engine(\n            \"oracle+cx_oracle://\" + username + \":\" + password + \"@\" + host + \":\" + str(port) + \"/\" + database)\n        df.to_sql(name=tablename, con=engine, if_exists='append', index=False, dtype=dtype_mapping)\n        print(\"Dataframe has been written to database.\")\n    except Exception as e:\n        print(\"**\")\n        print(e)\n        raise Exception(e)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433208_0.8139616894797463"}, "execution_count": null, "source": ["df_Doha_Data.drop(['SCORE_VALUE', 'CALCULATED_SCORE_VALUE'], axis = 1)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433208_0.5596646956152955"}, "execution_count": null, "source": ["# Calculate the total number of rows in the DataFrame\ntotal_rows = len(df_Doha_Data)\n# Define the batch size\nbatch_size = 5000\n\n# Calculate the number of batches\nnum_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)\n\n# Iterate over the DataFrame in batches\nfor batch_num in range(num_batches):\n    # Calculate the start and end indices of the current batch\n    start_idx = batch_num * batch_size\n    end_idx = min((batch_num + 1) * batch_size, total_rows)\n    print(\"--\")\n    print(start_idx)\n    print(end_idx)\n    \n   \n    # Extract the current batch DataFrame\n    batch_df = df_Doha_Data.iloc[start_idx:end_idx]\n    print(\"--\")\n    # Write the current batch DataFrame to MySQL database\n    oracle_writer('qauser', 'qauser_123', 'bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com', '1521', 'ORCL', 'testlarge_v1', batch_df)"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433208_0.410163569349006"}, "execution_count": null, "source": ["from sqlalchemy import create_engine\n\ndef oracle_writer(username, password, host, port, database, tablename, df):\n    \"\"\"Writes a DataFrame to an Oracle database.\"\"\"\n    try:\n        engine = create_engine(f\"oracle+cx_oracle://{username}:{password}@{host}:{port}/{database}\")\n        df.to_sql(name=tablename, con=engine, if_exists='append', index=False)\n        print(\"Dataframe has been written to database....\")\n    except Exception as e:\n        print(\"** An error occurred while writing to the database:\")\n        print(e)\n        raise Exception(e)\n\n# Example usage:\n# oracle_writer('username', 'password', 'localhost', '1521', 'dbname', 'tablename', df)\n"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433208_0.48944317590105735"}, "execution_count": null, "source": ["# Calculate the total number of rows in the DataFrame\ntotal_rows = len(df_Doha_Data)\n# Define the batch size\nbatch_size = 5000\n\n# Calculate the number of batches\nnum_batches = (total_rows // batch_size) + (1 if total_rows % batch_size != 0 else 0)\n\n# Iterate over the DataFrame in batches\nfor batch_num in range(num_batches):\n    # Calculate the start and end indices of the current batch\n    start_idx = batch_num * batch_size\n    end_idx = min((batch_num + 1) * batch_size, total_rows)\n    print(\"--\")\n    print(start_idx)\n    print(end_idx)\n    \n   \n    # Extract the current batch DataFrame\n    batch_df = df_Doha_Data.iloc[start_idx:end_idx]\n    print(\"--\")\n    # Write the current batch DataFrame to MySQL database\n    oracle_writer('qauser', 'qauser_123', 'bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com', '1521', 'ORCL', 'testlarge_v1', batch_df)"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}