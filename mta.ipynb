{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "from io import BytesIO\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "from time import perf_counter\n",
    "import typing\n",
    "import re\n",
    "import logging\n",
    "logging.basicConfig()\n",
    "logger = logging.getLogger('MTA')\n",
    "logging_level = logging.INFO\n",
    "logger.setLevel(logging_level)\n",
    "\n",
    "import awswrangler as wr\n",
    "import boto3\n",
    "import numpy as np\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import uuid\n",
    "\n",
    "\n",
    "def func_time(func):\n",
    "    logger = logging.getLogger(f'FUNC_{func.__name__}')\n",
    "    if 'logging_level' in globals():\n",
    "        logger.setLevel(logging_level)\n",
    "    else:\n",
    "        logger.setLevel(logging.INFO)\n",
    "\n",
    "    def decorated(*args, **kwargs):\n",
    "        func_start_time = perf_counter()\n",
    "        results = func(*args, **kwargs)\n",
    "        logger.debug(f'Finished running; {func.__name__}. Took {round(perf_counter() - func_start_time, 3)} seconds')\n",
    "        return results\n",
    "    return decorated\n",
    "\n",
    "\n",
    "class RedshiftConfig:\n",
    "    def __init__(self, REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REDSHIFT_MTA_SECRET_NAME):\n",
    "        self.REGION_NAME = REGION_NAME\n",
    "        self.AWS_ACCESS_KEY_ID = AWS_ACCESS_KEY_ID\n",
    "        self.AWS_SECRET_ACCESS_KEY = AWS_SECRET_ACCESS_KEY\n",
    "        self.REDSHIFT_MTA_SECRET_NAME = REDSHIFT_MTA_SECRET_NAME\n",
    "        self.connection = self.create_redshift_connection()\n",
    "        self.cursor = None\n",
    "    \n",
    "    def create_redshift_connection(self):\n",
    "        session = boto3.session.Session()\n",
    "        client = session.client(\n",
    "            service_name=\"secretsmanager\",\n",
    "            region_name=self.REGION_NAME,\n",
    "            aws_access_key_id=self.AWS_ACCESS_KEY_ID,\n",
    "            aws_secret_access_key=self.AWS_SECRET_ACCESS_KEY\n",
    "        )\n",
    "        secret_value = client.get_secret_value(SecretId=self.REDSHIFT_MTA_SECRET_NAME)\n",
    "        credentials = json.loads(secret_value[\"SecretString\"])\n",
    "        conn = psycopg2.connect(\n",
    "            host=credentials[\"HOST\"],\n",
    "            port=credentials[\"PORT\"],\n",
    "            database=credentials[\"DATABASE\"],\n",
    "            user=credentials[\"USER\"],\n",
    "            password=credentials[\"PASSWORD\"],\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        return conn\n",
    "    \n",
    "    def get_connection(self):\n",
    "        if self.connection.closed:\n",
    "            self.connection = self.create_redshift_connection()\n",
    "        return self.connection\n",
    "    \n",
    "    def get_cursor(self):\n",
    "        if not self.cursor or self.cursor.closed:\n",
    "            try:\n",
    "                self.cursor = self.connection.cursor()\n",
    "            except psycopg2.InterfaceError:\n",
    "                self.connection = self.get_connection()\n",
    "                self.cursor = self.connection.cursor()\n",
    "        return self.cursor\n",
    "\n",
    "\n",
    "@func_time\n",
    "def run_redshift_query(query):\n",
    "    global redshift_config\n",
    "    redshift_conn = redshift_config.get_connection()\n",
    "    logger.debug('Starting Redshift query execution')\n",
    "    logger.debug(f'Redshift query:\\n{query}')\n",
    "    df = pd.read_sql(query, redshift_conn)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_athena_connection(REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY):\n",
    "    athena_client = boto3.client(\n",
    "        'athena', \n",
    "        region_name=REGION_NAME, \n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID, \n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "    return athena_client\n",
    "\n",
    "\n",
    "def format_athena_results(query_results):\n",
    "    column_names = [col['Name'] for col in query_results['ResultSet']['ResultSetMetadata']['ColumnInfo']]\n",
    "    data = []\n",
    "    for row in query_results['ResultSet']['Rows'][1:]:   \n",
    "        values = [val['VarCharValue'] if 'VarCharValue' in val else None for val in row['Data']]     \n",
    "        data.append(values)\n",
    "    return pd.DataFrame(data, columns=column_names)\n",
    "\n",
    "@func_time\n",
    "def run_athena_query(query, only_metadata=False):\n",
    "    global S3, ATHENA_CLIENT, ATHENA_DATABASE, ATHENA_STATUS_OUTPUT_LOCATION, S3_TARGET_BUCKET_PARQUET\n",
    "    logger.debug(f'Starting Athena query execution')\n",
    "    logger.debug(f'Athena query:\\n{query}')\n",
    "    response = ATHENA_CLIENT.start_query_execution(\n",
    "        QueryString=query,\n",
    "        QueryExecutionContext={'Database': ATHENA_DATABASE},\n",
    "        ResultConfiguration={'OutputLocation': ATHENA_STATUS_OUTPUT_LOCATION},\n",
    "        WorkGroup='juice_data'\n",
    "    )\n",
    "    \n",
    "    # Get the query execution ID\n",
    "    query_execution_id = response['QueryExecutionId']\n",
    "\n",
    "    # Wait for the query to complete\n",
    "    while True:\n",
    "        query_status = ATHENA_CLIENT.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "        status = query_status['QueryExecution']['Status']['State']\n",
    "\n",
    "        if status in ['SUCCEEDED', 'FAILED', 'CANCELLED']:\n",
    "            break\n",
    "\n",
    "    if only_metadata:\n",
    "        # Retrieve and return the query results if the query succeeded\n",
    "        if status == 'SUCCEEDED':\n",
    "            results = ATHENA_CLIENT.get_query_results(QueryExecutionId=query_execution_id)\n",
    "            logger.debug(f'Athena query ran successfully')\n",
    "        elif status == 'FAILED':\n",
    "            raise Exception('Query from Athena failed') \n",
    "    else:\n",
    "        # Retrieve and return the query results if the query succeeded\n",
    "        if status == 'SUCCEEDED':\n",
    "            s3_response_object = S3.get_object(Bucket=S3_TARGET_BUCKET_PARQUET, Key=f\"{ATHENA_STATUS_OUTPUT_LOCATION.split('/')[-2]}/{query_execution_id}.csv\")\n",
    "            object_content = s3_response_object['Body'].read()\n",
    "            results = pd.read_csv(BytesIO(object_content))\n",
    "            logger.debug(f'Athena query ran successfully')\n",
    "            return results\n",
    "        elif status == 'FAILED':\n",
    "            raise Exception('Query from Athena failed')\n",
    "        \n",
    "\n",
    "        \n",
    "############################################\n",
    "############### Shapley Code ###############\n",
    "############################################\n",
    "\n",
    "# Shapley Functions\n",
    "def power_set(List):\n",
    "    PS = [list(j) for i in range(len(List)) for j in itertools.combinations(List, i+1)]\n",
    "    return PS\n",
    "\n",
    "\n",
    "# return all possible subsets from the channels\n",
    "def subsets(s):\n",
    "    '''\n",
    "    This function returns all the possible subsets of a set of channels.\n",
    "    input :\n",
    "            - s: a set of channels.\n",
    "    '''\n",
    "    if len(s) == 1:\n",
    "        return s\n",
    "    else:\n",
    "        sub_channels = []\n",
    "        for i in range(1, len(s)+1):\n",
    "            sub_channels.extend(map(list, itertools.combinations(s, i)))\n",
    "    return list(map(\",\".join, map(sorted, sub_channels)))\n",
    "\n",
    "\n",
    "# compute the worth of each coalition\n",
    "def v_function(A, C_values):\n",
    "    '''\n",
    "    This function computes the worth of each coalition.\n",
    "    inputs:\n",
    "            - A : a coalition of channels.\n",
    "            - C_values : A dictionnary containing the number of conversions that\n",
    "            each subset of channels has yielded.\n",
    "    '''\n",
    "    subsets_of_A = subsets(A)\n",
    "    worth_of_A = 0\n",
    "    for subset in subsets_of_A:\n",
    "        if subset in C_values:\n",
    "            worth_of_A += C_values[subset]\n",
    "    return worth_of_A\n",
    "\n",
    "\n",
    "FACTORIAL_DICT = {}\n",
    "def factorial(n):\n",
    "    global FACTORIAL_DICT\n",
    "    if n == 0:\n",
    "        FACTORIAL_DICT[n] = 1\n",
    "        return 1\n",
    "    if n not in FACTORIAL_DICT:\n",
    "        FACTORIAL_DICT[n] = factorial(n-1) * n\n",
    "    return FACTORIAL_DICT[n]\n",
    "\n",
    "\n",
    "# calculate shapley value\n",
    "def calculate_shapley(df, channel_name, conv_name):\n",
    "    '''\n",
    "    This function returns the shapley values\n",
    "            - df: A dataframe with the two columns: ['channel_name', 'conv_name'].\n",
    "            The channel_subset column is the channel(s) associated with the conversion and the\n",
    "            count is the sum of the conversions.\n",
    "            - channel_name: A string that is the name of the channel column\n",
    "            - conv_name: A string that is the name of the column with conversions\n",
    "            **Make sure that that each value in channel_subset is in alphabetical order.\n",
    "            Email,PPC and PPC,Email are the same in regards to this analysis and\n",
    "            should be combined under Email,PPC.\n",
    "    '''\n",
    "    # casting the subset into dict, and getting the unique channels\n",
    "    c_values = df.set_index(channel_name).to_dict()[conv_name]\n",
    "    df['channels'] = df[channel_name].apply(lambda x: x if len(x.split(\",\")) == 1 else np.nan)\n",
    "    channels = list(df['channels'].dropna().unique())\n",
    "\n",
    "    v_values = {}\n",
    "    for A in power_set(channels):  # generate all possible channel combination\n",
    "        v_values[','.join(sorted(A))] = v_function(A, c_values)\n",
    "    n = len(channels)  # no. of channels\n",
    "    shapley_values = defaultdict(int)\n",
    "\n",
    "    for channel in channels:\n",
    "        for A in v_values.keys():\n",
    "            if channel not in A.split(\",\"):\n",
    "                cardinal_A = len(A.split(\",\"))\n",
    "                A_with_channel = A.split(\",\")\n",
    "                A_with_channel.append(channel)\n",
    "                A_with_channel = \",\".join(sorted(A_with_channel))\n",
    "                weight = (factorial(cardinal_A)*factorial(n-cardinal_A-1)/factorial(n))  # Weight = |S|!(n-|S|-1)!/n!\n",
    "                contrib = (v_values[A_with_channel]-v_values[A])  # Marginal contribution = v(S U {i})-v(S)\n",
    "                shapley_values[channel] += weight * contrib\n",
    "        # Add the term corresponding to the empty set\n",
    "        shapley_values[channel] += v_values[channel]/n\n",
    "\n",
    "    return shapley_values\n",
    "\n",
    "\n",
    "def get_ips_for_dimension(dimension, adserver_df, pixel_df):\n",
    "    # Merge Adserver and Pixel data for dimension\n",
    "    merged_df = pd.merge(\n",
    "        pixel_df,\n",
    "        adserver_df,\n",
    "        how=\"left\",\n",
    "        left_on=[\"ip_address\"],\n",
    "        right_on=[\"ip_address\"],\n",
    "    )\n",
    "    merged_df['adserver_timestamp'] = pd.to_datetime(merged_df['adserver_timestamp'])\n",
    "    merged_df[\"keep_time\"] = np.where(\n",
    "        merged_df[\"event_timestamp\"] > merged_df[\"adserver_timestamp\"], 1, 0\n",
    "    )\n",
    "    merged_df1 = merged_df[merged_df[\"keep_time\"].isin([1])]\n",
    "\n",
    "    del pixel_df\n",
    "    del adserver_df\n",
    "\n",
    "    sorted_df = merged_df1[[\"ip_address\", dimension]].sort_values(\n",
    "        by=[\"ip_address\", dimension]\n",
    "    )\n",
    "    sorted_df[\"converted\"] = 1\n",
    "    del merged_df\n",
    "    del merged_df1\n",
    "\n",
    "    sorted_df.drop_duplicates(inplace=True)\n",
    "    return sorted_df\n",
    "\n",
    "\n",
    "def mta_process(dimension, adserver_df, pixel_df):\n",
    "    try:\n",
    "        sorted_df = get_ips_for_dimension(dimension, adserver_df, pixel_df)\n",
    "        grouped_ip_df = sorted_df.groupby(['ip_address'], as_index=False).agg({dimension: lambda x: ','.join(map(str, x.unique())), 'converted': max})\n",
    "\n",
    "        #create dummies for those channels without singular factorials\n",
    "        dummy_list = list(set(list(sorted_df[dimension])))\n",
    "        lens_1 = len(dummy_list)\n",
    "        lens_2 = range(lens_1)\n",
    "        dummies_df = pd.DataFrame(list(zip(lens_2, dummy_list)), columns=('ip_address', dimension))\n",
    "        dummies_df['converted'] = 0.00001\n",
    "        added_dummies_df = pd.concat([grouped_ip_df, dummies_df])\n",
    "        del sorted_df\n",
    "        del dummies_df\n",
    "\n",
    "        grouped_with_dummies_df = added_dummies_df.groupby([dimension], as_index=False)['converted'].sum()\n",
    "        del added_dummies_df\n",
    "\n",
    "        grouped_kpi_df = grouped_with_dummies_df.groupby([dimension], as_index=False)['converted'].sum()\n",
    "\n",
    "        del grouped_with_dummies_df\n",
    "\n",
    "        shapley_dict = calculate_shapley(grouped_kpi_df, dimension, 'converted')\n",
    "        del grouped_kpi_df\n",
    "        \n",
    "        # return shapley dictionary for upload\n",
    "        return shapley_dict\n",
    "    except Exception as e:\n",
    "        logger.error(f\"ERROR: MTA.Process Failed: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def process_mta_by_channel_and_event_date(channel, adserver_df, pixel_df):\n",
    "    return process_mta_by_event_date(\n",
    "        adserver_df[adserver_df['channel_name'].str.lower() == channel.lower()],\n",
    "        pixel_df\n",
    "    )\n",
    "\n",
    "def process_mta_by_event_date(adserver_df, pixel_df):\n",
    "    date_dict = {}\n",
    "    if adserver_df.empty:\n",
    "        logger.error('No click impression records returned from Adserver')\n",
    "    else:\n",
    "        # Run MTA Attribution for Impression Dates\n",
    "        shapley_dict = mta_process(\"event_date\", adserver_df, pixel_df)\n",
    "        shap_total = 0.0\n",
    "\n",
    "        for key in shapley_dict:\n",
    "            if key != \"nan\":\n",
    "                shap_total += float(shapley_dict[key])\n",
    "\n",
    "        for key in shapley_dict:\n",
    "            if key != \"nan\":\n",
    "                date_dict[key] = float(shapley_dict[key])/float(shap_total)\n",
    "\n",
    "    return date_dict\n",
    "\n",
    "\n",
    "############################################\n",
    "############# Utility Functions ############\n",
    "############################################\n",
    "        \n",
    "        \n",
    "def upload_results(data, branch, s3_destination):\n",
    "    global S3, S3_TARGET_BUCKET\n",
    "    try:\n",
    "        target_bucket = S3_TARGET_BUCKET\n",
    "        logger.info(f'Uploading file to S3 to prefix: {s3_destination}')\n",
    "        output = BytesIO()\n",
    "        pd.DataFrame(data).to_csv(output, mode='wb', encoding='UTF-8', index=False)\n",
    "        output.seek(0)\n",
    "        S3.upload_fileobj(output, target_bucket, s3_destination)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error: upload_results(): {e}')\n",
    "        raise\n",
    "\n",
    "def upload_results_parquet(data, branch, s3_destination):\n",
    "    global S3, S3_TARGET_BUCKET_PARQUET\n",
    "    try:\n",
    "        target_bucket = S3_TARGET_BUCKET_PARQUET\n",
    "        logger.info(f'Uploading file to S3 to prefix: {s3_destination}')\n",
    "        table = pa.Table.from_pandas(data)     \n",
    "        buf = BytesIO()     \n",
    "        pq.write_table(table, buf, compression='SNAPPY')\n",
    "        S3.put_object(Bucket=target_bucket, Key=s3_destination, Body=buf.getvalue())\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error: upload_results(): {e}')\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "############################################\n",
    "############# Channel Pipeline #############\n",
    "############################################\n",
    "\n",
    "def parse_timestamp(time_stamp):\n",
    "    try:\n",
    "        time_stamp\n",
    "        if \".\" in time_stamp:\n",
    "            return datetime.strptime(time_stamp.split(\".\")[0], \"%Y-%m-%d %H:%M:%S\")\n",
    "        else:\n",
    "            return datetime.strptime(time_stamp, \"%Y-%m-%d %H:%M:%S\")\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def load_display_df(clientkey: str, impression_date: str, pixel_date: str) -> pd.DataFrame:\n",
    "    global ATHENA_DATABASE\n",
    "    channel_dict = {25968151: 'display', 26065320: 'Dart Search', 29910233: 'PPC Display'}\n",
    "\n",
    "    if clientkey.lower() != 'constantcontact':\n",
    "        return pd.DataFrame()\n",
    "    else:\n",
    "        table_client_alias = 'ctct'\n",
    "\n",
    "    query = f'''SELECT\n",
    "        interaction_timestamp as event_date,\n",
    "        interaction_timestamp as adserver_timestamp,\n",
    "        campaign_id,\n",
    "        other_data\n",
    "    FROM {ATHENA_DATABASE}.bigquery_export_{table_client_alias}_cm360_activity\n",
    "    WHERE cast(interaction_timestamp as date) between cast('{impression_date}' as date) and cast('{pixel_date}' as date)\n",
    "        AND activity_id=11247128 and conversion_id > 0 \n",
    "    '''\n",
    "\n",
    "    display = run_athena_query(query)\n",
    "    display['event_date'] = [datetime.fromisoformat(i).date() for i in display['event_date']]\n",
    "    display['adserver_timestamp'] = pd.to_datetime(display['adserver_timestamp'])\n",
    "\n",
    "    display['channel_name'] = display['campaign_id'].map(channel_dict)\n",
    "    display['soid_front'] = np.where(display['other_data'].str.lower().str.find('u5=') > -1, display['other_data'].str.lower().str.find('u5=')+3, -1)\n",
    "\n",
    "    f_cut = []\n",
    "    for i, i, s in list(zip(display['soid_front'], display['soid_front'], display['other_data'].str.lower())):\n",
    "        f_cut.append(s[i:i+13])\n",
    "\n",
    "    display['soid'] = f_cut\n",
    "    return display   \n",
    "\n",
    "\n",
    "def get_display_matches_df(clientkey: str, impression_date: str, pixel_date: str, pixel_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if clientkey.lower() != 'constantcontact':\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    if pixel_df.empty:\n",
    "        return pixel_df\n",
    "\n",
    "    display_df = load_display_df(clientkey, impression_date, pixel_date)\n",
    "    if display_df.empty:\n",
    "        return display_df\n",
    "\n",
    "    # pull out \"soid\" from uri_query and append as field to later match display impressions\n",
    "    pixel_df['soid_front'] = np.where(pixel_df['uri_query'].str.lower().str.find('soid=') > -1, pixel_df['uri_query'].str.lower().str.find('soid=')+5, -1)\n",
    "    f_cut1 = []\n",
    "    for i, i, s in list(zip(pixel_df['soid_front'], pixel_df['soid_front'], pixel_df['uri_query'].str.lower())):\n",
    "        f_cut1.append(s[i:i+13])\n",
    "\n",
    "    pixel_df['soid'] = f_cut1\n",
    "\n",
    "    ips = pd.DataFrame(list(zip(pixel_df['ip_address'], pixel_df['soid'])), columns=('ip_address', 'soid'))\n",
    "\n",
    "    # match display impressions on soid\n",
    "    display_1 = display_df[display_df['channel_name'].isin(['display'])]\n",
    "    display_2 = pd.merge(display_1, ips, how='left', left_on=['soid'], right_on=['soid'])\n",
    "    display_2['keeper'] = np.where(display_2['ip_address'] != '', 1, 0)\n",
    "    display1 = display_2[display_2['keeper'].isin([1])].dropna()\n",
    "\n",
    "    return pd.DataFrame(list(zip(display1['channel_name'], display1['ip_address'], display1['adserver_timestamp'], display1['event_date'])), columns=('channel_name', 'ip_address', 'adserver_timestamp', 'event_date'))\n",
    "\n",
    "\n",
    "\n",
    "def format_records(\n",
    "        clientkey,\n",
    "        dimension,\n",
    "        kpi,\n",
    "        pixel_date,\n",
    "        lookback_days,\n",
    "        shapley_dict,\n",
    "        s3_destination,\n",
    "        first_write\n",
    "    ):\n",
    "    \n",
    "    first_write = True\n",
    "    try:\n",
    "        pix_year = datetime.fromisoformat(pixel_date).year\n",
    "        pix_month = datetime.fromisoformat(pixel_date).month\n",
    "        pix_day = datetime.fromisoformat(pixel_date).day\n",
    "        event_date = pixel_date.split(\" \")[0]\n",
    "        advertiser_name = clientkey\n",
    "        kpi_name = kpi\n",
    "        results = dict(shapley_dict)\n",
    "        process_row = 0\n",
    "        csv_records = []\n",
    "        csv_columns = [\n",
    "            \"event_date\",\n",
    "            \"imp_date\",\n",
    "            \"advertiser_name\",\n",
    "            \"channel_name\",\n",
    "            \"dimension\",\n",
    "            \"kpi_name\",\n",
    "            \"kpi_conversions\",\n",
    "            \"lookback_days\",\n",
    "            \"s3_source_file\",\n",
    "            \"ingest_timestamp\",\n",
    "            \"row_id\",\n",
    "        ]\n",
    "\n",
    "        for item in results:\n",
    "            channel_name = None\n",
    "\n",
    "            if dimension == \"channel_name\" and item != \"nan\":\n",
    "                channel_name = str(item)\n",
    "            elif item == \"nan\":\n",
    "                continue\n",
    "            else:\n",
    "                logger.error(f\"ERROR: format_records(): Invalid Dimension: {dimension}\")\n",
    "                raise Exception(f'ERROR: format_records(): Invalid Dimension: {dimension}')\n",
    "\n",
    "            for row in results[item]:\n",
    "                imp_date = row\n",
    "                kpi_conversions = results[item][row]\n",
    "                if kpi_conversions == 0:\n",
    "                    continue\n",
    "\n",
    "                new_row = {\n",
    "                    \"event_date\": event_date,\n",
    "                    \"imp_date\": imp_date,\n",
    "                    \"advertiser_name\": advertiser_name,\n",
    "                    \"channel_name\": channel_name,\n",
    "                    \"dimension\": dimension,\n",
    "                    \"kpi_name\": kpi_name,\n",
    "                    \"kpi_conversions\": kpi_conversions,\n",
    "                    \"lookback_days\": int(lookback_days),\n",
    "                    \"s3_source_file\": s3_destination,\n",
    "                    \"ingest_timestamp\": parse_timestamp(str(datetime.now())),\n",
    "                    \"row_id\": str(uuid.uuid4()),\n",
    "                }\n",
    "                csv_records.append(new_row)\n",
    "                process_row += 1\n",
    "\n",
    "        return csv_records\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ERROR: Error in format_records(). Exception: {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def save_channel_conversions(\n",
    "    postgres_connection, advertiser_name, event_name, event_date, adserver_df, pixel_df\n",
    "):\n",
    "    df = get_ips_for_dimension('channel_name', adserver_df, pixel_df)\n",
    "\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    df = df.groupby('channel_name')[\"converted\"].sum().reset_index()\n",
    "    df.rename({\"channel_name\": \"channel\", \"converted\": \"total\"}, axis=1, inplace=True)\n",
    "\n",
    "    df[\"event_date\"] = pd.to_datetime(event_date).date()\n",
    "    df[\"event_name\"] = event_name\n",
    "    df[\"advertiser_name\"] = advertiser_name\n",
    "    df[\"updated_at\"] = pd.Timestamp.utcnow()\n",
    "\n",
    "    records = df[['event_date', 'event_name', 'advertiser_name']].drop_duplicates().to_dict('records')\n",
    "    conditions = []\n",
    "    for record in records:\n",
    "        inner = []\n",
    "        for key in record:\n",
    "            inner.append(f\"\"\"{key} = '{record[key]}'\"\"\")\n",
    "        conditions.append(' AND '.join(inner))\n",
    "\n",
    "    if len(conditions) == 0:\n",
    "        return\n",
    "\n",
    "    # delete channel conversions for this group\n",
    "    with postgres_connection.cursor() as cursor:\n",
    "        cursor.execute(f\"\"\"\n",
    "            DELETE FROM public.channel_events\n",
    "            WHERE ({') OR ('.join(conditions)})\n",
    "        \"\"\")\n",
    "        postgres_connection.commit()\n",
    "\n",
    "    wr.postgresql.to_sql(\n",
    "        df=df,\n",
    "        con=postgres_connection,\n",
    "        table=\"channel_events\",\n",
    "        schema='public',\n",
    "        use_column_names=True,\n",
    "        mode=\"upsert\",\n",
    "        upsert_conflict_columns=[\n",
    "            \"event_date\",\n",
    "            \"event_name\",\n",
    "            \"advertiser_name\",\n",
    "            \"channel\",\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    del df\n",
    "\n",
    "def get_pixel_data(clientkey, kpi, source_channel_pixel, pixel_date, impression_date, dataset_start_date, source_pixel_s3ingest_exclusion):\n",
    "    global US_WHERE_COND\n",
    "    logger.info(f\"Getting pixel data for {kpi}\")\n",
    "    if clientkey == 'newstwelveny':\n",
    "        clientkey = 'news_twelve_ny'\n",
    "\n",
    "    if clientkey == 'constantcontact':\n",
    "        columns = 'DISTINCT ip_address, event_timestamp, uri_query'\n",
    "    else:\n",
    "        columns = 'DISTINCT ip_address, event_timestamp'\n",
    "\n",
    "    import_query = f\"\"\"\n",
    "        SELECT {columns}\n",
    "            FROM {source_channel_pixel} px \n",
    "            WHERE advertiser_name = '{clientkey}'\n",
    "                AND DATE(event_date) = DATE('{pixel_date}') \n",
    "                AND DATE(event_date) >= DATE('{dataset_start_date}')               \n",
    "                AND event_name = '{kpi}' \n",
    "                AND s3_source_file NOT LIKE '{source_pixel_s3ingest_exclusion}%'\n",
    "                {US_WHERE_COND}\n",
    "                AND ip_address <> '' AND ip_address NOT IN (\n",
    "                    SELECT DISTINCT ip_address \n",
    "                        FROM {source_channel_pixel} \n",
    "                        WHERE advertiser_name = '{clientkey}'\n",
    "                            AND DATE(event_date) > DATE('{impression_date}') \n",
    "                            AND s3_source_file NOT LIKE '{source_pixel_s3ingest_exclusion}%'\n",
    "                            AND DATE(event_date) <= CAST('{pixel_date}' AS TIMESTAMP) - INTERVAL '1' DAY\n",
    "                            AND event_name = '{kpi}'\n",
    "                            {US_WHERE_COND}\n",
    "                )\n",
    "            \n",
    "    \"\"\"\n",
    "    pixel_df = run_athena_query(import_query)\n",
    "    return pixel_df\n",
    "\n",
    "# parse the channel config json\n",
    "def parse_channel_config(channel_config):\n",
    "    channel_type = ''\n",
    "    channel_keys = [\"utm_source\", \"xtm_source\", \"pn\"]\n",
    "    channel_length = 1000\n",
    "    excluded_channels = []\n",
    "    labels = {}\n",
    "    if channel_config is None or len(channel_config) == 0:\n",
    "        return channel_type, channel_keys, channel_length, excluded_channels, labels\n",
    "    try:\n",
    "        config_json = json.loads(channel_config)\n",
    "        if \"channel_type\" in config_json and len(config_json[\"channel_type\"]) > 0:\n",
    "            channel_type = config_json[\"channel_type\"]\n",
    "        if \"channel_keys\" in config_json and len(config_json[\"channel_keys\"]) > 0:           \n",
    "            channel_keys = [x.lower() for x in config_json[\"channel_keys\"]]\n",
    "        if \"channel_length\" in config_json and config_json[\"channel_length\"] > 0:\n",
    "            channel_length = config_json[\"channel_length\"]\n",
    "        if \"channel_attributes\" in config_json:\n",
    "            channel_attributes = config_json[\"channel_attributes\"]\n",
    "            for x in channel_attributes:\n",
    "                if \"name\" in x:\n",
    "                    name = x[\"name\"].lower()\n",
    "                    label = name\n",
    "                    if \"label\" in x:\n",
    "                        label = x[\"label\"]\n",
    "                        labels[name] = label\n",
    "                    if \"excluded\" in x and x[\"excluded\"]:\n",
    "                        excluded_channels.append(name)\n",
    "    except Exception as e:\n",
    "        message = f\"exception while parsing channel config: {e}\"\n",
    "        logger.warn(message)\n",
    "    return channel_type, channel_keys, channel_length, excluded_channels, labels\n",
    "\n",
    "# returns a function that parses the channel name from url\n",
    "def parse_channel_name(channel_type, channel_keys, channel_length):\n",
    "    pattern = re.compile(r'[_\\W]+') # matches underscore or non-alphanumeric\n",
    "    def get_channel_name(site_url):\n",
    "        if pd.isna(site_url):\n",
    "            return ''\n",
    "        query_params = site_url.lower().split('?')\n",
    "        if len(query_params) == 1: # not found\n",
    "            query_params = site_url.lower().split('%3f') # encoded '?'\n",
    "            if len(query_params) == 1: # again not found\n",
    "                return ''\n",
    "        query_params = query_params[-1]  # changing query_params[1] to query_params[-1] because some site_urls are badly formatted (contain two instances of '?')\n",
    "        kv = {}\n",
    "        for x in query_params.split('&'):\n",
    "            tmp = x.split('=')\n",
    "            k = tmp[0]\n",
    "            v = '='.join(tmp[1:])\n",
    "            kv[k] = v\n",
    "        if channel_type == 'z': # zazzle. expect channel_keys = ['utm_source', 'utm_medium']. utm_source value should begin with 0.\n",
    "            if (len(channel_keys) > 1 and\n",
    "                channel_keys[1] in kv and kv[channel_keys[1]] == 'email' and\n",
    "                channel_keys[0] in kv and kv[channel_keys[0]][0] == '0'):\n",
    "                return 'email'\n",
    "        if channel_type == 'p': # pepper. expect channel_keys = ['utm_source', 'utm_medium'].\n",
    "            if len(channel_keys) > 1 and channel_keys[0] in kv and kv[channel_keys[0]] == 'google':\n",
    "                if channel_keys[1] in kv and kv[channel_keys[1]] == 'cpc':\n",
    "                    return 'google'\n",
    "                return ''\n",
    "        first_key = True\n",
    "        for key in channel_keys:\n",
    "            if key in kv:\n",
    "                name = kv[key]\n",
    "                if first_key:\n",
    "                    name = name[:channel_length] # truncate\n",
    "                    name = pattern.sub('', name) # remove offending chars\n",
    "                return name\n",
    "            first_key = False\n",
    "        return ''\n",
    "    return get_channel_name\n",
    "\n",
    "def get_adserver_data(clientkey, kpi, source_channel_pixel, source_ott_adserver, source_audio_adserver, source_digital_adserver, \n",
    "                      include_ott, include_audio, include_digital,\n",
    "                      pixel_date, impression_date, dataset_start_date, source_pixel_s3ingest_exclusion, athena_pixel_table, channel_type, channel_keys,\n",
    "                      channel_length, excluded_channels, labels):   \n",
    "    global US_WHERE_COND\n",
    "    import_query_ott = \"\"\n",
    "    import_query_audio = \"\"\n",
    "    import_query_digital = \"\"\n",
    "    import_query_click = \"\"\n",
    "    \n",
    "    if clientkey == 'newstwelveny':\n",
    "        clientkey = 'news_twelve_ny'\n",
    "    \n",
    "    # if clientkey in ['overstock', 'goldbelly']:\n",
    "    #     cp_where_condition = \"AND source = 'casualprecision'\"\n",
    "    # else:\n",
    "    #     cp_where_condition = ''\n",
    "    cp_where_condition = ''\n",
    "\n",
    "    logger.info(f\"Getting adserver data for channel_name for {kpi} ({pixel_date})\")\n",
    "        \n",
    "    if source_ott_adserver is not None and source_ott_adserver != '' and include_ott is True:\n",
    "        import_query_ott = f\"\"\" \n",
    "                            SELECT DISTINCT\n",
    "                                0 as is_click,\n",
    "                                'OTT' as channel_name,\n",
    "                                ads.ip_address,\n",
    "                                ads.event_timestamp as adserver_timestamp\n",
    "                            FROM {source_ott_adserver} ads\n",
    "                            WHERE LOWER(ads.media_type) = 'ott'\n",
    "                                {cp_where_condition}\n",
    "                                AND ads.advertiser_name = '{clientkey}'\n",
    "                                AND DATE(ads.event_timestamp) > DATE('{impression_date}') AND DATE(ads.event_timestamp) <= DATE('{pixel_date}') AND DATE(ads.event_timestamp) >= DATE('{dataset_start_date}') \n",
    "                                AND ads.ip_address IN (SELECT DISTINCT ip_address FROM converted_ips)\n",
    "\n",
    "                            UNION ALL\n",
    "                        \"\"\"\n",
    "\n",
    "    if source_audio_adserver is not None and source_audio_adserver != '' and include_audio is True:\n",
    "        import_query_audio = f\"\"\" \n",
    "                            SELECT DISTINCT\n",
    "                                0 as is_click,\n",
    "                                'Audio' as channel_name,\n",
    "                                ads.ip_address,\n",
    "                                ads.event_timestamp as adserver_timestamp\n",
    "                            FROM {source_audio_adserver} ads\n",
    "                            WHERE LOWER(ads.media_type) = 'audio'\n",
    "                                {cp_where_condition}\n",
    "                                AND ads.advertiser_name = '{clientkey}'\n",
    "                                AND DATE(ads.event_timestamp) > DATE('{impression_date}') AND DATE(ads.event_timestamp) <= DATE('{pixel_date}') AND DATE(ads.event_timestamp) >= DATE('{dataset_start_date}') \n",
    "                                AND ads.ip_address IN (SELECT DISTINCT ip_address FROM converted_ips)\n",
    "\n",
    "                            UNION ALL\n",
    "                        \"\"\"\n",
    "\n",
    "    if source_digital_adserver is not None and source_digital_adserver != '' and include_digital is False:\n",
    "        import_query_digital = f\"\"\" \n",
    "                            SELECT DISTINCT\n",
    "                            0 as is_click,\n",
    "                            'Display' as channel_name,\n",
    "                            ads.ip_address,\n",
    "                            ads.event_timestamp as adserver_timestamp\n",
    "                            FROM {source_digital_adserver} ads\n",
    "                            WHERE LOWER(ads.media_type) = 'digital'\n",
    "                                {cp_where_condition}\n",
    "                                AND ads.advertiser_name = '{clientkey}'\n",
    "                                AND DATE(ads.event_timestamp) > DATE('{impression_date}') AND DATE(ads.event_timestamp) <= DATE('{pixel_date}') AND DATE(ads.event_timestamp) >= DATE('{dataset_start_date}') \n",
    "                                AND ads.ip_address IN (SELECT DISTINCT ip_address FROM converted_ips)                                             \n",
    "\n",
    "                            UNION ALL\n",
    "                        \"\"\"\n",
    "\n",
    "    import_query_click = f\"\"\" \n",
    "                        SELECT DISTINCT\n",
    "                            1 as is_click,\n",
    "                            click.site_url as channel_name,\n",
    "                            click.ip_address,\n",
    "                            COALESCE(TRY(CAST(DATE_PARSE(click.event_timestamp, '%Y-%m-%d %H:%i:%s.%f') AS TIMESTAMP) )\n",
    "                            ,TRY(CAST(click.event_timestamp AS TIMESTAMP) )) as adserver_timestamp\n",
    "                        FROM {source_channel_pixel} click\n",
    "                        WHERE advertiser_name = '{clientkey}'\n",
    "                            AND click.utm_source <> '' AND click.utm_source IS NOT NULL AND click.traffic_source!='organic'\n",
    "                            AND DATE(COALESCE(TRY(CAST(DATE_PARSE(click.event_timestamp, '%Y-%m-%d %H:%i:%s.%f') AS TIMESTAMP) )\n",
    "                            ,TRY(CAST(click.event_timestamp AS TIMESTAMP) ))) > DATE('{impression_date}')\n",
    "                            AND DATE(COALESCE(TRY(CAST(DATE_PARSE(click.event_timestamp, '%Y-%m-%d %H:%i:%s.%f') AS TIMESTAMP) )\n",
    "                            ,TRY(CAST(click.event_timestamp AS TIMESTAMP) ))) <= DATE('{pixel_date}')\n",
    "                            AND DATE(COALESCE(TRY(CAST(DATE_PARSE(click.event_timestamp, '%Y-%m-%d %H:%i:%s.%f') AS TIMESTAMP) )\n",
    "                            ,TRY(CAST(click.event_timestamp AS TIMESTAMP) ))) >= DATE('{dataset_start_date}') \n",
    "                            {US_WHERE_COND}\n",
    "                            AND click.ip_address IN (SELECT DISTINCT ip_address FROM converted_ips)\n",
    "                    \"\"\"\n",
    "\n",
    "    import_query = f\"\"\"\n",
    "        WITH converted_ips AS (\n",
    "            SELECT DISTINCT ip_address\n",
    "            FROM {source_channel_pixel}\n",
    "            WHERE advertiser_name = '{clientkey}'\n",
    "            AND ip_address <> '' \n",
    "            AND s3_source_file NOT LIKE '{source_pixel_s3ingest_exclusion}%'\n",
    "            AND DATE(event_date) = DATE('{pixel_date}') AND DATE(event_date) >= DATE('{dataset_start_date}') AND event_name = '{kpi}'\n",
    "            {US_WHERE_COND}\n",
    "            AND ip_address NOT IN (\n",
    "                SELECT DISTINCT ip_address \n",
    "                FROM {source_channel_pixel} \n",
    "                WHERE advertiser_name = '{clientkey}'\n",
    "                    AND DATE(event_date) > DATE('{impression_date}') \n",
    "                    AND s3_source_file NOT LIKE '{source_pixel_s3ingest_exclusion}%'\n",
    "                    AND DATE(event_date) <= CAST('{pixel_date}' AS TIMESTAMP) - INTERVAL '1' DAY \n",
    "                    AND event_name = '{kpi}'\n",
    "                    {US_WHERE_COND}\n",
    "            )\n",
    "        )\n",
    "        SELECT\n",
    "            is_click,\n",
    "            channel_name,\n",
    "            ip_address,\n",
    "            adserver_timestamp,\n",
    "            DATE(adserver_timestamp) as event_date\n",
    "        FROM (\n",
    "            {import_query_ott}\n",
    "            {import_query_audio}\n",
    "            {import_query_digital}\n",
    "            {import_query_click}\n",
    "        )\n",
    "    \"\"\"\n",
    "    df = run_athena_query(import_query)\n",
    "    df['adserver_timestamp'] = pd.to_datetime(df['adserver_timestamp'])\n",
    "\n",
    "    # work on click data\n",
    "    df_click = df[df['is_click'] == 1].copy().reset_index(drop=True)\n",
    "\n",
    "    # remove is_click data from the frame\n",
    "    df = df[df['is_click'] == 0]\n",
    "\n",
    "    # convert site_url to channel name for click data\n",
    "    df_click['channel_name'] = df_click['channel_name'].apply(parse_channel_name(channel_type, channel_keys, channel_length))\n",
    "\n",
    "    # drop rows for excluded channels\n",
    "    df_click = df_click[~(df_click['channel_name'].isin(excluded_channels))]\n",
    "\n",
    "    # drop rows for empty channels\n",
    "    df_click = df_click[~(df_click['channel_name'] == '')]\n",
    "\n",
    "    # use label instead of channel name\n",
    "    df_click['channel_name'] = df_click['channel_name'].apply(lambda name: labels[name] if name in labels else name)\n",
    "\n",
    "    # union\n",
    "    df = pd.concat((df, df_click), axis=0).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def pipeline_channel_run(clientkey, impression_date, pixel_date, kpi, lookback_days, \n",
    "                         include_ott, include_audio, include_digital, branch,\n",
    "                         write_results=True):\n",
    "    logger.debug(f'input vars are: {clientkey, impression_date, pixel_date, kpi, lookback_days, include_ott, include_audio, include_digital, branch}')\n",
    "    global redshift_config, ATHENA_PIXEL_TABLE, REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, ATHENA_OTT_ADSERVER, ATHENA_AUDIO_ADSERVER, ATHENA_DIGITAL_ADSERVER\n",
    "    redshift_cursor = redshift_config.get_cursor()\n",
    "    athena_pixel_table = ATHENA_PIXEL_TABLE\n",
    "    source_ott_adserver = ATHENA_OTT_ADSERVER\n",
    "    source_audio_adserver = ATHENA_AUDIO_ADSERVER\n",
    "    source_digital_adserver = ATHENA_DIGITAL_ADSERVER\n",
    "\n",
    "    global S3_DIRECTORY_PREFIX_CSV, S3_FILE_PREFIX_CSV, S3_DIRECTORY_PREFIX_PARQUET, S3_FILE_PREFIX_PARQUET\n",
    "    \n",
    "    pipeline_channel_run_start_time = perf_counter()\n",
    "    try:        \n",
    "        impression_date = datetime.fromisoformat(impression_date).date().isoformat()\n",
    "        pixel_date = datetime.fromisoformat(pixel_date).date().isoformat()\n",
    "\n",
    "        pix_year = datetime.fromisoformat(pixel_date).year\n",
    "        pix_month = datetime.fromisoformat(pixel_date).month\n",
    "        pix_day = datetime.fromisoformat(pixel_date).day\n",
    "        pixel_date_date = datetime.fromisoformat(pixel_date).date().isoformat()\n",
    "       \n",
    "        # S3 PATH FOR channel\n",
    "        upload_file_name_csv = f'{S3_FILE_PREFIX_CSV}_channel_{clientkey}_{kpi}_{lookback_days}day_{str(pix_year)}_{str(\"{:02d}\".format(pix_month))}_{str(\"{:02d}\".format(pix_day))}.csv'\n",
    "        s3_destination_csv = f'{S3_DIRECTORY_PREFIX_CSV}_channel/{clientkey}/{str(pix_year)}/{str(\"{:02d}\".format(pix_month))}/{upload_file_name_csv}'\n",
    "        upload_file_name_parquet = f'{S3_FILE_PREFIX_PARQUET}_channel_{clientkey}_{kpi}_{lookback_days}day_{str(pix_year)}_{str(\"{:02d}\".format(pix_month))}_{str(\"{:02d}\".format(pix_day))}.parquet'\n",
    "        s3_destination_parquet = f'{S3_DIRECTORY_PREFIX_PARQUET}_channel_parquet/advertiser_name={clientkey}/event_date={pixel_date_date}/{upload_file_name_parquet}'\n",
    "\n",
    "        # GET CLIENT VARIABLES FROM REDSHIFT\n",
    "        query = f\"SELECT source_channel_pixel, source_ott_adserver, source_audio_adserver, source_digital_adserver FROM portal_global_settings.client_mappings WHERE LOWER(clientkey_map) = '{clientkey.lower()}' AND enabled_mta_channel = 'true'\"\n",
    "        redshift_cursor.execute(query)\n",
    "        query_results = redshift_cursor.fetchall()\n",
    "\n",
    "        if len(query_results) == 0:\n",
    "            message = f\"WARNING: No records returned from portal_global_settings.client_mappings.  Pipeline aborted\"\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "\n",
    "        # Channel source\n",
    "        if query_results[0][0] is None or str(query_results[0][0]) == \"\":\n",
    "            message = f\"ERROR: There is no value for source_channel_pixel\"\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "        else:\n",
    "#             source_channel_pixel = query_results[0][0]\n",
    "            pass\n",
    "\n",
    "        # OTT source\n",
    "        if include_ott is False:\n",
    "            source_ott_adserver = \"\"\n",
    "        elif include_ott is True and (query_results[0][1] is None or str(query_results[0][1]) == \"\"):\n",
    "            message = f\"ERROR: include_ott is set to true but there is no value for source_ott_adserver\"\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "        else:\n",
    "            pass\n",
    "#             source_ott_adserver = query_results[0][1]\n",
    "\n",
    "        # Audio source\n",
    "        if include_audio is False:\n",
    "            source_audio_adserver = \"\"\n",
    "        elif include_audio is True and (query_results[0][2] is None or str(query_results[0][2]) == \"\"):\n",
    "            message = f\"ERROR: include_audio is set to true but there is no value for source_audio_adserver\"\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "        else:\n",
    "            pass\n",
    "#             source_audio_adserver = query_results[0][2]\n",
    "\n",
    "        # Digital source\n",
    "        if include_digital is False:\n",
    "            source_digital_adserver = \"\"\n",
    "        elif include_digital is True and (query_results[0][3] is None or str(query_results[0][3]) == \"\"):\n",
    "            message = f\"ERROR: include_digital is set to true but there is no value for source_digital_adserver\"\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "        else:\n",
    "            pass\n",
    "#             source_digital_adserver = query_results[0][3]\n",
    "\n",
    "        # get client's dataset start/end dates and pixel s3ingest_exlusions fro global db\n",
    "        query = f\"SELECT dataset_start_date, dataset_end_date, NVL(source_pixel_s3ingest_exclusion,'none'), channel_config FROM portal_global_settings.client_mappings WHERE LOWER(clientkey_map) = '{clientkey.lower()}'\"\n",
    "        redshift_cursor.execute(query)\n",
    "        query_results = redshift_cursor.fetchall()\n",
    "        dataset_start_date = query_results[0][0]\n",
    "        dataset_end_date = query_results[0][1]\n",
    "        source_pixel_s3ingest_exclusion = query_results[0][2]\n",
    "        channel_config = query_results[0][3]\n",
    "        channel_type, channel_keys, channel_length, excluded_channels, labels = parse_channel_config(channel_config)\n",
    "\n",
    "        # Get Pixel data for dimension attribution\n",
    "        pixel_df = get_pixel_data(clientkey, kpi, athena_pixel_table, pixel_date, impression_date, dataset_start_date, source_pixel_s3ingest_exclusion)\n",
    "\n",
    "        if pixel_df.empty:\n",
    "            message = 'No records returned for Pixel'\n",
    "            logger.error(message)\n",
    "            raise Exception(message)\n",
    "        else:\n",
    "            logger.info(f\"Loaded {len(pixel_df)} rows\")\n",
    "\n",
    "            # Process only kpi in list that had data for the lookback period\n",
    "            # Loop through dimensions and run MTA. channel_name must always run first since all other compound values are based on it\n",
    "            dimensions = (\"channel_name\",)\n",
    "            first_write = True\n",
    "\n",
    "            display_df = get_display_matches_df(clientkey, impression_date, pixel_date, pixel_df)\n",
    "\n",
    "            for dimension in dimensions:\n",
    "                # channel_name\n",
    "                if dimension == \"channel_name\":\n",
    "                    # Get Adserver data for dimension\n",
    "                    adserver_df1 = get_adserver_data(clientkey, kpi, \n",
    "                        athena_pixel_table, source_ott_adserver, source_audio_adserver, source_digital_adserver, \n",
    "                        include_ott, include_audio, include_digital,\n",
    "                        pixel_date, impression_date, dataset_start_date, source_pixel_s3ingest_exclusion, athena_pixel_table, channel_type, channel_keys,\n",
    "                        channel_length, excluded_channels, labels)\n",
    "                    \n",
    "                    if not display_df.empty:\n",
    "                        adserver_df1 = pd.concat([adserver_df1, display_df], ignore_index=True)\n",
    "                        adserver_df1.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                    adserver_df1['event_date'] = adserver_df1['event_date'].astype(str)\n",
    "\n",
    "                    adserver2=adserver_df1[~adserver_df1['channel_name'].isin(['OTT','Audio'])]\n",
    "                    counts = adserver2['channel_name'].value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "                    counts['ratio']=counts['counts']/counts['counts'].sum()\n",
    "                    counts['keeper1']=np.where(counts['ratio']>=.001,1,0)\n",
    "                    counts1=counts[counts['keeper1'].isin([1])]\n",
    "                    del adserver2\n",
    "                    \n",
    "                    cnts2=list(counts1['unique_values'])\n",
    "                    cnts2.append('OTT')\n",
    "                    cnts2.append('Audio')\n",
    "        \n",
    "                    keeps = dict(zip(cnts2,cnts2))\n",
    "                    adserver_df1['dimension1']=adserver_df1['channel_name'].map(keeps)\n",
    "                    adserver_df2=adserver_df1.dropna(subset=['dimension1'])\n",
    "                    del adserver_df1\n",
    "        \n",
    "                    cnts=list(counts1['unique_values'])[:10]\n",
    "                    cnts.append('OTT')\n",
    "                    cnts.append('Audio')\n",
    "                    cnts1=list(set(cnts))\n",
    "                    keeps1 = dict(zip(cnts1,cnts1))\n",
    "                    adserver_df2['channel_name']=adserver_df2['dimension1'].map(keeps1)\n",
    "                    adserver_df=adserver_df2.fillna('other')\n",
    "                    \n",
    "                    if adserver_df.empty:\n",
    "                        message = \"No records returned from Pixel Data intersecting with Adserver Data\"\n",
    "                        logger.error(message)\n",
    "                        raise Exception(message)\n",
    "                    else:\n",
    "                        logger.info(f\"Loaded {len(adserver_df)} rows\")\n",
    "\n",
    "                        if dimension == \"channel_name\":\n",
    "                            if write_results:\n",
    "                                postgres_connection = wr.postgresql.connect(\n",
    "                                    boto3_session=boto3.session.Session(aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "                                                                        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "                                        region_name=REGION_NAME),\n",
    "                                    secret_id=\"juice-rds-xmp-prod\" if branch == 'master' else \"juice-rds-xmp-dev\",\n",
    "                                )\n",
    "                                save_channel_conversions(\n",
    "                                    postgres_connection=postgres_connection,\n",
    "                                    advertiser_name=clientkey,\n",
    "                                    event_name=kpi,\n",
    "                                    event_date=pixel_date,\n",
    "                                    adserver_df=adserver_df,\n",
    "                                    pixel_df=pixel_df\n",
    "                                )\n",
    "                                postgres_connection.close()\n",
    "\n",
    "                        # Run MTA Attribution for all of the Channels\n",
    "                        shapley_dict = mta_process(dimension, adserver_df, pixel_df)\n",
    "\n",
    "                        # compute impression breakdown by day\n",
    "                        combined_dict = {}\n",
    "                        for key in shapley_dict:\n",
    "                            if key != \"nan\":\n",
    "                                combined_dict[key] = {}\n",
    "                                date_dict = process_mta_by_channel_and_event_date(key, adserver_df, pixel_df)\n",
    "                                for k in date_dict:\n",
    "                                    combined_dict[key][k] = date_dict[k] * shapley_dict[key]\n",
    "\n",
    "                        if not combined_dict:\n",
    "                            raise Exception('MTA result for channel is empty')\n",
    "\n",
    "                        logger.info(f\"Formatting data for {dimension} for {kpi}\")\n",
    "                        channel_results = format_records(\n",
    "                            clientkey,\n",
    "                            dimension,\n",
    "                            kpi,\n",
    "                            pixel_date,\n",
    "                            lookback_days,\n",
    "                            combined_dict,\n",
    "                            s3_destination_csv,\n",
    "                            first_write\n",
    "                        )\n",
    "                        logger.info(f'Channel run for clientkey: {clientkey}, KPI: {kpi} finished in {round(perf_counter() - pipeline_channel_run_start_time, 3)} seconds')\n",
    "                \n",
    "                        channel_results = pd.DataFrame(channel_results)\n",
    "                        logger.debug(f'channel_results:\\n{channel_results}')\n",
    "                        channel_results['ingest_timestamp'] = channel_results['ingest_timestamp'].astype(str)\n",
    "\n",
    "                        if write_results:\n",
    "                            upload_results_parquet(channel_results, branch, s3_destination_parquet)\n",
    "                            upload_results(channel_results.to_dict(orient='records'), branch, s3_destination_csv)\n",
    "                        return channel_results, shapley_dict\n",
    "\n",
    "    except Exception as e:\n",
    "        message = f\"ERROR: pipeline_channel.Run Failed: {e}\"\n",
    "        logger.error(message)\n",
    "        raise Exception(message)\n",
    "\n",
    "\n",
    "############################################\n",
    "########## OTT and Audio Pipeline ##########\n",
    "############################################\n",
    "\n",
    "\n",
    "class RunContext:\n",
    "    media_type: str\n",
    "    branch: str\n",
    "\n",
    "    client: str\n",
    "    kpi: str\n",
    "    pixel_date: str\n",
    "    lookback_days: int\n",
    "    max_conversions: float\n",
    "\n",
    "    # from client globals\n",
    "    pixel_table: str\n",
    "    adserver_table: str\n",
    "    dataset_start_date: str\n",
    "    dataset_end_date: str\n",
    "    source_pixel_s3ingest_exclusion: str\n",
    "\n",
    "    output_region: str\n",
    "    output_bucket: str\n",
    "    output_file_prefix: str\n",
    "    output_file_name_csv: str\n",
    "    output_file_path_csv: str\n",
    "    output_file_name_parquet: str\n",
    "    output_file_path_parquet: str\n",
    "\n",
    "\n",
    "class ClientSettings:\n",
    "    pixel_table: str\n",
    "    adserver_table: str\n",
    "    dataset_start_date: str\n",
    "    dataset_end_date: str\n",
    "    source_pixel_s3ingest_exclusion: str\n",
    "\n",
    "\n",
    "def load_client_settings(\n",
    "        client: str, media_type: str\n",
    "    ) -> ClientSettings:\n",
    "    global redshift_config\n",
    "    redshift_cursor = redshift_config.get_cursor()\n",
    "\n",
    "    query = f\"\"\"\n",
    "        SELECT\n",
    "            source_{media_type}_pixel,\n",
    "            source_{media_type}_adserver,\n",
    "            dataset_start_date,\n",
    "            dataset_end_date,\n",
    "            NVL(source_pixel_s3ingest_exclusion, 'none') as source_pixel_s3ingest_exclusion\n",
    "        FROM portal_global_settings.client_mappings\n",
    "        WHERE LOWER(clientkey_map) = '{client.lower()}' AND enabled_mta_{media_type} = 'true'\n",
    "    \"\"\"\n",
    "    redshift_cursor.execute(query)\n",
    "    settings = redshift_cursor.fetchone()\n",
    "\n",
    "    if settings is None:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"pixel_table\": settings[0],\n",
    "        \"adserver_table\": settings[1],\n",
    "        \"dataset_start_date\": settings[2],\n",
    "        \"dataset_end_date\": settings[3],\n",
    "        \"source_pixel_s3ingest_exclusion\": settings[4],\n",
    "    }\n",
    "\n",
    "def converted_ips_query_athena(context: RunContext, fields: str):\n",
    "    global US_WHERE_COND\n",
    "    pixel_table = context['athena_pixel_table']\n",
    "    if context['client'] == 'newstwelveny':\n",
    "        clientkey = 'news_twelve_ny'\n",
    "    else:\n",
    "        clientkey = context['client']\n",
    "    return f\"\"\"\n",
    "        SELECT {fields}\n",
    "        FROM {pixel_table}\n",
    "        WHERE advertiser_name = '{clientkey}'\n",
    "        AND ip_address <> '' \n",
    "        AND s3_source_file NOT LIKE '{context['source_pixel_s3ingest_exclusion']}%'\n",
    "        AND date(event_date) = date('{context['pixel_date']}')\n",
    "        AND date(event_date) >= date('{context['dataset_start_date']}')\n",
    "        AND event_name = '{context['kpi']}'\n",
    "        {US_WHERE_COND}\n",
    "        AND ip_address NOT IN (\n",
    "            SELECT DISTINCT ip_address \n",
    "            FROM {pixel_table} \n",
    "            WHERE advertiser_name = '{clientkey}'\n",
    "                AND date(event_date) > date('{context['lookback_date']}')\n",
    "                AND s3_source_file NOT LIKE '{context['source_pixel_s3ingest_exclusion']}%'\n",
    "                AND date(event_date) <= CAST('{context['pixel_date']}' AS TIMESTAMP) - INTERVAL '1' DAY\n",
    "                AND event_name = '{context['kpi']}'\n",
    "                {US_WHERE_COND}\n",
    "        )\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def load_pixels_df(context: RunContext):\n",
    "    logger.info(f\"\"\"MTA[{context['media_type']}]: Loading pixels for KPI \"{context['kpi']}\"...\"\"\")\n",
    "    return run_athena_query(converted_ips_query_athena(context, \"DISTINCT ip_address, event_timestamp\"))\n",
    "\n",
    "\n",
    "def enrich_adserver(ads, media_type):\n",
    "    if media_type == 'ott':\n",
    "        genre_map = get_genre_mappings()\n",
    "        ads = merge_adserver_genre(ads, genre_map)\n",
    "    elif media_type == 'audio':\n",
    "        ads['genre_name'] = 'audio'\n",
    "    return ads\n",
    "\n",
    "\n",
    "def merge_adserver_genre(ads, genre_map):\n",
    "    # join only for media_type == 'ott'\n",
    "    ads['publisher_name_lower']   = ads['publisher_name'].str.lower()\n",
    "    genre_map['network_name_map_lower'] = genre_map['network_name_map'].str.lower()\n",
    "    df = ads.merge(genre_map, how='left', left_on='publisher_name_lower', right_on='network_name_map_lower')\n",
    "    df['genre_name'] = df['genre_name_map'].fillna('Entertainment')\n",
    "    df = df[['genre_name', 'publisher_name', 'audience_name', 'creative_isci', 'device_type', 'adserver_timestamp', 'event_date', 'ip_address']]\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_genre_mappings():\n",
    "    query = \"\"\"\n",
    "        select network_name_map, genre_name_map from portal_global_settings.genre_mappings genres \n",
    "        where genres.media_type_map = 'ott'\n",
    "    \"\"\"\n",
    "    df = run_redshift_query(query)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_impressions_df(context: RunContext):\n",
    "    logger.info(\n",
    "        f\"\"\"MTA[{context['media_type']}]: Loading impressions for KPI \"{context['kpi']}\"...\"\"\"\n",
    "    )\n",
    "\n",
    "    # if context['client'] in ['overstock', 'goldbelly']:\n",
    "    #     cp_where_condition = \"AND source = 'casualprecision'\"\n",
    "    # else:\n",
    "    #     cp_where_condition = ''\n",
    "    cp_where_condition = ''\n",
    "        \n",
    "    query = f\"\"\"\n",
    "    WITH converted_ips AS (\n",
    "        {converted_ips_query_athena(context, 'DISTINCT ip_address')}\n",
    "    )\n",
    "    SELECT DISTINCT\n",
    "        COALESCE(ads.publisher_name, 'Unknown') as publisher_name,\n",
    "        COALESCE(ads.audience_name, 'Unknown') as audience_name,\n",
    "        COALESCE(ads.creative_isci, 'Unknown') as creative_isci,\n",
    "        CASE\n",
    "            WHEN LOWER(ads.device_type) = 'desktop' THEN 'DESKTOP'\n",
    "            WHEN LOWER(ads.device_type) = 'tv' THEN 'TV'\n",
    "            WHEN LOWER(ads.device_type) = 'tablet' THEN 'TABLET'\n",
    "            WHEN LOWER(ads.device_type) = 'phablet' THEN 'TABLET'\n",
    "            WHEN LOWER(ads.device_type) = 'none' THEN 'UNKNOWN'\n",
    "            WHEN LOWER(ads.device_type) = 'television' THEN 'TV'\n",
    "            WHEN LOWER(ads.device_type) = 'mobile' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'smart speaker' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'camera' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'smartphone' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'feature phone' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'console' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'smart display' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'portable media player' THEN 'MOBILE'\n",
    "            WHEN LOWER(ads.device_type) = 'car' THEN 'MOBILE'\n",
    "            ELSE 'UNKNOWN'\n",
    "        END AS device_type,\n",
    "        ads.event_timestamp as adserver_timestamp,\n",
    "        DATE(ads.event_timestamp) as event_date,\n",
    "        ads.ip_address\n",
    "    FROM {context['athena_adserver_table']} ads\n",
    "    WHERE LOWER(ads.media_type) = '{context['media_type']}'\n",
    "    {cp_where_condition}\n",
    "    AND ads.advertiser_name = '{context['client']}'\n",
    "    AND DATE(ads.event_timestamp) > DATE('{context['lookback_date']}')\n",
    "    AND DATE(ads.event_timestamp) <= DATE('{context['pixel_date']}')\n",
    "    AND DATE(ads.event_timestamp) >= DATE('{context['dataset_start_date']}')\n",
    "    AND ads.ip_address IN (SELECT DISTINCT ip_address FROM converted_ips)\n",
    "    \"\"\"    \n",
    "\n",
    "    df_tmp = run_athena_query(query)\n",
    "    df = enrich_adserver(df_tmp, context['media_type'])\n",
    "    df[\"event_date\"] = df[\"event_date\"].astype(str)\n",
    "    return df\n",
    "\n",
    "\n",
    "class DimensionColumns:\n",
    "    genre_name: str\n",
    "    publisher_name: str\n",
    "    audience_name: str\n",
    "    creative_isci: str\n",
    "\n",
    "\n",
    "def get_dimension_rows(\n",
    "        context: RunContext,\n",
    "        dimension_name: str,\n",
    "        dimension_column: str,\n",
    "        dimension_columns: DimensionColumns,\n",
    "        date_shapley_values: dict,\n",
    "    ):\n",
    "    assert isinstance(dimension_columns, dict), 'invalid \"dimension_columns\" argument'\n",
    "    assert \"genre_name\" in dimension_columns, 'missing \"genre_name\"'\n",
    "\n",
    "    dimension_columns.setdefault(\"audience_name\", \"\")\n",
    "    dimension_columns.setdefault(\"creative_isci\", \"\")\n",
    "    dimension_columns.setdefault(\"device_type\", \"\")\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    for k in date_shapley_values:\n",
    "        dimension_columns[dimension_column] = k\n",
    "\n",
    "        for impression_date in date_shapley_values[k]:\n",
    "            assert \"publisher_name\" in dimension_columns, 'missing \"publisher_name\"'\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"event_date\": context[\"pixel_date\"],\n",
    "                    \"imp_date\": impression_date,\n",
    "                    \"advertiser_name\": context[\"client\"],\n",
    "                    \"genre_name\": dimension_columns[\"genre_name\"],\n",
    "                    \"publisher_name\": dimension_columns[\"publisher_name\"],\n",
    "                    \"audience_name\": dimension_columns[\"audience_name\"],\n",
    "                    \"creative_isci\": dimension_columns[\"creative_isci\"],\n",
    "                    \"device_type\": dimension_columns[\"device_type\"],\n",
    "                    \"dimension\": dimension_name,\n",
    "                    \"kpi_name\": context[\"kpi\"],\n",
    "                    \"kpi_conversions\": date_shapley_values[k][impression_date],\n",
    "                    \"lookback_days\": context[\"lookback_days\"],\n",
    "                    \"s3_source_file\": context[\"output_file_path_csv\"],\n",
    "                    \"ingest_timestamp\": datetime.now(),\n",
    "                    \"row_id\": str(uuid.uuid4()),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "def get_kpi_responses_for_dimension(\n",
    "        dimension_column: str,\n",
    "        dimension_df: pd.DataFrame,\n",
    "        pixel_df: pd.DataFrame,\n",
    "        max_conversions: float,\n",
    "    ):\n",
    "    shapley_values_by_date = {}\n",
    "    shapley_values = mta_process(\n",
    "        dimension_column,\n",
    "        dimension_df,\n",
    "        pixel_df,\n",
    "    )\n",
    "    shapley_values_sum = sum(shapley_values.values())\n",
    "\n",
    "    # apply proportional values based on the maximum amount of conversions.\n",
    "    # we do this because the channel MTA run assigns partial credit to media type and\n",
    "    # we need to not let totals exceed those.\n",
    "\n",
    "    proportional_ratio = 0\n",
    "    if max_conversions > 0 and shapley_values_sum > 0:\n",
    "        proportional_ratio = max_conversions / shapley_values_sum\n",
    "\n",
    "    for shapley_dimension in shapley_values:\n",
    "        shapley_values[shapley_dimension] *= proportional_ratio\n",
    "\n",
    "        shapley_values_by_date_for_dimension = mta_process(\n",
    "            \"event_date\",\n",
    "            dimension_df[dimension_df[dimension_column] == shapley_dimension],\n",
    "            pixel_df,\n",
    "        )\n",
    "\n",
    "        shapley_values_by_date_for_dimension_total = sum(\n",
    "            shapley_values_by_date_for_dimension.values()\n",
    "        )\n",
    "\n",
    "        shapley_values_by_date[shapley_dimension] = {}\n",
    "        for date in shapley_values_by_date_for_dimension:\n",
    "            shapley_values_by_date[shapley_dimension][date] = (\n",
    "                shapley_values_by_date_for_dimension[date]\n",
    "                / shapley_values_by_date_for_dimension_total\n",
    "            ) * shapley_values[shapley_dimension]\n",
    "\n",
    "    return {\"totals\": shapley_values, \"event_dates\": shapley_values_by_date}\n",
    "\n",
    "\n",
    "# this is pulled from the original way MTA was ran, but should probably do shapley values\n",
    "# against each genre then do proportional values against shapley values\n",
    "def get_genre_totals(context: RunContext, impression_df: pd.DataFrame):\n",
    "    genre_counts = (\n",
    "        impression_df[[\"genre_name\", \"ip_address\"]]\n",
    "        .drop_duplicates()\n",
    "        .groupby([\"genre_name\"])\n",
    "        .size()\n",
    "        .to_dict()\n",
    "    )\n",
    "    genre_total = sum(genre_counts.values())\n",
    "    genre_totals = {}\n",
    "    for k in genre_counts:\n",
    "        genre_totals[k] = 0\n",
    "        if genre_total > 0:\n",
    "            genre_totals[k] = (genre_counts[k] / genre_total) * context[\"max_conversions\"]\n",
    "\n",
    "    return genre_totals\n",
    "\n",
    "\n",
    "def get_kpi_responses(\n",
    "        context: RunContext, impression_df: pd.DataFrame, pixel_df: pd.DataFrame\n",
    "    ):\n",
    "    genre_totals = get_genre_totals(context, impression_df)\n",
    "\n",
    "    rows = []\n",
    "    for genre_name, genre_df in impression_df.groupby(\"genre_name\"):\n",
    "        publisher_shapley_values = get_kpi_responses_for_dimension(\n",
    "            dimension_column=\"publisher_name\",\n",
    "            dimension_df=genre_df,\n",
    "            pixel_df=pixel_df,\n",
    "            max_conversions=genre_totals[genre_name],\n",
    "        )\n",
    "\n",
    "        rows += get_dimension_rows(\n",
    "            context=context,\n",
    "            dimension_name=\"publisher_name\",\n",
    "            dimension_column=\"publisher_name\",\n",
    "            dimension_columns={\n",
    "                \"genre_name\": genre_name,\n",
    "            },\n",
    "            date_shapley_values=publisher_shapley_values[\"event_dates\"],\n",
    "        )\n",
    "\n",
    "        for k, publisher_df in genre_df.groupby([\"genre_name\", \"publisher_name\"]):\n",
    "            genre_name, publisher_name = k\n",
    "\n",
    "            # calculate results for audience_name and don't let it exceed publisher totals\n",
    "            audience_shapley_values = get_kpi_responses_for_dimension(\n",
    "                dimension_column=\"audience_name\",\n",
    "                dimension_df=publisher_df,\n",
    "                pixel_df=pixel_df,\n",
    "                max_conversions=publisher_shapley_values[\"totals\"][publisher_name],\n",
    "            )\n",
    "            rows += get_dimension_rows(\n",
    "                context=context,\n",
    "                dimension_name=\"publisher_audience\",\n",
    "                dimension_column=\"audience_name\",\n",
    "                dimension_columns={\n",
    "                    \"publisher_name\": publisher_name,\n",
    "                    \"genre_name\": genre_name,\n",
    "                },\n",
    "                date_shapley_values=audience_shapley_values[\"event_dates\"],\n",
    "            )\n",
    "\n",
    "            # calculate creative_isci results within each audience group and don't let it\n",
    "            # exceed audience totals\n",
    "            for audience_name, audience_df in publisher_df.groupby(\"audience_name\"):\n",
    "                creative_shapley_values = get_kpi_responses_for_dimension(\n",
    "                    dimension_column=\"creative_isci\",\n",
    "                    dimension_df=audience_df,\n",
    "                    pixel_df=pixel_df,\n",
    "                    max_conversions=audience_shapley_values[\"totals\"][audience_name],\n",
    "                )\n",
    "\n",
    "                rows += get_dimension_rows(\n",
    "                    context=context,\n",
    "                    dimension_name=\"publisher_audience_creative\",\n",
    "                    dimension_column=\"creative_isci\",\n",
    "                    dimension_columns={\n",
    "                        \"publisher_name\": publisher_name,\n",
    "                        \"genre_name\": genre_name,\n",
    "                        \"audience_name\": audience_name,\n",
    "                    },\n",
    "                    date_shapley_values=creative_shapley_values[\"event_dates\"],\n",
    "                )\n",
    "\n",
    "                # calculate device_type results within each creative_isci group and don't let it\n",
    "                # exceed creative totals\n",
    "                for creative_isci, creative_df in audience_df.groupby(\"creative_isci\"):\n",
    "                    if creative_shapley_values[\"totals\"][creative_isci] == 0:\n",
    "                        continue\n",
    "\n",
    "                    device_shapley_values = get_kpi_responses_for_dimension(\n",
    "                        dimension_column=\"device_type\",\n",
    "                        dimension_df=creative_df,\n",
    "                        pixel_df=pixel_df,\n",
    "                        max_conversions=creative_shapley_values[\"totals\"][\n",
    "                            creative_isci\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "                    rows += get_dimension_rows(\n",
    "                        context=context,\n",
    "                        dimension_name=\"publisher_audience_creative_devicetype\",\n",
    "                        dimension_column=\"device_type\",\n",
    "                        dimension_columns={\n",
    "                            \"publisher_name\": publisher_name,\n",
    "                            \"genre_name\": genre_name,\n",
    "                            \"audience_name\": audience_name,\n",
    "                            \"creative_isci\": creative_isci,\n",
    "                        },\n",
    "                        date_shapley_values=device_shapley_values[\"event_dates\"],\n",
    "                    )\n",
    "\n",
    "    if len(rows) == 0:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame.from_records(rows)\n",
    "    df[\"ingest_timestamp\"] = df[\"ingest_timestamp\"].astype(\"datetime64[s]\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_run_context(\n",
    "        media_type: str,\n",
    "        clientkey: str,\n",
    "        kpi: str,\n",
    "        pixel_iso_date: str,\n",
    "        impression_iso_date: str,\n",
    "        lookback_days: int,\n",
    "        max_conversions: float,\n",
    "        output_region: str,\n",
    "        output_bucket: str,\n",
    "        output_prefix: str,\n",
    "        source_pixel: str\n",
    "    ) -> RunContext:\n",
    "    global S3_DIRECTORY_PREFIX_CSV, S3_FILE_PREFIX_CSV, S3_DIRECTORY_PREFIX_PARQUET, S3_FILE_PREFIX_PARQUET, ATHENA_PIXEL_TABLE, ATHENA_OTT_ADSERVER\n",
    "    client_settings = load_client_settings(client=clientkey, media_type=media_type)\n",
    "    client_settings['pixel_table'] = source_pixel\n",
    "\n",
    "    pixel_dt = datetime.fromisoformat(pixel_iso_date)\n",
    "    impression_dt = datetime.fromisoformat(impression_iso_date)\n",
    "\n",
    "    pix_year = pixel_dt.strftime(\"%Y\")\n",
    "    pix_month = pixel_dt.strftime(\"%m\")\n",
    "    pix_day = pixel_dt.strftime(\"%d\")\n",
    "    pixel_date_date = datetime.fromisoformat(pixel_iso_date).date().isoformat()\n",
    "\n",
    "#     output_file_name = f\"{output_prefix}_{client}_{kpi}_{lookback_days}day_{pix_year}_{pix_month}_{pix_day}.csv\"\n",
    "#     output_file_path = (\n",
    "#         f\"{output_prefix}/{client}/{pix_year}/{pix_month}/{output_file_name}\"\n",
    "#     )\n",
    "\n",
    "    upload_file_name_csv = f'{S3_FILE_PREFIX_CSV}_{media_type}_{clientkey}_{kpi}_{lookback_days}day_{pix_year}_{pix_month:>02}_{pix_day:>02}.csv'\n",
    "    s3_destination_csv = f'{S3_DIRECTORY_PREFIX_CSV}_{media_type}/{clientkey}/{pix_year}/{pix_month:>02}/{upload_file_name_csv}'\n",
    "    upload_file_name_parquet = f'{S3_FILE_PREFIX_PARQUET}_{media_type}_{clientkey}_{kpi}_{lookback_days}day_{pix_year}_{pix_month:>02}_{pix_day:>02}.parquet'\n",
    "    s3_destination_parquet = f'{S3_DIRECTORY_PREFIX_PARQUET}_{media_type}_parquet/advertiser_name={clientkey}/event_date={pixel_date_date}/{upload_file_name_parquet}'\n",
    "\n",
    "    return {\n",
    "        \"media_type\": media_type,\n",
    "        \"client\": clientkey,\n",
    "        \"kpi\": kpi,\n",
    "        \"pixel_date\": pixel_dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"lookback_date\": impression_dt.strftime(\"%Y-%m-%d\"),\n",
    "        \"lookback_days\": lookback_days,\n",
    "        \"max_conversions\": max_conversions,\n",
    "        \"pixel_table\": client_settings[\"pixel_table\"],\n",
    "        \"athena_pixel_table\": ATHENA_PIXEL_TABLE,\n",
    "        \"adserver_table\": client_settings[\"adserver_table\"],\n",
    "        \"athena_adserver_table\": ATHENA_OTT_ADSERVER,\n",
    "        \"dataset_start_date\": client_settings[\"dataset_start_date\"],\n",
    "        \"dataset_end_date\": client_settings[\"dataset_end_date\"],\n",
    "        \"source_pixel_s3ingest_exclusion\": client_settings[\"source_pixel_s3ingest_exclusion\"],\n",
    "        \"output_region\": output_region,\n",
    "        \"output_bucket\": output_bucket,\n",
    "        \"output_prefix\": output_prefix,\n",
    "        \"output_file_name_csv\": upload_file_name_csv,\n",
    "        \"output_file_path_csv\": s3_destination_csv,\n",
    "        \"output_file_name_parquet\": upload_file_name_parquet,\n",
    "        \"output_file_path_parquet\" : s3_destination_parquet\n",
    "    }\n",
    "\n",
    "\n",
    "def pipeline_ott_audio_run(\n",
    "        media_type: str,\n",
    "        clientkey: str,\n",
    "        kpi: str,\n",
    "        pixel_iso_date: str,\n",
    "        impression_iso_date: str,\n",
    "        lookback_days: int,\n",
    "        max_conversions: float,\n",
    "        write_s3: bool = True,\n",
    "        write_local: bool = False,\n",
    "        branch: str = 'dev',\n",
    "        source_pixel: str = ''\n",
    "    ):\n",
    "    try:\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        global REGION_NAME, S3_TARGET_BUCKET, ATHENA_PIXEL_TABLE\n",
    "        output_prefix = f'fact_mta_{media_type}'\n",
    "        context = get_run_context(\n",
    "            media_type=media_type,\n",
    "            clientkey=clientkey,\n",
    "            kpi=kpi,\n",
    "            pixel_iso_date=pixel_iso_date,\n",
    "            impression_iso_date=impression_iso_date,\n",
    "            lookback_days=lookback_days,\n",
    "            max_conversions=max_conversions,\n",
    "            output_region=REGION_NAME,\n",
    "            output_bucket=S3_TARGET_BUCKET,\n",
    "            output_prefix=output_prefix,\n",
    "            source_pixel=source_pixel\n",
    "        )\n",
    "        \n",
    "        pixel_df = load_pixels_df(context)\n",
    "\n",
    "        if pixel_df.empty:\n",
    "            message = f'No {type} pixels loaded for \"{clientkey}\" and kpi \"{kpi}\" for \"{pixel_iso_date}'\n",
    "            logger.info(f'''MTA[{context['media_type']}]: {message}''')\n",
    "            raise Exception(message)\n",
    "\n",
    "        impression_df = load_impressions_df(context)\n",
    "        logger.debug(f'pixel_df: {pixel_df.shape},   impression_df: {impression_df.shape}')\n",
    "        kpi_responses_df = get_kpi_responses(\n",
    "            context=context, impression_df=impression_df, pixel_df=pixel_df\n",
    "        )\n",
    "\n",
    "        if kpi_responses_df.empty:\n",
    "            message = f'No KPI responses for \"{clientkey}\" and kpi \"{kpi}\" for \"{pixel_iso_date}\"'\n",
    "            logger.info(f'''MTA[{context['media_type']}]: {message}''')\n",
    "            raise Exception(message)\n",
    "        kpi_responses_df['ingest_timestamp'] = kpi_responses_df['ingest_timestamp'].astype(str)\n",
    "\n",
    "        if write_s3:\n",
    "            upload_results_parquet(kpi_responses_df, branch, context['output_file_path_parquet'])\n",
    "            upload_results(kpi_responses_df.to_dict(orient='records'), branch, context['output_file_path_csv'])\n",
    "\n",
    "        total_execution_time = time.time() - total_start_time\n",
    "        total_minutes = float(total_execution_time) / 60.0\n",
    "\n",
    "        logger.info(\n",
    "            f\"\"\"MTA[{context['media_type']}]: Process completed at {datetime.now()} in {round(float(total_minutes),2)} minutes\"\"\"\n",
    "        )\n",
    "        return kpi_responses_df\n",
    "    except Exception as e:\n",
    "        message = f'ott_audio run failed: {e}'\n",
    "        logger.error(message)\n",
    "        raise Exception(e)\n",
    "\n",
    "\n",
    "def run_ott_audio(media_type, max_conversions_cap, clientkey, kpi, pixel_dts, impression_dts, lookback_days, source_pixel,\n",
    "                 write_results=True):\n",
    "    results = pd.DataFrame()\n",
    "    start_time = perf_counter()\n",
    "    run_type = 'OTT' if media_type == 'ott' else 'Audio'\n",
    "\n",
    "    logger.info(f\"--------------------------------\")\n",
    "    logger.info(f\"STARTING {run_type} PROCESSING\")\n",
    "    try:\n",
    "        results = pipeline_ott_audio_run(\n",
    "            media_type=media_type,\n",
    "            clientkey=clientkey,\n",
    "            kpi=kpi,\n",
    "            pixel_iso_date=pixel_dts,\n",
    "            impression_iso_date=impression_dts,\n",
    "            lookback_days=abs(lookback_days),\n",
    "            max_conversions=max_conversions_cap,\n",
    "#                         output_bucket=\"tn-datalake-attribution-mta\" if branch == 'dev' else \"tn-datalake-attribution\",\n",
    "#                         output_prefix=\"mta_ott_dev\" if branch == 'dev' else \"mta_ott\",\n",
    "            write_local=False,\n",
    "            write_s3=write_results,\n",
    "            source_pixel=source_pixel\n",
    "        )\n",
    "        log = {\n",
    "            f'{media_type}_success' : 'success',\n",
    "            f'{media_type}_count'   : results.shape[0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        log = {\n",
    "            f'{media_type}_success' : 'failed',\n",
    "            f'{media_type}_error'   : str(e)\n",
    "        }\n",
    "    \n",
    "    run_time_minutes = round((perf_counter() - start_time) / 60, 2)\n",
    "    log.update({f'{media_type}_run_time_minutes' : run_time_minutes})\n",
    "    logger.info(f'{run_type} run finished for client {clientkey}, KPI {kpi}. Shape of results: {results.shape}')\n",
    "    return results, log\n",
    "\n",
    "\n",
    "def run_channel(clientkey,\n",
    "            impression_dts,\n",
    "            pixel_dts,\n",
    "            kpi,\n",
    "            lookback_days,\n",
    "            include_channel_ott,\n",
    "            include_channel_audio,\n",
    "            include_channel_digital,\n",
    "            branch,\n",
    "            write_results=True\n",
    "    ):\n",
    "    channel_results = pd.DataFrame()\n",
    "    shapley_dict = defaultdict(int)\n",
    "\n",
    "    channel_start_time = perf_counter()\n",
    "    try:\n",
    "        channel_results_tuple = pipeline_channel_run(\n",
    "            clientkey,\n",
    "            impression_dts,\n",
    "            pixel_dts,\n",
    "            kpi,\n",
    "            abs(lookback_days),\n",
    "            include_channel_ott,\n",
    "            include_channel_audio,\n",
    "            include_channel_digital,\n",
    "            branch,\n",
    "            write_results=write_results\n",
    "        )\n",
    "        channel_results, shapley_dict = channel_results_tuple\n",
    "        channel_log = {\n",
    "            'channel_success' : 'success',\n",
    "            'channel_count'   : channel_results.shape[0]\n",
    "        }\n",
    "    except Exception as e:\n",
    "        channel_log = {\n",
    "            'channel_success' : 'failed',\n",
    "            'channel_error'   : str(e)\n",
    "        }\n",
    "    \n",
    "    channel_time_minutes = round((perf_counter() - channel_start_time) / 60, 2)\n",
    "    channel_log.update({'channel_run_time_minutes' : channel_time_minutes})\n",
    "    logger.info(f'Channel run finished for client {clientkey}, KPI {kpi}. Shape of results: {channel_results.shape}')\n",
    "    return channel_results, shapley_dict, channel_log\n",
    "\n",
    "\n",
    "def run(data, branch, write_results=True):\n",
    "    global ATHENA_DATABASE\n",
    "    start_time = perf_counter()\n",
    "    data = data[0]\n",
    "    clientkey                 = data['clientkey']\n",
    "    impression_dts            = data['impression_dts']\n",
    "    pixel_dts                 = data['pixel_dts']\n",
    "    kpi                       = data['kpi']\n",
    "    media_type                = data['media_type']\n",
    "    lookback_days             = data['lookback_days']\n",
    "    include_channel_ott       = data['include_channel_ott']\n",
    "    include_channel_audio     = data['include_channel_audio']\n",
    "    include_channel_digital   = data['include_channel_digital']\n",
    "\n",
    "    logger.info(f'==============================================')\n",
    "    logger.info(f'MTA run started for client: {clientkey}, KPI: {kpi}, pixel date: {pixel_dts}, lookback_days: {7}')\n",
    "\n",
    "    # Run the MTA pipeline\n",
    "    channel_results = pd.DataFrame()\n",
    "    ott_results = pd.DataFrame()\n",
    "    audio_results = pd.DataFrame()\n",
    "    final_results = defaultdict(pd.DataFrame)\n",
    "    final_log = {}\n",
    "    if media_type == \"audio\" or media_type == \"ott\":\n",
    "        source_pixel = f'{ATHENA_DATABASE}.pixels' if media_type == 'ott' else f'{ATHENA_DATABASE}.pixels'\n",
    "        ott_audio_results, ott_audio_log = run_ott_audio(media_type, 0, clientkey, kpi, pixel_dts, impression_dts, lookback_days, source_pixel, write_results=write_results)\n",
    "\n",
    "        final_results[f'{media_type}_results'] = ott_audio_results\n",
    "        final_log.update(ott_audio_log)\n",
    "        logger.info(f'Shape of {media_type} results: {ott_audio_results.shape}')\n",
    "\n",
    "    elif media_type.lower() == \"channel\":\n",
    "        channel_results, shapley_dict, channel_log = run_channel(\n",
    "                                    clientkey,\n",
    "                                    impression_dts,\n",
    "                                    pixel_dts,\n",
    "                                    kpi,\n",
    "                                    lookback_days,\n",
    "                                    include_channel_ott,\n",
    "                                    include_channel_audio,\n",
    "                                    include_channel_digital,\n",
    "                                    branch,\n",
    "                                    write_results=write_results\n",
    "        )\n",
    "        final_log.update(channel_log)\n",
    "\n",
    "        if include_channel_ott is True:\n",
    "            max_conversions_cap = float(shapley_dict['OTT'])\n",
    "            if max_conversions_cap > 0:\n",
    "                ott_results, ott_log = run_ott_audio('ott', max_conversions_cap, clientkey, kpi, pixel_dts, impression_dts, lookback_days, f'{ATHENA_DATABASE}.pixels', write_results=write_results)\n",
    "            else:\n",
    "                logger.info(f'OTT run could not be completed: {f\"max_conversions_cap = {max_conversions_cap}\"}')\n",
    "                ott_log = {\n",
    "                f'ott_success' : 'na',\n",
    "                f'ott_info'    : f\"max_conversions_cap = {max_conversions_cap}\"\n",
    "            }\n",
    "            final_log.update(ott_log)            \n",
    "\n",
    "        if include_channel_audio is True:\n",
    "            max_conversions_cap = float(shapley_dict['Audio'])\n",
    "            if max_conversions_cap > 0:\n",
    "                audio_results, audio_log = run_ott_audio('audio', max_conversions_cap, clientkey, kpi, pixel_dts, impression_dts, lookback_days, f'{ATHENA_DATABASE}.pixels', write_results=write_results)\n",
    "            else:\n",
    "                logger.info(f'Audio run could not be completed: {f\"max_conversions_cap = {max_conversions_cap}\"}')\n",
    "                audio_log = {\n",
    "                    f'audio_success' : 'na',\n",
    "                    f'audio_info'   : f\"max_conversions_cap = {max_conversions_cap}\"\n",
    "                }\n",
    "            final_log.update(audio_log)\n",
    "\n",
    "        final_results['channel_results'] = channel_results\n",
    "        final_results['ott_results'] = ott_results\n",
    "        final_results['audio_results'] = audio_results\n",
    "        logger.info(f'Shape of channel results: {channel_results.shape};   shape of ott results: {ott_results.shape};   shape of audio results: {audio_results.shape}')\n",
    "\n",
    "    logger.info(f'MTA run finished for client:{clientkey}, KPI:{kpi} in {round(perf_counter() - start_time, 3)} seconds')\n",
    "    return final_results, final_log\n",
    "\n",
    "\n",
    "def init_globals(aws_access_key_id, aws_secret_access_key, region_name, redshift_mta_secret_name, \n",
    "        athena_database, athena_status_output_location, athena_pixel_table,\n",
    "        athena_ott_adserver, athena_audio_adserver, athena_digital_adserver,\n",
    "        branch, s3_target_bucket, s3_target_bucket_parquet,\n",
    "        s3_directory_prefix_csv, s3_file_prefix_csv,\n",
    "        s3_directory_prefix_parquet, s3_file_prefix_parquet,\n",
    "        use_us_condition, list_clients_no_us_condition, clientkey\n",
    "    ):\n",
    "\n",
    "    logger.debug('Initing globals')\n",
    "    global REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REDSHIFT_MTA_SECRET_NAME\n",
    "    REGION_NAME = region_name\n",
    "    AWS_ACCESS_KEY_ID = aws_access_key_id\n",
    "    AWS_SECRET_ACCESS_KEY = aws_secret_access_key\n",
    "    REDSHIFT_MTA_SECRET_NAME = redshift_mta_secret_name\n",
    "\n",
    "    global redshift_config\n",
    "    redshift_config = RedshiftConfig(REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, REDSHIFT_MTA_SECRET_NAME)\n",
    "\n",
    "    global S3\n",
    "    S3 = boto3.client(\n",
    "        \"s3\",\n",
    "        region_name=REGION_NAME,\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    "    )\n",
    "\n",
    "    global ATHENA_CLIENT, ATHENA_DATABASE, ATHENA_STATUS_OUTPUT_LOCATION, ATHENA_PIXEL_TABLE, ATHENA_OTT_ADSERVER, ATHENA_AUDIO_ADSERVER, ATHENA_DIGITAL_ADSERVER\n",
    "    ATHENA_CLIENT = get_athena_connection(REGION_NAME, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)\n",
    "    ATHENA_DATABASE = athena_database\n",
    "    ATHENA_STATUS_OUTPUT_LOCATION = athena_status_output_location\n",
    "    ATHENA_PIXEL_TABLE = athena_pixel_table\n",
    "    ATHENA_OTT_ADSERVER = athena_ott_adserver\n",
    "    ATHENA_AUDIO_ADSERVER = athena_audio_adserver\n",
    "    ATHENA_DIGITAL_ADSERVER = athena_digital_adserver\n",
    "\n",
    "    global S3_TARGET_BUCKET, S3_TARGET_BUCKET_PARQUET\n",
    "    S3_TARGET_BUCKET = s3_target_bucket\n",
    "    S3_TARGET_BUCKET_PARQUET = s3_target_bucket_parquet\n",
    "\n",
    "    global S3_DIRECTORY_PREFIX_CSV, S3_FILE_PREFIX_CSV\n",
    "    if s3_directory_prefix_csv[-1] == '/':\n",
    "        s3_directory_prefix_csv = s3_directory_prefix_csv[:-1]\n",
    "    S3_DIRECTORY_PREFIX_CSV = s3_directory_prefix_csv\n",
    "    S3_FILE_PREFIX_CSV = s3_file_prefix_csv\n",
    "    \n",
    "    global S3_DIRECTORY_PREFIX_PARQUET, S3_FILE_PREFIX_PARQUET\n",
    "    if s3_directory_prefix_parquet[-1] == '/':\n",
    "        s3_directory_prefix_parquet = s3_directory_prefix_parquet[:-1]\n",
    "    S3_DIRECTORY_PREFIX_PARQUET = s3_directory_prefix_parquet\n",
    "    S3_FILE_PREFIX_PARQUET = s3_file_prefix_parquet\n",
    "    \n",
    "    global US_WHERE_COND\n",
    "    if use_us_condition.lower() == 'true':\n",
    "        if clientkey in [i.strip() for i in list_clients_no_us_condition.split(',')]:\n",
    "            US_WHERE_COND = ''\n",
    "        else:\n",
    "            US_WHERE_COND = \"AND UPPER(geo_country_code) = 'US'\"\n",
    "    elif use_us_condition.lower() == 'false':\n",
    "        US_WHERE_COND = ''\n",
    "\n",
    "\n",
    "def close_connections():\n",
    "    global redshift_config, S3, ATHENA_CLIENT\n",
    "    redshift_config.cursor.close()\n",
    "    redshift_config.connection.close()\n",
    "#     S3.close()\n",
    "#     ATHENA_CLIENT.close()\n",
    "    logger.debug(f'Closed all connections')\n",
    "\n",
    "\n",
    "def repair_mta_tables(clientkey, pixel_date):\n",
    "    global ATHENA_DATABASE\n",
    "    table_name_prefix = 'fact_mta'\n",
    "    table_suffixes = ['channel', 'ott', 'audio']\n",
    "\n",
    "    # Define the SQL query to run the MSCK repair operation\n",
    "    for table in table_suffixes:\n",
    "        table_name = f'{ATHENA_DATABASE}.{table_name_prefix}_{table}'\n",
    "        alter_table_query = f\"ALTER TABLE {table_name} ADD IF NOT EXISTS PARTITION (advertiser_name = '{clientkey}', event_date = '{pixel_date}');\"\n",
    "        run_athena_query(alter_table_query, only_metadata=True)\n",
    "\n",
    "\n",
    "def start_func(data, \n",
    "               aws_access_key_id, aws_secret_access_key, region_name, secret_name, \n",
    "               athena_database, athena_status_output_location, \n",
    "               branch, s3_target_bucket, s3_target_bucket_parquet,\n",
    "               s3_directory_prefix_csv, s3_file_prefix_csv,\n",
    "               s3_directory_prefix_parquet, s3_file_prefix_parquet,\n",
    "               use_us_condition, list_clients_no_us_condition,\n",
    "               debug=False, write_results=True):\n",
    "\n",
    "    clientkey     = data[0]['clientkey']\n",
    "    pixel_date    = data[0]['pixel_dts'][:10]\n",
    "    kpi           = data[0]['kpi']\n",
    "    lookback_days = data[0]['lookback_days']\n",
    "    \n",
    "    athena_pixel_table = f'{athena_database}.pixels'\n",
    "    athena_ott_adserver = f'{athena_database}.adserver'\n",
    "    athena_audio_adserver = f'{athena_database}.adserver'\n",
    "    athena_digital_adserver = f'{athena_database}.adserver'\n",
    "\n",
    "    init_globals(aws_access_key_id, aws_secret_access_key, region_name, secret_name, \n",
    "                 athena_database, athena_status_output_location, athena_pixel_table,\n",
    "                 athena_ott_adserver, athena_audio_adserver, athena_digital_adserver,\n",
    "                 branch, s3_target_bucket, s3_target_bucket_parquet,\n",
    "                 s3_directory_prefix_csv, s3_file_prefix_csv,\n",
    "                 s3_directory_prefix_parquet, s3_file_prefix_parquet,\n",
    "                 use_us_condition, list_clients_no_us_condition, clientkey)\n",
    "\n",
    "    start_time = perf_counter()\n",
    "    \n",
    "    results = run(data, branch, write_results)\n",
    "    repair_mta_tables(clientkey, pixel_date)\n",
    "    close_connections()\n",
    "    mta_results, results_log = results\n",
    "    \n",
    "    logger.info(f'MTA process finished in {round(perf_counter() - start_time, 3)} seconds')\n",
    "    final_log = {\n",
    "        'clientkey'       : clientkey,\n",
    "        'kpi'             : kpi,\n",
    "        'event_date'      : pixel_date,\n",
    "        'lookback_days'   : lookback_days,\n",
    "    }\n",
    "    final_log.update(results_log)\n",
    "    final_log = [final_log]\n",
    "\n",
    "    if debug:\n",
    "        return mta_results['channel_results'], mta_results['ott_results'], mta_results['audio_results'], final_log\n",
    "    return final_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
