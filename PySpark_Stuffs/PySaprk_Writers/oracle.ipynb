{"cells": [{"cell_type": "code", "metadata": {"id": "73433243_0.7722090810150755"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433243_0.38485657719721034"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433243_0.7524697382689598"}, "execution_count": null, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to an Oracle database using JDBC.\n\n    Args:\n        df (pyspark.sql.DataFrame): DataFrame to write.\n        table (str): Name of the table to write to.\n        user (str): Username to authenticate with.\n        password (str): Password to authenticate with.\n        host (str): Hostname or IP address of the Oracle server.\n        port (int): Port number of the Oracle server.\n        database (str): Name of the Oracle database to use.\n    \"\"\"\n    try:\n        #         In case of secrets comment url and directly pass url string, username, password in option\n        #  eg:  df.write \\\n        #   .format(\"jdbc\") \\\n        #   .mode(\"append\") \\\n        #   .option(\"url\", \"jdbc:oracle:thin:@\"+@ENV.ORACLE_HOST+\":\"+@ENV.ORACLE_PORT+\":\"+@ENV.ORACLE_DATABASE) \\\n        #   .option(\"dbtable\", table) \\\n        #   .option(\"user\", @ENV.ORACLE_USERNAME) \\\n        #   .option(\"password\", @ENV.ORACLE_PASSWORD) \\\n        #   .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n        #   .save()\n        url = \"jdbc:oracle:thin:@\"+host+\":\"+str(port)+\":\"+database\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write \\\n            .format(\"jdbc\") \\\n            .mode(\"append\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"oracle.jdbc.driver.OracleDriver\") \\\n            .save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\n\npyspark_write(df=df_Doha_Data, table=\"Oracle_Append_PySpark_V1\", user=\"qauser\", password=\"qauser_123\", host=\"bdboracle.c9lo3db08qkg.ap-south-1.rds.amazonaws.com\", port=\"1521\", database=\"ORCL\")"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}