{"cells": [{"cell_type": "code", "metadata": {"id": "73433239_0.8003687914907225"}, "execution_count": 0, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": [{"name": "stdout", "text": ["+-------+------------------+-----------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|summary|       SCHOOL_YEAR|       STUDENT_ID|    student_name|teacher_id|      teacher_name|          term_id|term_name|    ASSESSMENT_ID|             subject|          category|             GRADE|       SCORE_VALUE|CALCULATED_SCORE_VALUE|  CALCULATED_SCORE|PROFICIENCY_LEVEL|\n+-------+------------------+-----------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|  count|           1586464|          1586464|         1586464|   1586187|           1586461|          1586461|  1586461|          1586461|             1586461|           1586117|           1586461|           1586461|               1586458|           1586458|          1586458|\n|   mean|2015.3423746313335|18711.63181067798|             8.0|     100.0|224.66666666666666|6.943723691393028|     null|16389.69381729614|  2.6666666666666665|2.6666666666666665|1.6666666666666667|209.92513216211637|     203.5026790010602|3.1553529939021394|             null|\n| stddev|3.8318100412050438| 5393737.27391304|             0.0|       0.0|207.27115895206774|3.421009677415462|     null|40825.85992317848|  0.5773502691896258|0.5773502691896258|0.5773502691896258|203.42570927166355|    203.35109761984197|0.9119469365054239|             null|\n|    min|           \",\"S32\"|        100032356|               8|       100|               104|                1|      BoY|                1|          IB Bio SL2|              2.00|                 1|               ...|                  0.00|               0.0|         Advanced|\n|    max|              2021|Teacher32_Grade12|zuijdgeest,niels|    ms2020|       zarter,nick|        Chemistry|     Year|         Grade 12|ndependent Study-...|                \\N|                PK|                 S|                    \\N|               4.0|       Proficient|\n+-------+------------------+-----------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "73433239_0.11384270311384515"}, "execution_count": 1, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to a MySQL database using JDBC.\n\n    Args:\n        df (pyspark.sql.DataFrame): DataFrame to write.\n        table (str): Name of the table to write to.\n        user (str): Username to authenticate with.\n        password (str): Password to authenticate with.\n        host (str): Hostname or IP address of the MySQL server.\n        port (int): Port number of the MySQL server.\n        database (str): Name of the MySQL database to use.\n    \"\"\"\n    try:\n        #         In case of secrets comment url and directly pass url string, username, password in option\n        # eg: df.write \\\n        #    .format(\"jdbc\") \\\n        #    .option(\"url\", \"jdbc:mysql://\"+@ENV.MYSQL_HOST+\":\"+@ENV.MYSQL_PORT+\"/\"+@ENV.MYSQL_DATABASE) \\\n        #    .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n        #    .option(\"dbtable\", table) \\\n        #    .option(\"user\", @ENV.MYSQL_USERNAME) \\\n        #    .option(\"password\", @ENV.MYSQL_PASSWORD) \\\n        #   .mode(\"append\") \\\n        #    .save()\n        url = \"jdbc:mysql://\"+host+\":\"+str(port)+\"/\"+database\n        # uncomment below lines if dataframe size is big \n        # numPartitions = 8\n        # df = df.repartition(numPartitions)\n        df.write \\\n            .format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\npyspark_write(df=df_Doha_Data, table=\"Append_Test_PySpark_2M_v4\", user=\"qateam\", password=\"Temz6a52iud67\", host=\"db.dev.bdb.ai\", port=\"3306\", database=\"qa_test\")"], "outputs": [{"name": "stdout", "text": ["Saved Successfully\n"], "output_type": "stream"}]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}