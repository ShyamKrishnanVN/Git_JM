{"cells": [{"cell_type": "code", "metadata": {"id": "73433245_0.06177181570890666"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433245_0.8891566189752735"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433245_0.5061276809603805"}, "execution_count": null, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to MongoDB using the com.mongodb.spark.sql.DefaultSource format.\n\n    Parameters:\n    df (pyspark.sql.DataFrame): The DataFrame to write to MongoDB.\n    table (str): The name of the collection to write to in the format \"database.collection\".\n    user (str): The username to authenticate with MongoDB.\n    password (str): The password to authenticate with MongoDB.\n    host (str): The hostname or IP address of the MongoDB server.\n    port (str): The port number of the MongoDB server.\n    database (str): The name of the MongoDB database.\n\n    Returns:\n    None\n    \"\"\"\n    try:\n    #         uncomment below lines if dataframe size is big \n    #         numPartitions = 8\n    #         df = df.repartition(numPartitions)\n    #         In case of secrets comment mongo_uri and directly pass uri string in option\n    #         eg: df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"spark.mongodb.output.uri\", \"mongodb://\"+@ENV.MONGOQL_USERNAME+\":\"+@ENV.MONGOQL_PASSWORD+\"@\"+@ENV.MONGOQL_HOST+\":\"+@ENV.MONGOQL_PORT+\"/\"+@ENV.MONGOQL_DATABASE+\".\"+table).save()\n\n        mongo_uri = \"mongodb://\"+user+\":\"+password+\"@\"+host+\":\"+str(port)+\"/\"+database+\".\"+table+\"?authSource=\"+database\n        df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n            .mode(\"append\") \\\n            .option(\"spark.mongodb.output.uri\", mongo_uri).save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\n\npyspark_write(df=df_Doha_Data, table=\"Mongo_Append_PySpark_V1\", user=\"shyam\", password=\"Sndyix8dhif\", host=\"mongodb-dm.mongodb-dm\", port=\"27017\", database=\"dslab_test\")"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}