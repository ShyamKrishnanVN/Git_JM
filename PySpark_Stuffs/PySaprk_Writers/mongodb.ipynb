{"cells": [{"cell_type": "code", "metadata": {"id": "73433245_0.06177181570890666"}, "execution_count": 0, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": [{"name": "stdout", "text": ["+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n|summary| sex|             length|           diameter|             height|       weight_whole|     weight_shucked|     weight_viscera|       weight_shell|             rings|\n+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n|  count|4177|               4177|               4177|               4177|               4177|               4177|               4177|               4177|              4177|\n|   mean|null| 0.5239920995930099|  0.407881254488869| 0.1395163993296614|   0.82874215944458|0.35936748862820106|0.18059360785252604|0.23883085946851795| 9.933684462532918|\n| stddev|null|0.12009291256479936|0.09923986613365941|0.04182705660725731|0.49038901823099795|0.22196294903322014|0.10961425025968445|0.13920266952238622|3.2241690320681315|\n|    min|   F|              0.075|              0.055|                0.0|              0.002|              0.001|             5.0E-4|             0.0015|                 1|\n|    max|   M|              0.815|               0.65|               1.13|             2.8255|              1.488|               0.76|              1.005|                29|\n+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "73433245_0.5061276809603805"}, "execution_count": null, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to MongoDB using the com.mongodb.spark.sql.DefaultSource format.\n\n    Parameters:\n    df (pyspark.sql.DataFrame): The DataFrame to write to MongoDB.\n    table (str): The name of the collection to write to in the format \"database.collection\".\n    user (str): The username to authenticate with MongoDB.\n    password (str): The password to authenticate with MongoDB.\n    host (str): The hostname or IP address of the MongoDB server.\n    port (str): The port number of the MongoDB server.\n    database (str): The name of the MongoDB database.\n\n    Returns:\n    None\n    \"\"\"\n    try:\n    #         uncomment below lines if dataframe size is big \n    #         numPartitions = 8\n    #         df = df.repartition(numPartitions)\n    #         In case of secrets comment mongo_uri and directly pass uri string in option\n    #         eg: df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").option(\"spark.mongodb.output.uri\", \"mongodb://\"+@ENV.MONGOQL_USERNAME+\":\"+@ENV.MONGOQL_PASSWORD+\"@\"+@ENV.MONGOQL_HOST+\":\"+@ENV.MONGOQL_PORT+\"/\"+@ENV.MONGOQL_DATABASE+\".\"+table).save()\n\n        mongo_uri = \"mongodb://\"+user+\":\"+password+\"@\"+host+\":\"+str(port)+\"/\"+database+\".\"+table+\"?authSource=\"+database\n        df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n            .mode(\"append\") \\\n            .option(\"spark.mongodb.output.uri\", mongo_uri).save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\n\npyspark_write(df=df_Abalone_data, table=\"APPEND_TEST_PYSPARK_4K_V1\", user=\"shyam\", password=\"Sndyix8dhif\", host=\"mongodb-dm.mongodb-dm\", port=\"27017\", database=\"dslab_test\")"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}