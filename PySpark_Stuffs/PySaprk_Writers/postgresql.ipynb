{"cells": [{"cell_type": "code", "metadata": {"id": "73433247_0.5496965273094931"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433247_0.2606409453688201"}, "execution_count": null, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to a PostgreSQL database using JDBC.\n\n    Args:\n        df (pyspark.sql.DataFrame): DataFrame to write.\n        table (str): Name of the table to write to.\n        user (str): Username to authenticate with.\n        password (str): Password to authenticate with.\n        host (str): Hostname or IP address of the PostgreSQL server.\n        port (int): Port number of the PostgreSQL server.\n        database (str): Name of the PostgreSQL database to use.\n    \"\"\"\n    try:\n        #         In case of secrets comment url and directly pass url string, username, password in option\n        # eg : df.write.format(\"jdbc\") \\\n        #   .option(\"url\", \"jdbc:postgresql://\"+@ENV.POSTGRES_HOST+\":\"+@ENV.POSTGRES_PORT+\"/\"+@ENV.POSTGRES_DATABASE) \\\n        #   .option(\"dbtable\", table) \\\n        #   .option(\"user\", @ENV.POSTGRES_USERNAME) \\\n        #   .option(\"password\", @ENV.POSTGRES_PASSWORD) \\\n        #   .mode(\"append\") \\\n        #   .save()\n        url = \"jdbc:postgresql://\"+host+\":\"+str(port)+\"/\"+database\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\npyspark_write(df=df_Abalone_data, table=\"APPEND_TEST_PYSPARK_4K_V1\", user=\"qauser\", password=\"qa@1234!\", host=\"db.stg.bdb.ai\", port=\"5432\", database=\"postgres\")"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}