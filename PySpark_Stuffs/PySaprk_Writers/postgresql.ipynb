{"cells": [{"cell_type": "code", "metadata": {"id": "73433247_0.5496965273094931"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433247_0.1796431034999264"}, "execution_count": 0, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": [{"name": "stdout", "text": ["+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|summary|       SCHOOL_YEAR|        STUDENT_ID|    student_name|teacher_id|      teacher_name|          term_id|term_name|    ASSESSMENT_ID|             subject|          category|             GRADE|       SCORE_VALUE|CALCULATED_SCORE_VALUE|  CALCULATED_SCORE|PROFICIENCY_LEVEL|\n+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|  count|           1586464|           1586464|         1586464|   1586187|           1586461|          1586461|  1586461|          1586461|             1586461|           1586117|           1586461|           1586461|               1586458|           1586458|          1586458|\n|   mean|2015.3423746313335| 18711.63181067798|             8.0|     100.0|224.66666666666666|6.943723691393028|     null|16389.69381729614|  2.6666666666666665|2.6666666666666665|1.6666666666666667|209.92513216211682|     203.5026790010693|3.1553529939021394|             null|\n| stddev| 3.831810041212584|5393737.2739129765|             0.0|       0.0|207.27115895206774|3.421009677415485|     null|40825.85992317898|  0.5773502691896258|0.5773502691896258|0.5773502691896258|203.42570927166335|    203.35109761984563|0.9119469365054234|             null|\n|    min|           \",\"S32\"|         100032356|               8|       100|               104|                1|      BoY|                1|          IB Bio SL2|              2.00|                 1|               ...|                  0.00|               0.0|         Advanced|\n|    max|              2021| Teacher32_Grade12|zuijdgeest,niels|    ms2020|       zarter,nick|        Chemistry|     Year|         Grade 12|ndependent Study-...|                \\N|                PK|                 S|                    \\N|               4.0|       Proficient|\n+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "73433247_0.2606409453688201"}, "execution_count": 1, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to a PostgreSQL database using JDBC.\n\n    Args:\n        df (pyspark.sql.DataFrame): DataFrame to write.\n        table (str): Name of the table to write to.\n        user (str): Username to authenticate with.\n        password (str): Password to authenticate with.\n        host (str): Hostname or IP address of the PostgreSQL server.\n        port (int): Port number of the PostgreSQL server.\n        database (str): Name of the PostgreSQL database to use.\n    \"\"\"\n    try:\n        #         In case of secrets comment url and directly pass url string, username, password in option\n        # eg : df.write.format(\"jdbc\") \\\n        #   .option(\"url\", \"jdbc:postgresql://\"+@ENV.POSTGRES_HOST+\":\"+@ENV.POSTGRES_PORT+\"/\"+@ENV.POSTGRES_DATABASE) \\\n        #   .option(\"dbtable\", table) \\\n        #   .option(\"user\", @ENV.POSTGRES_USERNAME) \\\n        #   .option(\"password\", @ENV.POSTGRES_PASSWORD) \\\n        #   .mode(\"append\") \\\n        #   .save()\n        url = \"jdbc:postgresql://\"+host+\":\"+str(port)+\"/\"+database\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write.format(\"jdbc\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\n\npyspark_write(df=df_Doha_Data, table=\"Postgre_Append_PySpark_V1\", user=\"qauser\", password=\"qa@1234!\", host=\"db.stg.bdb.ai\", port=\"5432\", database=\"postgres\")"], "outputs": [{"name": "stdout", "text": ["Saved Successfully\n"], "output_type": "stream"}]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}