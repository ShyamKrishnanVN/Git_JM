{"cells": [{"cell_type": "code", "metadata": {"id": "92831749_0.5498603520512355"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "92831749_0.40896843307850594"}, "execution_count": null, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Write a PySpark DataFrame to a SQL Server database using JDBC.\n\n    Args:\n        df (pyspark.sql.DataFrame): DataFrame to write.\n        table (str): Name of the table to write to.\n        user (str): Username to authenticate with.\n        password (str): Password to authenticate with.\n        host (str): Hostname or IP address of the SQL Server.\n        port (int): Port number of the SQL Server.\n        database (str): Name of the SQL Server database to use.\n    \"\"\"\n    try:\n        #         In case of secrets comment url and directly pass url string, username, password in option\n        # eg: df.write \\\n        #    .format(\"jdbc\") \\\n        #    .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n        #    .option(\"url\", \"jdbc:sqlserver://\"+@ENV.MSSQL_HOST+\":\"+@ENV.MSSQL_PORT+\";databaseName=\"+@ENV.MSSQL_DATABASE+\";encrypt=true;trustServerCertificate=true;\") \\\n        #    .option(\"dbtable\", table) \\\n        #    .option(\"user\", @ENV.MSSQL_USERNAME) \\\n        #    .option(\"password\", @ENV.MSSQL_PASSWORD) \\\n        #   .mode(\"append\") \\\n        #    .save()\n\n        url = \"jdbc:sqlserver://\"+host+\":\"+str(port)+\";databaseName=\"+database+\";encrypt=true;trustServerCertificate=true;\"\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write \\\n            .format(\"jdbc\") \\\n            .option(\"driver\", \"com.microsoft.sqlserver.jdbc.SQLServerDriver\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .mode(\"append\") \\\n            .save()\n        print(\"Saved Successfully\")\n    except Exception as e:\n        print(e)\npyspark_write(df=df_Doha_Data, table=\"MSSQL_Append_PySpark_V1\", user=\"shyam\", password=\"Shy@8746dd8i23\", host=\"bdbcirclek.c9lo3db08qkg.ap-south-1.rds.amazonaws.com\", port=\"1433\", database=\"dslabtest\")"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}