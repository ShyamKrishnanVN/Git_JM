{"cells": [{"cell_type": "code", "metadata": {"id": "73433244_0.1663005957211412"}, "execution_count": 2, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": [{"name": "stdout", "text": ["+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n|summary| sex|             length|           diameter|             height|       weight_whole|     weight_shucked|     weight_viscera|       weight_shell|             rings|\n+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n|  count|4177|               4177|               4177|               4177|               4177|               4177|               4177|               4177|              4177|\n|   mean|null| 0.5239920995930099|  0.407881254488869| 0.1395163993296614|   0.82874215944458|0.35936748862820106|0.18059360785252604|0.23883085946851795| 9.933684462532918|\n| stddev|null|0.12009291256479936|0.09923986613365941|0.04182705660725731|0.49038901823099795|0.22196294903322014|0.10961425025968445|0.13920266952238622|3.2241690320681315|\n|    min|   F|              0.075|              0.055|                0.0|              0.002|              0.001|             5.0E-4|             0.0015|                 1|\n|    max|   M|              0.815|               0.65|               1.13|             2.8255|              1.488|               0.76|              1.005|                29|\n+-------+----+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+-------------------+------------------+\n\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "73433244_0.11254433380664675"}, "execution_count": 0, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Doha_Data = nb.get_data('11111707978183523', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Doha_Data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": [{"name": "stdout", "text": ["+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|summary|       SCHOOL_YEAR|        STUDENT_ID|    student_name|teacher_id|      teacher_name|          term_id|term_name|    ASSESSMENT_ID|             subject|          category|             GRADE|       SCORE_VALUE|CALCULATED_SCORE_VALUE|  CALCULATED_SCORE|PROFICIENCY_LEVEL|\n+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n|  count|           1586464|           1586464|         1586464|   1586187|           1586461|          1586461|  1586461|          1586461|             1586461|           1586117|           1586461|           1586461|               1586458|           1586458|          1586458|\n|   mean|2015.3423746313335| 18711.63181067798|             8.0|     100.0|224.66666666666666|6.943723691393028|     null|16389.69381729614|  2.6666666666666665|2.6666666666666665|1.6666666666666667|209.92513216211682|     203.5026790010693|3.1553529939021394|             null|\n| stddev| 3.831810041212584|5393737.2739129765|             0.0|       0.0|207.27115895206774|3.421009677415485|     null|40825.85992317898|  0.5773502691896258|0.5773502691896258|0.5773502691896258|203.42570927166335|    203.35109761984563|0.9119469365054234|             null|\n|    min|           \",\"S32\"|         100032356|               8|       100|               104|                1|      BoY|                1|          IB Bio SL2|              2.00|                 1|               ...|                  0.00|               0.0|         Advanced|\n|    max|              2021| Teacher32_Grade12|zuijdgeest,niels|    ms2020|       zarter,nick|        Chemistry|     Year|         Grade 12|ndependent Study-...|                \\N|                PK|                 S|                    \\N|               4.0|       Proficient|\n+-------+------------------+------------------+----------------+----------+------------------+-----------------+---------+-----------------+--------------------+------------------+------------------+------------------+----------------------+------------------+-----------------+\n\n"], "output_type": "stream"}]}, {"cell_type": "code", "metadata": {"id": "73433244_0.7308989777107027"}, "execution_count": 4, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Writes a PySpark DataFrame to a ClickHouse database using JDBC.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to write.\n        table (str): The name of the table in ClickHouse to write to.\n        user (str): The username for the ClickHouse database.\n        password (str): The password for the ClickHouse database.\n        host (str): The hostname for the ClickHouse database.\n        port (str): The port for the ClickHouse database.\n        database (str): The name of the database in ClickHouse to write to.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If there is an error writing to the database.\n    \"\"\"\n    #         In case of secrets comment url and directly pass url string, username, password in option\n    # eg:  df.write \\\n    #       .format(\"jdbc\") \\\n    #       .mode(\"append\") \\\n    #       .option(\"url\", \"jdbc:clickhouse://\"+@ENV.CLICKHOUSE_HOST+\":\"@ENV.CLICKHOUSE_PORT+\"/\"+@ENV.CLICKHOUSE_DATABASE) \\\n    #       .option(\"dbtable\", table) \\\n    #       .option(\"user\", @ENV.CLICKHOUSE_USER) \\\n    #       .option(\"password\", @ENV.CLICKHOUSE_PASSWORD) \\\n    #       .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n    #       .option(\"createTableOptions\", \"ENGINE=Memory\") \\\n    #       .save() \n    url = \"jdbc:clickhouse://\"+host+\":\"+str(port)+\"/\"+database\n\n    try:\n        # Write the DataFrame to ClickHouse using the JDBC connector\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write \\\n            .format(\"jdbc\") \\\n            .mode(\"append\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n            .option(\"createTableOptions\", \"ENGINE=Memory\") \\\n            .save()\n\n        print(\"Saved Successfully\")\n\n    except Exception as e:\n        print(e)\n\n\npyspark_write(df=df_Doha_Data, table=\"Oracle_Append_PySpark_V1\", user=\"qateam\", password=\"Qid6127#d7\", host=\"db.stg.bdb.ai\", port=\"8123\", database=\"qa_test\")"], "outputs": [{"name": "stdout", "text": ["An error occurred while calling o71.save.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 19) (pysparkrepo-hqvy-5d59678d8c-7fppq executor driver): java.sql.SQLException: Cannot set null to non-nullable column #10 [category String]\n\tat com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)\n\tat com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:328)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:1009)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:1007)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:890)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:82)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: java.sql.SQLException: Cannot set null to non-nullable column #10 [category String]\n\tat com.clickhouse.jdbc.SqlExceptionUtils.clientError(SqlExceptionUtils.java:73)\n\tat com.clickhouse.jdbc.internal.InputBasedPreparedStatement.addBatch(InputBasedPreparedStatement.java:328)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.savePartition(JdbcUtils.scala:731)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1(JdbcUtils.scala:891)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.$anonfun$saveTable$1$adapted(JdbcUtils.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1009)\n\tat org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1009)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2303)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n\n"], "output_type": "stream"}]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}