{"cells": [{"cell_type": "code", "metadata": {"id": "73433244_0.1663005957211412"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Abalone_data = nb.get_data('11111707390804200', '@SYS.USERID', 'True', {}, [], None, sparkSession)\ndf_Abalone_data.describe().show()\n# The first function parameter refers to the service ID of the dataset.\n# @SYS.USERID refers to the user ID of the current user.\n# If the Sandbox key is 'false', it is referred to as a dataset, and if it's 'true', then the file is a sandbox file.\n# {} refers to the filters applied to the dataset.\n# [] refers to the data preparations applied to the dataset.\n# After [], users can specify the number of rows to limit the headcount of the dataset with a comma separator.\n# There will be a Spark session enabled in spark environment which will help user to get data."], "outputs": []}, {"cell_type": "code", "metadata": {"id": "73433244_0.7308989777107027"}, "execution_count": 4, "source": ["def pyspark_write(df, table, user, password, host, port, database):\n    \"\"\"\n    Writes a PySpark DataFrame to a ClickHouse database using JDBC.\n\n    Args:\n        df (DataFrame): The PySpark DataFrame to write.\n        table (str): The name of the table in ClickHouse to write to.\n        user (str): The username for the ClickHouse database.\n        password (str): The password for the ClickHouse database.\n        host (str): The hostname for the ClickHouse database.\n        port (str): The port for the ClickHouse database.\n        database (str): The name of the database in ClickHouse to write to.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: If there is an error writing to the database.\n    \"\"\"\n    #         In case of secrets comment url and directly pass url string, username, password in option\n    # eg:  df.write \\\n    #       .format(\"jdbc\") \\\n    #       .mode(\"append\") \\\n    #       .option(\"url\", \"jdbc:clickhouse://\"+@ENV.CLICKHOUSE_HOST+\":\"@ENV.CLICKHOUSE_PORT+\"/\"+@ENV.CLICKHOUSE_DATABASE) \\\n    #       .option(\"dbtable\", table) \\\n    #       .option(\"user\", @ENV.CLICKHOUSE_USER) \\\n    #       .option(\"password\", @ENV.CLICKHOUSE_PASSWORD) \\\n    #       .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n    #       .option(\"createTableOptions\", \"ENGINE=Memory\") \\\n    #       .save() \n    url = \"jdbc:clickhouse://\"+host+\":\"+str(port)+\"/\"+database\n\n    try:\n        # Write the DataFrame to ClickHouse using the JDBC connector\n        #         uncomment below lines if dataframe size is big \n        #         numPartitions = 8\n        #         df = df.repartition(numPartitions)\n        df.write \\\n            .format(\"jdbc\") \\\n            .mode(\"append\") \\\n            .option(\"url\", url) \\\n            .option(\"dbtable\", table) \\\n            .option(\"user\", user) \\\n            .option(\"password\", password) \\\n            .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n            .option(\"createTableOptions\", \"ENGINE=Memory\") \\\n            .save()\n\n        print(\"Saved Successfully\")\n\n    except Exception as e:\n        print(e)\n\n\npyspark_write(df=df_Abalone_data, table=\"APPEND_TEST_PYSPARK_4K_V1\", user=\"qateam\", password=\"Qid6127#d7\", host=\"db.stg.bdb.ai\", port=\"8123\", database=\"qa_test\")"], "outputs": [{"name": "stdout", "text": ["Saved Successfully\n"], "output_type": "stream"}]}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}