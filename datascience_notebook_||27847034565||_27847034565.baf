{"featureSets":null,"autoML":null,"superFeatureSet":null,"errorMessage":null,"featureSet":null,"errorCode":null,"notebookModels":null,"message":"Successfully retrived!","notebookModelAsApi":null,"notebookScriptAsApi":null,"notebookModel":null,"notebookModelAsApis":null,"success":true,"notebooks":null,"customNotebookScript":null,"superFeatureSets":null,"messageCode":null,"stackTrace":null,"notebookContent":"{\"sucess\":true,\"content\":{\"cells\":[{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8419592009416064\"},\"execution_count\":null,\"source\":[\"from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\\nnb = NotebookExecutor()\\ndf = nb.get_data('513606089', '@SYS.USERID', 'True',{})\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8369002647430777\"},\"execution_count\":null,\"source\":[\"import numpy as np\\nimport pandas as pds\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport time\\n\\n# Model Evaluation libraries\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nfrom sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, accuracy_score\\nfrom sklearn.metrics import plot_roc_curve\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.18858852328217357\"},\"execution_count\":null,\"source\":[\"df.head()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.16366431844262963\"},\"execution_count\":null,\"source\":[\"df.info()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.6127967262919054\"},\"execution_count\":null,\"source\":[\"df.isna().sum()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.3912173291379557\"},\"execution_count\":null,\"source\":[\"df['Fraudulent'] = df['Fraudulent'].str.replace('NO','No')\\ndf['Fraudulent'].value_counts()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.01737105693063623\"},\"execution_count\":null,\"source\":[\"df[\\\"Fraudulent\\\"].value_counts().plot(kind=\\\"bar\\\", color=['salmon', 'lightblue'])\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8347540748781987\"},\"execution_count\":null,\"source\":[\"plt.figure(figsize=(18,3))\\ndf[\\\"age\\\"].value_counts().plot(kind=\\\"bar\\\")\\nplt.xticks(rotation = 0);\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.011656382695249157\"},\"execution_count\":null,\"source\":[\"# Which columns are categorical\\ncat_col = [col for col in df.columns if df[col].dtypes == \\\"O\\\"]\\nfrom sklearn.preprocessing import LabelEncoder\\n\\nfor col in cat_col:\\n    enc = LabelEncoder()\\n    values = df[col].unique()\\n    enc_fit = enc.fit(values);\\n    df[col] = enc_fit.transform(df[col])\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.73241358074034\"},\"execution_count\":null,\"source\":[\"df.sample(10)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.25778305180358796\"},\"execution_count\":null,\"source\":[\"df.describe().T\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.2391521668615435\"},\"execution_count\":null,\"source\":[\"df.corr()\\n\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.92773825734819\"},\"execution_count\":null,\"source\":[\"# Let's viualize the corr between independent variables and with dependant (target variable)\\nplt.figure(figsize=(12,7))\\nsns.heatmap(df.corr(), annot=True, cmap=\\\"plasma_r\\\")\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.5176271592391428\"},\"execution_count\":null,\"source\":[\"pd.crosstab(df.Fraudulent, df.gender)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.43787945486040125\"},\"execution_count\":null,\"source\":[\"from sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, classification_report\\nfrom sklearn.ensemble import AdaBoostClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.7458101857754993\"},\"execution_count\":null,\"source\":[\"df.head()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.3003639588450038\"},\"execution_count\":null,\"source\":[\"%%time\\nx_train, x_test, y_train, y_test = train_test_split(df.drop(['Fraudulent'], axis=1), df['Fraudulent'], train_size = 0.75, random_state=42)\\ndt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42)\\n\\nada_fit = AdaBoostClassifier(base_estimator = dt_fit, n_estimators = 7000, learning_rate = 0.03, random_state=42)\\nada_fit.fit(x_train, y_train);\\n\\ncrosstab_train = pd.crosstab(y_train, ada_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\\naccuracy_train = round(accuracy_score(y_train, ada_fit.predict(x_train)),3)\\nclassification_train = classification_report(y_train, ada_fit.predict(x_train))\\n\\ncrosstab_test = pd.crosstab(y_test, ada_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\\naccuracy_test = round(accuracy_score(y_test, ada_fit.predict(x_test)),3)\\nclassification_test = classification_report(y_test, ada_fit.predict(x_test))\\n\\nprint('Training Accuracy:', accuracy_train)\\nprint('Testing Accuracy:', accuracy_test)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.4383285652147144\"},\"execution_count\":null,\"source\":[\"model_ranks = pd.Series(ada_fit.feature_importances_, index=x_train.columns, name='Importance').sort_values(ascending=False, inplace=False)\\nmodel_ranks.index.name = 'Variables'\\ntop_features = model_ranks.iloc[:9].sort_values(ascending=True, inplace=False)\\nplt.figure(figsize=(20,10))\\nax = top_features.plot(kind='barh')\\nax.set_title('Variable Importance Plot')\\nax.set_xlabel('Mean Decrease in Variance')\\nax.set_yticklabels(top_features.index, fontsize=13)\\n\\nprint(top_features.sort_values(ascending=False))\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.7858074856782877\"},\"execution_count\":null,\"source\":[\"from sklearn.utils import class_weight\\nclass_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight(\\n                                                                        class_weight='balanced',\\n                                                                        classes= np.unique(y_train), \\n                                                                        y = y_train)))\\nclass_weights\"],\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"0_0.20243627336163605\"},\"execution_count\":null,\"source\":[\"noticed another performance issue\\r\\nDecisionTreeClassifier() without class_weight, its executed within a minute (57.3sec)\\r\\nDecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42)\\r\\n\\r\\n\\r\\n\\r\\nbut when I add class_weight parameters like below it is not coming out from the cell in DS lab, Jupyter notebook executed this in 43.7 sec.\\r\\nDecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42,class_weight={0: 0.6200787401574803, 1: 2.581967213114754})\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.3251999214849153\"},\"execution_count\":null,\"source\":[\"#%%time\\n#x_train, x_test, y_train, y_test = train_test_split(df.drop(['Fraudulent'], axis=1), df['Fraudulent'], train_size = 0.75, random_state=42)\\n#dt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42,class_weight={0: 0.6200787401574803, 1: 2.581967213114754})\\n\\n#ada_fit = AdaBoostClassifier(base_estimator = dt_fit, n_estimators = 7000, learning_rate = 0.03, random_state=42)\\n#ada_fit.fit(x_train, y_train);\\n\\n#crosstab_train = pd.crosstab(y_train, ada_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\\n#accuracy_train = round(accuracy_score(y_train, ada_fit.predict(x_train)),3)\\n#classification_train = classification_report(y_train, ada_fit.predict(x_train))\\n\\n#crosstab_test = pd.crosstab(y_test, ada_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\\n#accuracy_test = round(accuracy_score(y_test, ada_fit.predict(x_test)),3)\\n#classification_test = classification_report(y_test, ada_fit.predict(x_test))\\n\\n#print('Training Accuracy:', accuracy_train)\\n#print('Testing Accuracy:', accuracy_test)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.09541028411773955\"},\"execution_count\":null,\"source\":[\"# Models from scikit-learn & XGboost\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom xgboost import XGBClassifier\\nfrom xgboost import plot_importance\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.feature_selection import SelectKBest, chi2\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9936617264979428\"},\"execution_count\":null,\"source\":[\"# Let's put our models into dictionary \\nmodels = {\\\"Logistic Regression\\\": LogisticRegression(class_weight=class_weights,solver = 'liblinear'),\\n          \\\"KNN\\\": KNeighborsClassifier(),\\n          \\\"Random Forest Classifier\\\": RandomForestClassifier(class_weight=class_weights),\\n          \\\"XGboost\\\": XGBClassifier()}\\n\\n# Let's create a function to fit and later score our models\\ndef fit_score(models, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fits and evaluates the given machine learning models\\n    \\\"\\\"\\\"\\n    # random seed for reproduction\\n    np.random.seed(42)\\n    \\n    # Let's create a empty dictionary to keep model score\\n    model_score = {}\\n    \\n    # Let's loop through the models dictionary\\n    for name, model in models.items():\\n        # Fit the model\\n        model.fit(X_train, y_train);\\n        # Evaluate the score and append it\\n        model_score[name] = model.score(X_test,y_test)\\n    return model_score\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8952339902210626\"},\"execution_count\":null,\"source\":[\"model_score = fit_score(models=models,\\n                        X_train=x_train,\\n                        X_test=x_test,\\n                        y_train=y_train,\\n                        y_test=y_test)\\nmodel_score\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.15293827485658462\"},\"execution_count\":null,\"source\":[\"# Save into DataFrame\\nmodel_compare = pd.DataFrame(model_score,index=[\\\"Accuracy\\\"])\\nmodel_compare\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.4118059479597216\"},\"execution_count\":null,\"source\":[\"# Let's Plot The Models and Compare\\nmodel_compare.T.plot(kind=\\\"bar\\\")\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8780312158858925\"},\"execution_count\":null,\"source\":[\"# First Let's tune KNN\\ntrain_score = []\\ntest_score  = []\\n\\n# Let's create a list for different neighbors\\nneighbors = range(1, 21)\\n\\n# Setup knn instance\\nknn = KNeighborsClassifier()\\n\\n# loop through different neighbors\\nfor i in neighbors:\\n    knn.set_params(n_neighbors = i)\\n    \\n    # Fit the model\\n    knn.fit(x_train, y_train)\\n    \\n    # Update the training score list\\n    train_score.append(knn.score(x_train, y_train))\\n    \\n    # Update the test score list\\n    test_score.append(knn.score(x_test, y_test))\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9068334556882442\"},\"execution_count\":null,\"source\":[\"# Let's Plot And Viusalize The KNN Tunned Model\\nplt.plot(neighbors, train_score, label= \\\"Train score\\\")\\nplt.plot(neighbors, test_score, label= \\\"Test score\\\")\\nplt.xlabel(\\\"Neighbors\\\")\\nplt.ylabel(\\\"Model Accuracy\\\")\\nplt.legend()\\n\\nprint(f\\\"Maximum KNN score on Test data: {max(test_score)*100 :.2f}%\\\")\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.0733797672030212\"},\"execution_count\":null,\"source\":[\"# For Hyperparameter tunning of Xgboost\\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.07043634769678642\"},\"execution_count\":null,\"source\":[\"# Let's Create  a dictionary with random Parameters for all XGBoostClassifier parameters\\nspace={\\n    'objective': 'binary:logistic', \\n    'use_label_encoder': False, \\n    'base_score': 0.5,\\n    'booster': 'gbtree',\\n    'colsample_bylevel' : 1,\\n    'colsample_bynode' : 1,\\n    'colsample_bytree' : 1,\\n    'enable_categorical': False,\\n    'gamma': hp.uniform('gamma', 0,10),\\n    'gpu_id': -1,\\n    'importance_type': None,\\n    'interaction_constraints': '', \\n    'learning_rate': 0.300000012, \\n    'max_delta_step': 0,\\n    'max_depth': hp.randint(\\\"max_depth\\\", 10)+3,\\n    'min_child_weight' : hp.randint('min_child_weight', 4)+1,\\n    'monotone_constraints': '()',\\n    'n_estimators': hp.randint('n_estimators', 150)+50,\\n    'n_jobs': -1,\\n    'num_parallel_tree':1, \\n    'predictor':'auto', \\n    'random_state': 0,\\n    'reg_alpha' : hp.randint('reg_alpha', 10),\\n    'reg_lambda' : hp.randint('reg_lambda', 10),\\n    'scale_pos_weight': 1,\\n    'subsample': 1,\\n    'tree_method': 'exact',\\n    'validate_parameters':1,\\n    'verbosity': None,\\n    'eval_metric': 'aucpr'\\n    }\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9599033545184767\"},\"execution_count\":null,\"source\":[\"# Let's Define a function for our Space Dictionary and train our model\\ndef objective(space):\\n    clf_model= XGBClassifier(**space)\\n    \\n    evaluation = [( x_train, y_train), ( x_test, y_test)]\\n    \\n    clf_model.fit(x_train, y_train,\\n            eval_set=evaluation, \\n            early_stopping_rounds=10,verbose=False)    \\n\\n    pred = clf_model.predict(x_test)\\n    accuracy = accuracy_score(y_test, pred>0.5)\\n    print (\\\"SCORE:\\\", accuracy)\\n    return {'loss': -accuracy, 'status': STATUS_OK }\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.024888978302045173\"},\"execution_count\":null,\"source\":[\"trials = Trials()\\nbest_hyperparams = fmin(fn = objective,\\n                        space = space,\\n                        algo = tpe.suggest,\\n                        max_evals = 100,\\n                        trials = trials)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9831067362388082\"},\"execution_count\":null,\"source\":[\"print(best_hyperparams)\"],\"outputs\":[]},{\"cell_type\":\"markdown\",\"metadata\":{\"id\":\"0_0.19419000124239383\"},\"execution_count\":null,\"source\":[\"{'gamma': 5.403079910519033,\\n 'max_depth': 9,\\n 'min_child_weight': 0,\\n 'n_estimators': 132,\\n 'reg_alpha': 3,\\n 'reg_lambda': 0}\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9532903721851578\"},\"execution_count\":null,\"source\":[\"best_hyperparams = {'gamma': 1.847682713108156,\\n 'max_depth': 5,\\n 'min_child_weight': 0,\\n 'n_estimators': 22,\\n 'reg_alpha': 1,\\n 'reg_lambda': 0}\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8981334121504663\"},\"execution_count\":null,\"source\":[\"# Let's Create The optimized model with best hyperparameters of XGboost Classifier\\nclf_model_optimizied = XGBClassifier(\\n    objective= 'binary:logistic', \\n    use_label_encoder= False, \\n    base_score= 0.5, \\n    booster= 'gbtree', \\n    colsample_bylevel= 1, \\n    colsample_bynode= 1, \\n    colsample_bytree= 1, \\n    enable_categorical= False, \\n    gamma= best_hyperparams['gamma'], \\n    gpu_id= -1, \\n    importance_type= None, \\n    interaction_constraints= '', \\n    learning_rate= 0.300000012, \\n    max_delta_step= 0, \\n    max_depth= 5, \\n    min_child_weight= best_hyperparams['min_child_weight'], \\n    monotone_constraints= '()',\\n    n_estimators= best_hyperparams['n_estimators'], \\n    n_jobs= 4, \\n    num_parallel_tree= 1, \\n    predictor= 'auto', \\n    random_state= 0, \\n    reg_alpha= best_hyperparams['reg_alpha'], \\n    reg_lambda= best_hyperparams['reg_lambda'], \\n    scale_pos_weight= 1, \\n    subsample= 1, \\n    tree_method= 'exact', \\n    validate_parameters= 1, \\n    verbosity= None, \\n    eval_metric= 'aucpr'\\n)\\nprint(clf_model_optimizied.get_params())\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.26593808361627924\"},\"execution_count\":null,\"source\":[\"# Let's Fit our optimized model\\nxgb_model = clf_model_optimizied.fit(x_train, y_train);\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.6298459771884382\"},\"execution_count\":null,\"source\":[\"# Let's Predict on our Optimized model\\ny_preds = xgb_model.predict(x_test)\\ny_preds\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.4422959778806843\"},\"execution_count\":null,\"source\":[\"# Let's plot ROC Curve and calculate the AUC metric\\nplot_roc_curve(xgb_model, x_test,y_test);\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8839270110742867\"},\"execution_count\":null,\"source\":[\"print(confusion_matrix(y_test,y_preds))\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.8125692920139491\"},\"execution_count\":null,\"source\":[\"print(classification_report(y_test,y_preds));\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.9745374672180587\"},\"execution_count\":null,\"source\":[\"# Let's visualize the confusion matrix\\n\\ndef conf_plot(y_test, y_preds):\\n    \\\"\\\"\\\"\\n    Plots a nice looking heatmap on seaborn\\n    \\\"\\\"\\\"\\n    fix, ax = plt.subplots(figsize=(10,6))\\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\\n                     annot=True,\\n                     cbar=True,\\n                     fmt=\\\"g\\\");\\n    plt.xlabel(\\\"Predicted Labels\\\")\\n    plt.ylabel(\\\"True Labels\\\")\\n\\nconf_plot(y_test, y_preds)  \"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.2841346719121369\"},\"execution_count\":null,\"source\":[\"# Checking the best_parameters\\nxgb_model.get_params()\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.6742304700242385\"},\"execution_count\":null,\"source\":[\"# Creating a new classifier with best parameters\\n\\nfinal_clf_xgb = XGBClassifier(objective= 'binary:logistic', \\n                              use_label_encoder= False, \\n                              base_score= 0.5, \\n                              booster= 'gbtree', \\n                              colsample_bylevel= 1, \\n                              colsample_bynode= 1, \\n                              colsample_bytree= 1, \\n                              enable_categorical= False, \\n                              gamma= 1.847682713108156,      \\n                              importance_type= None, \\n                              interaction_constraints= '', \\n                              learning_rate= 0.300000012, \\n                              max_bin=256,\\n                              max_cat_to_onehot= 4,\\n                              max_delta_step= 0, \\n                              max_depth= 5, \\n                              min_child_weight= 0, \\n                              n_estimators= 22, \\n                              n_jobs= 4, \\n                              num_parallel_tree= 1,                               \\n                              random_state= 0, \\n                              reg_alpha= 2, \\n                              reg_lambda= 1, \\n                              scale_pos_weight= 1, \\n                              subsample= 1, \\n                              sampling_method= 'uniform',\\n                              validate_parameters= 1,\\n                              eval_metric= 'aucpr')\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.18872092127119378\"},\"execution_count\":null,\"source\":[\"xgb_model2 = final_clf_xgb.fit(x_train, y_train)\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.28534779966615753\"},\"execution_count\":null,\"source\":[\"y_preds = xgb_model2.predict(x_test)\\n# Let's plot ROC Curve and calculate the AUC metric\\nplot_roc_curve(xgb_model2, x_test,y_test);\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.561186905465116\"},\"execution_count\":null,\"source\":[\"print(confusion_matrix(y_test,y_preds))\"],\"outputs\":[]},{\"cell_type\":\"code\",\"metadata\":{\"id\":\"0_0.13519202370468242\"},\"execution_count\":null,\"source\":[\"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Cross-validated accuracy\\ncv_acc = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv=100,\\n                         scoring=\\\"accuracy\\\")\\n# Let's take over all mean of the accuracy\\ncv_acc = np.mean(cv_acc)\\nprint(f\\\"The Accuracy for our XGboost Classifier is: {cv_acc : .2f}%\\\")\\n\\n\\n# Cross-validated Precision\\ncv_precision = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"precision\\\")\\n# Let's take over all mean of the Precision\\ncv_precision = np.mean(cv_precision)\\nprint(f\\\"The Precision for our XGboost Classifier is: {cv_precision : .2f}%\\\")\\n\\n\\n# Cross-validated Recall\\ncv_recall = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"recall\\\")\\n# Let's take over all mean of the Recall\\ncv_recall = np.mean(cv_recall)\\nprint(f\\\"The Recall for our XGboost Classifier is: {cv_recall : .2f}%\\\")\\n\\n\\n# Cross-validated f1-score\\ncv_f1 = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"f1\\\")\\n# Let's take over all mean of the Precision\\ncv_f1 = np.mean(cv_f1)\\nprint(f\\\"The f1-score for our XGboost Classifier is:{cv_f1 :.2f}%\\\")\"],\"outputs\":[]}],\"metadata\":{},\"nbformat\":4,\"nbformat_minor\":2}}","bizvizNotebook":null,"autoMLs":null,"scripts":null,"bizvizNotebooks":null,"notebook":{"mongoQL":null,"updatedBy":9018360,"data":"{\"datasets\":[],\"uncheckeddatasets\":[],\"code\":[{\"id\":\"0_0.8419592009416064\",\"conflict\":\"false\",\"code\":\"from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\\nnb = NotebookExecutor()\\ndf = nb.get_data('513606089', '@SYS.USERID', 'True',{})\",\"count\":1,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8369002647430777\",\"conflict\":\"false\",\"code\":\"import numpy as np\\nimport pandas as pds\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\nimport time\\n\\n# Model Evaluation libraries\\nfrom sklearn.model_selection import train_test_split, cross_val_score\\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold\\nfrom sklearn.metrics import classification_report, confusion_matrix\\nfrom sklearn.metrics import precision_score, recall_score, f1_score, make_scorer, accuracy_score\\nfrom sklearn.metrics import plot_roc_curve\",\"count\":2,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.18858852328217357\",\"conflict\":\"false\",\"code\":\"df.head()\",\"count\":3,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.16366431844262963\",\"conflict\":\"false\",\"code\":\"df.info()\",\"count\":4,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.6127967262919054\",\"conflict\":\"false\",\"code\":\"df.isna().sum()\",\"count\":5,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.3912173291379557\",\"conflict\":\"false\",\"code\":\"df['Fraudulent'] = df['Fraudulent'].str.replace('NO','No')\\ndf['Fraudulent'].value_counts()\",\"count\":6,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.01737105693063623\",\"conflict\":\"false\",\"code\":\"df[\\\"Fraudulent\\\"].value_counts().plot(kind=\\\"bar\\\", color=['salmon', 'lightblue'])\",\"count\":7,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8347540748781987\",\"conflict\":\"false\",\"code\":\"plt.figure(figsize=(18,3))\\ndf[\\\"age\\\"].value_counts().plot(kind=\\\"bar\\\")\\nplt.xticks(rotation = 0);\",\"count\":8,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.011656382695249157\",\"conflict\":\"false\",\"code\":\"# Which columns are categorical\\ncat_col = [col for col in df.columns if df[col].dtypes == \\\"O\\\"]\\nfrom sklearn.preprocessing import LabelEncoder\\n\\nfor col in cat_col:\\n    enc = LabelEncoder()\\n    values = df[col].unique()\\n    enc_fit = enc.fit(values);\\n    df[col] = enc_fit.transform(df[col])\",\"count\":9,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.73241358074034\",\"conflict\":\"false\",\"code\":\"df.sample(10)\",\"count\":10,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.25778305180358796\",\"conflict\":\"false\",\"code\":\"df.describe().T\",\"count\":11,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.2391521668615435\",\"conflict\":\"false\",\"code\":\"df.corr()\\n\",\"count\":12,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.92773825734819\",\"conflict\":\"false\",\"code\":\"# Let's viualize the corr between independent variables and with dependant (target variable)\\nplt.figure(figsize=(12,7))\\nsns.heatmap(df.corr(), annot=True, cmap=\\\"plasma_r\\\")\",\"count\":13,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.5176271592391428\",\"conflict\":\"false\",\"code\":\"pd.crosstab(df.Fraudulent, df.gender)\",\"count\":14,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.43787945486040125\",\"conflict\":\"false\",\"code\":\"from sklearn.model_selection import train_test_split\\nfrom sklearn.metrics import accuracy_score, classification_report\\nfrom sklearn.ensemble import AdaBoostClassifier\\nfrom sklearn.tree import DecisionTreeClassifier\",\"count\":15,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.7458101857754993\",\"conflict\":\"false\",\"code\":\"df.head()\",\"count\":16,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.3003639588450038\",\"conflict\":\"false\",\"code\":\"%%time\\nx_train, x_test, y_train, y_test = train_test_split(df.drop(['Fraudulent'], axis=1), df['Fraudulent'], train_size = 0.75, random_state=42)\\ndt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42)\\n\\nada_fit = AdaBoostClassifier(base_estimator = dt_fit, n_estimators = 7000, learning_rate = 0.03, random_state=42)\\nada_fit.fit(x_train, y_train);\\n\\ncrosstab_train = pd.crosstab(y_train, ada_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\\naccuracy_train = round(accuracy_score(y_train, ada_fit.predict(x_train)),3)\\nclassification_train = classification_report(y_train, ada_fit.predict(x_train))\\n\\ncrosstab_test = pd.crosstab(y_test, ada_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\\naccuracy_test = round(accuracy_score(y_test, ada_fit.predict(x_test)),3)\\nclassification_test = classification_report(y_test, ada_fit.predict(x_test))\\n\\nprint('Training Accuracy:', accuracy_train)\\nprint('Testing Accuracy:', accuracy_test)\",\"count\":17,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.4383285652147144\",\"conflict\":\"false\",\"code\":\"model_ranks = pd.Series(ada_fit.feature_importances_, index=x_train.columns, name='Importance').sort_values(ascending=False, inplace=False)\\nmodel_ranks.index.name = 'Variables'\\ntop_features = model_ranks.iloc[:9].sort_values(ascending=True, inplace=False)\\nplt.figure(figsize=(20,10))\\nax = top_features.plot(kind='barh')\\nax.set_title('Variable Importance Plot')\\nax.set_xlabel('Mean Decrease in Variance')\\nax.set_yticklabels(top_features.index, fontsize=13)\\n\\nprint(top_features.sort_values(ascending=False))\",\"count\":18,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.7858074856782877\",\"conflict\":\"false\",\"code\":\"from sklearn.utils import class_weight\\nclass_weights = dict(zip(np.unique(y_train), class_weight.compute_class_weight(\\n                                                                        class_weight='balanced',\\n                                                                        classes= np.unique(y_train), \\n                                                                        y = y_train)))\\nclass_weights\",\"count\":19,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.20243627336163605\",\"conflict\":\"false\",\"code\":\"noticed another performance issue\\r\\nDecisionTreeClassifier() without class_weight, its executed within a minute (57.3sec)\\r\\nDecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42)\\r\\n\\r\\n\\r\\n\\r\\nbut when I add class_weight parameters like below it is not coming out from the cell in DS lab, Jupyter notebook executed this in 43.7 sec.\\r\\nDecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42,class_weight={0: 0.6200787401574803, 1: 2.581967213114754})\",\"count\":20,\"mode\":\"preview\",\"type\":\"markdown\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[],\"result\":\"\"},{\"id\":\"0_0.3251999214849153\",\"conflict\":\"false\",\"code\":\"#%%time\\n#x_train, x_test, y_train, y_test = train_test_split(df.drop(['Fraudulent'], axis=1), df['Fraudulent'], train_size = 0.75, random_state=42)\\n#dt_fit = DecisionTreeClassifier(criterion = 'gini', max_depth = 5, min_samples_split =2, min_samples_leaf = 1, random_state = 42,class_weight={0: 0.6200787401574803, 1: 2.581967213114754})\\n\\n#ada_fit = AdaBoostClassifier(base_estimator = dt_fit, n_estimators = 7000, learning_rate = 0.03, random_state=42)\\n#ada_fit.fit(x_train, y_train);\\n\\n#crosstab_train = pd.crosstab(y_train, ada_fit.predict(x_train), rownames = ['Actual'], colnames = ['Predicted'])\\n#accuracy_train = round(accuracy_score(y_train, ada_fit.predict(x_train)),3)\\n#classification_train = classification_report(y_train, ada_fit.predict(x_train))\\n\\n#crosstab_test = pd.crosstab(y_test, ada_fit.predict(x_test), rownames = ['Actual'], colnames = ['Predicted'])\\n#accuracy_test = round(accuracy_score(y_test, ada_fit.predict(x_test)),3)\\n#classification_test = classification_report(y_test, ada_fit.predict(x_test))\\n\\n#print('Training Accuracy:', accuracy_train)\\n#print('Testing Accuracy:', accuracy_test)\",\"count\":21,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.09541028411773955\",\"conflict\":\"false\",\"code\":\"# Models from scikit-learn & XGboost\\nfrom sklearn.ensemble import RandomForestClassifier\\nfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom xgboost import XGBClassifier\\nfrom xgboost import plot_importance\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.feature_selection import SelectKBest, chi2\",\"count\":22,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.9936617264979428\",\"conflict\":\"false\",\"code\":\"# Let's put our models into dictionary \\nmodels = {\\\"Logistic Regression\\\": LogisticRegression(class_weight=class_weights,solver = 'liblinear'),\\n          \\\"KNN\\\": KNeighborsClassifier(),\\n          \\\"Random Forest Classifier\\\": RandomForestClassifier(class_weight=class_weights),\\n          \\\"XGboost\\\": XGBClassifier()}\\n\\n# Let's create a function to fit and later score our models\\ndef fit_score(models, X_train, X_test, y_train, y_test):\\n    \\\"\\\"\\\"\\n    Fits and evaluates the given machine learning models\\n    \\\"\\\"\\\"\\n    # random seed for reproduction\\n    np.random.seed(42)\\n    \\n    # Let's create a empty dictionary to keep model score\\n    model_score = {}\\n    \\n    # Let's loop through the models dictionary\\n    for name, model in models.items():\\n        # Fit the model\\n        model.fit(X_train, y_train);\\n        # Evaluate the score and append it\\n        model_score[name] = model.score(X_test,y_test)\\n    return model_score\",\"count\":23,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8952339902210626\",\"conflict\":\"false\",\"code\":\"model_score = fit_score(models=models,\\n                        X_train=x_train,\\n                        X_test=x_test,\\n                        y_train=y_train,\\n                        y_test=y_test)\\nmodel_score\",\"count\":24,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.15293827485658462\",\"conflict\":\"false\",\"code\":\"# Save into DataFrame\\nmodel_compare = pd.DataFrame(model_score,index=[\\\"Accuracy\\\"])\\nmodel_compare\",\"count\":25,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.4118059479597216\",\"conflict\":\"false\",\"code\":\"# Let's Plot The Models and Compare\\nmodel_compare.T.plot(kind=\\\"bar\\\")\",\"count\":26,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8780312158858925\",\"conflict\":\"false\",\"code\":\"# First Let's tune KNN\\ntrain_score = []\\ntest_score  = []\\n\\n# Let's create a list for different neighbors\\nneighbors = range(1, 21)\\n\\n# Setup knn instance\\nknn = KNeighborsClassifier()\\n\\n# loop through different neighbors\\nfor i in neighbors:\\n    knn.set_params(n_neighbors = i)\\n    \\n    # Fit the model\\n    knn.fit(x_train, y_train)\\n    \\n    # Update the training score list\\n    train_score.append(knn.score(x_train, y_train))\\n    \\n    # Update the test score list\\n    test_score.append(knn.score(x_test, y_test))\",\"count\":27,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.9068334556882442\",\"conflict\":\"false\",\"code\":\"# Let's Plot And Viusalize The KNN Tunned Model\\nplt.plot(neighbors, train_score, label= \\\"Train score\\\")\\nplt.plot(neighbors, test_score, label= \\\"Test score\\\")\\nplt.xlabel(\\\"Neighbors\\\")\\nplt.ylabel(\\\"Model Accuracy\\\")\\nplt.legend()\\n\\nprint(f\\\"Maximum KNN score on Test data: {max(test_score)*100 :.2f}%\\\")\",\"count\":28,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.0733797672030212\",\"conflict\":\"false\",\"code\":\"# For Hyperparameter tunning of Xgboost\\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\",\"count\":29,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.07043634769678642\",\"conflict\":\"false\",\"code\":\"# Let's Create  a dictionary with random Parameters for all XGBoostClassifier parameters\\nspace={\\n    'objective': 'binary:logistic', \\n    'use_label_encoder': False, \\n    'base_score': 0.5,\\n    'booster': 'gbtree',\\n    'colsample_bylevel' : 1,\\n    'colsample_bynode' : 1,\\n    'colsample_bytree' : 1,\\n    'enable_categorical': False,\\n    'gamma': hp.uniform('gamma', 0,10),\\n    'gpu_id': -1,\\n    'importance_type': None,\\n    'interaction_constraints': '', \\n    'learning_rate': 0.300000012, \\n    'max_delta_step': 0,\\n    'max_depth': hp.randint(\\\"max_depth\\\", 10)+3,\\n    'min_child_weight' : hp.randint('min_child_weight', 4)+1,\\n    'monotone_constraints': '()',\\n    'n_estimators': hp.randint('n_estimators', 150)+50,\\n    'n_jobs': -1,\\n    'num_parallel_tree':1, \\n    'predictor':'auto', \\n    'random_state': 0,\\n    'reg_alpha' : hp.randint('reg_alpha', 10),\\n    'reg_lambda' : hp.randint('reg_lambda', 10),\\n    'scale_pos_weight': 1,\\n    'subsample': 1,\\n    'tree_method': 'exact',\\n    'validate_parameters':1,\\n    'verbosity': None,\\n    'eval_metric': 'aucpr'\\n    }\",\"count\":30,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.9599033545184767\",\"conflict\":\"false\",\"code\":\"# Let's Define a function for our Space Dictionary and train our model\\ndef objective(space):\\n    clf_model= XGBClassifier(**space)\\n    \\n    evaluation = [( x_train, y_train), ( x_test, y_test)]\\n    \\n    clf_model.fit(x_train, y_train,\\n            eval_set=evaluation, \\n            early_stopping_rounds=10,verbose=False)    \\n\\n    pred = clf_model.predict(x_test)\\n    accuracy = accuracy_score(y_test, pred>0.5)\\n    print (\\\"SCORE:\\\", accuracy)\\n    return {'loss': -accuracy, 'status': STATUS_OK }\",\"count\":31,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.024888978302045173\",\"conflict\":\"false\",\"code\":\"trials = Trials()\\nbest_hyperparams = fmin(fn = objective,\\n                        space = space,\\n                        algo = tpe.suggest,\\n                        max_evals = 100,\\n                        trials = trials)\",\"count\":32,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.9831067362388082\",\"conflict\":\"false\",\"code\":\"print(best_hyperparams)\",\"count\":33,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.19419000124239383\",\"conflict\":\"false\",\"code\":\"{'gamma': 5.403079910519033,\\n 'max_depth': 9,\\n 'min_child_weight': 0,\\n 'n_estimators': 132,\\n 'reg_alpha': 3,\\n 'reg_lambda': 0}\",\"count\":34,\"mode\":\"preview\",\"type\":\"markdown\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[],\"result\":\"\"},{\"id\":\"0_0.9532903721851578\",\"conflict\":\"false\",\"code\":\"best_hyperparams = {'gamma': 1.847682713108156,\\n 'max_depth': 5,\\n 'min_child_weight': 0,\\n 'n_estimators': 22,\\n 'reg_alpha': 1,\\n 'reg_lambda': 0}\",\"count\":35,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8981334121504663\",\"conflict\":\"false\",\"code\":\"# Let's Create The optimized model with best hyperparameters of XGboost Classifier\\nclf_model_optimizied = XGBClassifier(\\n    objective= 'binary:logistic', \\n    use_label_encoder= False, \\n    base_score= 0.5, \\n    booster= 'gbtree', \\n    colsample_bylevel= 1, \\n    colsample_bynode= 1, \\n    colsample_bytree= 1, \\n    enable_categorical= False, \\n    gamma= best_hyperparams['gamma'], \\n    gpu_id= -1, \\n    importance_type= None, \\n    interaction_constraints= '', \\n    learning_rate= 0.300000012, \\n    max_delta_step= 0, \\n    max_depth= 5, \\n    min_child_weight= best_hyperparams['min_child_weight'], \\n    monotone_constraints= '()',\\n    n_estimators= best_hyperparams['n_estimators'], \\n    n_jobs= 4, \\n    num_parallel_tree= 1, \\n    predictor= 'auto', \\n    random_state= 0, \\n    reg_alpha= best_hyperparams['reg_alpha'], \\n    reg_lambda= best_hyperparams['reg_lambda'], \\n    scale_pos_weight= 1, \\n    subsample= 1, \\n    tree_method= 'exact', \\n    validate_parameters= 1, \\n    verbosity= None, \\n    eval_metric= 'aucpr'\\n)\\nprint(clf_model_optimizied.get_params())\",\"count\":36,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.26593808361627924\",\"conflict\":\"false\",\"code\":\"# Let's Fit our optimized model\\nxgb_model = clf_model_optimizied.fit(x_train, y_train);\",\"count\":37,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.6298459771884382\",\"conflict\":\"false\",\"code\":\"# Let's Predict on our Optimized model\\ny_preds = xgb_model.predict(x_test)\\ny_preds\",\"count\":38,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.4422959778806843\",\"conflict\":\"false\",\"code\":\"# Let's plot ROC Curve and calculate the AUC metric\\nplot_roc_curve(xgb_model, x_test,y_test);\",\"count\":39,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8839270110742867\",\"conflict\":\"false\",\"code\":\"print(confusion_matrix(y_test,y_preds))\",\"count\":40,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.8125692920139491\",\"conflict\":\"false\",\"code\":\"print(classification_report(y_test,y_preds));\",\"count\":41,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.9745374672180587\",\"conflict\":\"false\",\"code\":\"# Let's visualize the confusion matrix\\n\\ndef conf_plot(y_test, y_preds):\\n    \\\"\\\"\\\"\\n    Plots a nice looking heatmap on seaborn\\n    \\\"\\\"\\\"\\n    fix, ax = plt.subplots(figsize=(10,6))\\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\\n                     annot=True,\\n                     cbar=True,\\n                     fmt=\\\"g\\\");\\n    plt.xlabel(\\\"Predicted Labels\\\")\\n    plt.ylabel(\\\"True Labels\\\")\\n\\nconf_plot(y_test, y_preds)  \",\"count\":42,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.2841346719121369\",\"conflict\":\"false\",\"code\":\"# Checking the best_parameters\\nxgb_model.get_params()\",\"count\":43,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.6742304700242385\",\"conflict\":\"false\",\"code\":\"# Creating a new classifier with best parameters\\n\\nfinal_clf_xgb = XGBClassifier(objective= 'binary:logistic', \\n                              use_label_encoder= False, \\n                              base_score= 0.5, \\n                              booster= 'gbtree', \\n                              colsample_bylevel= 1, \\n                              colsample_bynode= 1, \\n                              colsample_bytree= 1, \\n                              enable_categorical= False, \\n                              gamma= 1.847682713108156,      \\n                              importance_type= None, \\n                              interaction_constraints= '', \\n                              learning_rate= 0.300000012, \\n                              max_bin=256,\\n                              max_cat_to_onehot= 4,\\n                              max_delta_step= 0, \\n                              max_depth= 5, \\n                              min_child_weight= 0, \\n                              n_estimators= 22, \\n                              n_jobs= 4, \\n                              num_parallel_tree= 1,                               \\n                              random_state= 0, \\n                              reg_alpha= 2, \\n                              reg_lambda= 1, \\n                              scale_pos_weight= 1, \\n                              subsample= 1, \\n                              sampling_method= 'uniform',\\n                              validate_parameters= 1,\\n                              eval_metric= 'aucpr')\",\"count\":44,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.18872092127119378\",\"conflict\":\"false\",\"code\":\"xgb_model2 = final_clf_xgb.fit(x_train, y_train)\",\"count\":45,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.28534779966615753\",\"conflict\":\"false\",\"code\":\"y_preds = xgb_model2.predict(x_test)\\n# Let's plot ROC Curve and calculate the AUC metric\\nplot_roc_curve(xgb_model2, x_test,y_test);\",\"count\":46,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.561186905465116\",\"conflict\":\"false\",\"code\":\"print(confusion_matrix(y_test,y_preds))\",\"count\":47,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]},{\"id\":\"0_0.13519202370468242\",\"conflict\":\"false\",\"code\":\"import warnings\\nwarnings.filterwarnings(\\\"ignore\\\")\\n\\n# Cross-validated accuracy\\ncv_acc = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv=100,\\n                         scoring=\\\"accuracy\\\")\\n# Let's take over all mean of the accuracy\\ncv_acc = np.mean(cv_acc)\\nprint(f\\\"The Accuracy for our XGboost Classifier is: {cv_acc : .2f}%\\\")\\n\\n\\n# Cross-validated Precision\\ncv_precision = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"precision\\\")\\n# Let's take over all mean of the Precision\\ncv_precision = np.mean(cv_precision)\\nprint(f\\\"The Precision for our XGboost Classifier is: {cv_precision : .2f}%\\\")\\n\\n\\n# Cross-validated Recall\\ncv_recall = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"recall\\\")\\n# Let's take over all mean of the Recall\\ncv_recall = np.mean(cv_recall)\\nprint(f\\\"The Recall for our XGboost Classifier is: {cv_recall : .2f}%\\\")\\n\\n\\n# Cross-validated f1-score\\ncv_f1 = cross_val_score(xgb_model2,\\n                         x_train,\\n                         y_train,\\n                         cv = 100,\\n                         scoring=\\\"f1\\\")\\n# Let's take over all mean of the Precision\\ncv_f1 = np.mean(cv_f1)\\nprint(f\\\"The f1-score for our XGboost Classifier is:{cv_f1 :.2f}%\\\")\",\"count\":48,\"mode\":\"preview\",\"type\":\"code\",\"hover\":true,\"interrupt\":false,\"warning\":false,\"outputArray\":[],\"lineNumber\":false,\"laodmodel\":false,\"pre_cell_id\":0,\"is_pre_cell\":false,\"succORerr\":true,\"expand\":false,\"outputexpand\":false,\"readonly\":false,\"markDowns\":false,\"loader\":false,\"image\":\"\",\"error\":\"\",\"result\":\"\",\"algorithms\":\"\",\"semicolon\":\"\",\"cputimes\":\"\",\"showcputime\":false,\"secret\":[]}],\"kernalID\":\"20965095-ee5f-496d-b8e5-11464d25e319\",\"Algorithms\":[],\"isScheduled\":0,\"last_modified_date\":\"\"}","isImported":0,"customComponentscript":"","description":"Demo","lastcommittedDate":1726048275609,"isScheduled":null,"committedBy":9018360,"type":1,"uuid":"52311726048275625","loggedUserId":null,"spaceKey":"5231","migrationId":null,"path":"","lastUpdatedDate":1739789569203,"createdDate":1726048275609,"createdBy":9018360,"notebookName":"Sample Imported Notebook","id":27847034565,"projectId":27529882792,"isShared":null,"status":1}}