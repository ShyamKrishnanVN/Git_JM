{"cells": [{"cell_type": "markdown", "metadata": {"id": "37191975_0.42857695802971074"}, "execution_count": null, "source": ["# Model fitting & training"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.02205527562208487"}, "execution_count": 0, "source": ["#sklearn train model new_V6\nimport pandas as pd\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nimport pickle as pkle\nurl = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv\"\nnames = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\ndataframe = pd.read_csv(url, names=names)\narray = dataframe.values\nX = array[:,0:17]\nY = array[:,8]\ntest_size = 0.33\nseed = 7\nX_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = MultinomialNB()\nmodel.fit(X_train, Y_train);"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.0268651763667902"}, "execution_count": null, "source": ["# Model Save"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.6038840533545646"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = model, modelName = 'Model_newTenant_Explainer', modelType = 'ml', X = X_train, y = Y_train, estimator_type='classification')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.9511809870460948"}, "execution_count": null, "source": ["# Model Load"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.42031002161452125"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nloaded_model = nb.load_saved_model('95211690968020598')"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.37315854785075864"}, "execution_count": null, "source": ["X_test_copy = X_test.copy()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.690539898745574"}, "execution_count": null, "source": ["# Model Predict"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.2536934240234978"}, "execution_count": null, "source": ["nb.predict(model = loaded_model, dataframe = X_test_copy, modeltype='ml') \n #Choose modeltype 'ml' for machine learning models and 'cv' for computer vision model \n #ex: For machine learning model nb.predict(model = model, modeltype = 'ml', dataframe = df) \n #ex: For computer vision keras model nb.predict(model = model, modeltype = 'cv', imgs = imgs, imgsize = (28, 28), dim = 1, class_names = class_names) \n #and for pytorch model(model = model, modeltype = 'cv', imgs = imgs, class_names = class_names) \n #Note: incase any error in prediction user squeezed image data in keras"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.8951116898447711"}, "execution_count": null, "source": ["X_test_copy = X_test.copy()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.0059533884386617775"}, "execution_count": null, "source": ["Y_pred = nb.predict(model = loaded_model, dataframe = X_test_copy, modeltype='ml') "], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.9856702205519454"}, "execution_count": null, "source": ["Y_pred.head()"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.12427602801875826"}, "execution_count": null, "source": ["from sklearn.metrics import accuracy_score\naccuracy_score(Y_test, Y_pred.predictions)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.5667844352371305"}, "execution_count": null, "source": ["# Sandbox file read"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.25161633576851705"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\ndf_Test_Data = nb.get_data('95211690968328879', '@SYS.USERID', 'True', {}, [])\ndf_Test_Data.head()"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.4240950038077973"}, "execution_count": null, "source": ["# Artifacts file save"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.17476022256153456"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\n#File extension should be with .csv/.json/.txt\nnb.save_artifact(dataframe = df_Test_Data, name = 'df_Test_Data.txt')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.3047838277018078"}, "execution_count": null, "source": ["# Artifacts saved file read"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.41828740094130445"}, "execution_count": null, "source": ["@SYS.ARTIFACT_PATH+'df_Test_Data.txt'"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.7673772430870638"}, "execution_count": null, "source": ["print(open(@SYS.ARTIFACT_PATH+'df_Test_Data.txt').read())"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.6368721781408218"}, "execution_count": null, "source": ["# Reading uploaded file in forder structure"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.9957714795928714"}, "execution_count": null, "source": ["@SYS.DATASANDBOX_PATH + '76578817/Data/Folder_V1/churn_data_new.csv'"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.6382105343527804"}, "execution_count": null, "source": ["pd.read_csv(@SYS.DATASANDBOX_PATH + '76578817/Data/Folder_V1/churn_data_new.csv')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.6826014459183778"}, "execution_count": null, "source": ["# Utility file read"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.10122121321649713"}, "execution_count": null, "source": ["from Utiity_script import Person\nFuture = Person(\"Shyam\", \"29\")\nprint(Future.name)\nprint(Future.age)"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.25898801772933333"}, "execution_count": null, "source": ["# Data Transformation save"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.41821577432840495"}, "execution_count": null, "source": ["from sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom pickle import dump\n# prepare dataset\nX, y = make_blobs(n_samples=100, centers=2, n_features=2, random_state=1)\n# split data into train and test sets\nX_train, _, y_train, _ = train_test_split(X, y, test_size=0.33, random_state=1)\n# define scaler\nscaler = MinMaxScaler()\n# fit scaler on the training dataset\nscaler.fit(X_train);\n# transform the training dataset\nX_train_scaled = scaler.transform(X_train)\nfrom Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nsaved_model = nb.save_model(model = scaler, modelName = 'ScalerTransform', modelType = 'dp', X = None, y = None, estimator_type='')\n#X and y are training datasets to get explainer dashboard.\n#estimator_type is to specify algorithm type i.e., classification and regression.\n#Only 'ml\u2019 models with tabular data as input will support in Explainer Dashboard.\n#Choose modelType = 'ml' for machine learning models, modelType = 'cv' for computer vision models and modelType = 'dp' for data transformation pickle files. \n#Provide \u2018column_headers\u2019 as a parameter if they have to be saved in the model.\n#If using custom layer in keras, use native save functionality from keras."], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.6451047120949065"}, "execution_count": null, "source": ["# Transformation load"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.8736293397740724"}, "execution_count": null, "source": ["from Notebook.DSNotebook.NotebookExecutor import NotebookExecutor\nnb = NotebookExecutor()\nloaded_model = nb.load_model('95211690970180614')"], "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "37191975_0.02581435260117848"}, "execution_count": null, "source": ["# Transforming traing data"], "outputs": []}, {"cell_type": "code", "metadata": {"id": "37191975_0.33049792686875423"}, "execution_count": null, "source": ["loaded_model.transform(X_train)"], "outputs": []}], "metadata": {}, "nbformat": 4, "nbformat_minor": 2}